{
  
    
        "post0": {
            "title": "Data science revision - chapter 6",
            "content": ". Chapter 6: NumPy Implementation Details . Chapter Outline . . 1. Introduction | 2. Numpy Arrays | 3. Memory Layout and Strides | 4. Why Do We Care? | . Chapter Learning Objectives . Understand at a high level how NumPy arrays are stored in memory. | Explain the concept of &quot;strides&quot; in NumPy. | Use strides to efficiently access data with NumPy. | . 1. Introduction . In this chapter, I&#39;ll show more about how NumPy manages and stores arrays in memory. I&#39;ll stress that a thorough understanding of the NumPy internals like how memory is handled and accessed is really more for developers working on super-optimized algorithms or for those interested in these kinds of things. The data scientist who does not care about these implementation details can skip this chapter and just continue to visualize and use arrays as N-dimensional data structures - afterall the point of NumPy is to abstract away the technical implementation details so such users can focus on writing code and wrangling data! . This chapter draws on excellent material presented in: . The NumPy documentation. | Guide to NumPy by Travis Oliphant, 2006. | . import numpy as np . 2. Numpy Arrays . Recall that an N-dimensional array (&quot;ndarray&quot;) is just a homogenous set of elements. You may be more familiar with the term &quot;vector&quot; (a 1-d array) or a &quot;matrix&quot; (a 2-d array). There are two key pieces of information that describe any given ndarray: . The shape of the array; and, | The datatype of the array elements. | Array Shape . You can think of the shape of an ndarray as the &quot;length&quot; of each dimension. For example, consider the following vector which has 6 elements and shape (6,): . a = np.array([1, 2, 3, 4, 5, 6]) a . array([1, 2, 3, 4, 5, 6]) . This vector has one dimension full of 6 elements: . print(f&quot;The shape of this ndarray is: {a.shape}&quot;) print(f&quot; The number of dimensions is: {a.ndim}&quot;) print(f&quot; The number of elements is: {a.size}&quot;) . The shape of this ndarray is: (6,) The number of dimensions is: 1 The number of elements is: 6 . Compare this vector to the following matrix of shape (2, 3): . a = np.array([[1, 2, 3], [4, 5, 6]]) a . array([[1, 2, 3], [4, 5, 6]]) . print(f&quot;The shape of this ndarray is: {a.shape}&quot;) print(f&quot; The number of dimensions is: {a.ndim}&quot;) print(f&quot; The number of elements is: {a.size}&quot;) . The shape of this ndarray is: (2, 3) The number of dimensions is: 2 The number of elements is: 6 . Finally, here&#39;s a 4-d array (try visualizing that! ü§∑‚Äç‚ôÇÔ∏è): . a = np.arange(36).reshape(2, 3, 3, 2) a . array([[[[ 0, 1], [ 2, 3], [ 4, 5]], [[ 6, 7], [ 8, 9], [10, 11]], [[12, 13], [14, 15], [16, 17]]], [[[18, 19], [20, 21], [22, 23]], [[24, 25], [26, 27], [28, 29]], [[30, 31], [32, 33], [34, 35]]]]) . print(f&quot;The shape of this ndarray is: {a.shape}&quot;) print(f&quot; The number of dimensions is: {a.ndim}&quot;) print(f&quot; The number of elements is: {a.size}&quot;) . The shape of this ndarray is: (2, 3, 3, 2) The number of dimensions is: 4 The number of elements is: 36 . Array Data Type . All ndarrays are homogeneous, meaning that every element has the exact same data-type (e.g., integer, float, string, etc) which takes up the exact same amount of memory. . For example, consider the following 1d-array which is full of 8-bit integers (int8): . a = np.array([1, 2, 3, 4, 5, 6], dtype=&#39;int8&#39;) a . array([1, 2, 3, 4, 5, 6], dtype=int8) . One byte is equal to eight bits (refresh yourself on bits and bytes here), so for this array of int8 data-types, we would expect each element to take up one byte. We can confirm using: . a.itemsize . 1 . An aside on the difference between e.g., int8, int16, int32. The number here refers to the number of bits used to represent each integer. For example, int8 is an integer represented with one byte (one byte = 8 bits). Recall that bits are the basic unit of information &quot;0/1&quot; used by computers. So the maximum unsigned number that can be held with an int8 datatype is:2^8 (but because Python indexes from 0, the unsigned range of int8 is 0 to 257). If we wish to have negative numbers, we need to use one of those bits to represent the sign, and we are left with 2^7 bits to make numbers with, and so the signed range of int8 is -128 to +127. Likewise, int16 has an unsigned range of 0 to 65,535 (2^16), or a signed range of -32,768 to +32,767, etc. It&#39;s interesting to watch what happens if you try to use a dtype that does not support the number you wish to store: . np.array([126, 127, 128, 129, 130, 131, 132], dtype=&#39;int8&#39;) . array([ 126, 127, -128, -127, -126, -125, -124], dtype=int8) . Above, notice how when we exceeded the integer 127 (the max of the int8 signed range), NumPy automatically represents this number by counting up from the minimum of the signed range (-128). Cool! Of course, this wouldn&#39;t be a problem if we used int16: . np.array([126, 127, 128, 129, 130, 131, 132], dtype=&#39;int16&#39;) . array([126, 127, 128, 129, 130, 131, 132], dtype=int16) . Finally I&#39;ll say that technically it is possible to have mixed data-types in an array (i.e., a heterogenous array), but in this case, the array still &quot;sees&quot; each element as the same thing: a reference to some Python object, and the dtype would be &quot;object&quot;. . a = np.array([[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;], 1, 3.14159], dtype=&#39;object&#39;) a . array([list([&#39;a&#39;, &#39;b&#39;, &#39;c&#39;]), 1, 3.14159], dtype=object) . Above is an ndarrays of objects, each one being a reference to some other Python object with its own data-type: . list(map(type, a)) . [list, int, float] . Using arrays like this negates much of the optimized functionality that comes with them, and I can&#39;t recall a time when I&#39;ve used a &quot;heterogenous array&quot;. For mixed data-types, I would typically use other structures like lists or dictionaries. . 3. Memory Layout and Strides . Now that we&#39;ve covered the basic concepts of ndarrays, we can talk more about how arrays are represented in memory. An ndarray is stored as a single ‚Äúchunk‚Äù of memory starting at some location. It&#39;s helpful to think of it as a one-dimensional sequence in memory but with &quot;fancy indexing&quot; that can help map an N-dimensional index (for ndarrays) into that one-dimensional representation. . Consider &quot;Panel a&quot; in the below conceptual figure from the paper Array programming with NumPy, showing how a 2d-array of data-type int64 (8 bytes) is represented in memory as a single chunk: . . That word strides is the number of bytes you need to step in each dimension when traversing the array. As you can see in the example, the stride information is particularly important for mapping the chunk of memory back to a n-dimensional array structure. So in the above case, the strides is (24, 8) meaning 24 bytes (three 8-byte int64 elements) and 8 bytes (one 8-byte int64 element), meaning that every 3 elements we increment our first dimension (i.e., move to the next row) and every 1 element we increment our second dimension (i.e., move to the next column). . Let&#39;s go through another example: . a = np.array([[1, 2], [3, 4], [5, 6]], dtype=&#39;int16&#39;) a . array([[1, 2], [3, 4], [5, 6]], dtype=int16) . Here we have an ndarray of shape (3, 2) and with a dtype of int16 (2 bytes per element in the array). We would expect the stride to be (4, 2) (every 4 bytes, which is 2 elements here for int16, we begin a new row, and every 2 bytes, which is 1 element here, we begin a new column). We can confirm with: . a.strides . (4, 2) . Neat! We could actually change how our ndarray maps from the memory block back to the ndarray by changing the stride information: . a.strides = (2, 4) a . array([[1, 3], [2, 4], [3, 5]], dtype=int16) . To further drive the point home, what do you expect the strides to be of a 1D array? In that case, there is only one dimension to traverse, so we&#39;d expect the strides to just be the number of bytes of 1 element, (2,) (i.e., every 2 bytes, which is one int16 element). Let&#39;s confirm: . a = a.flatten() a . array([1, 3, 2, 4, 3, 5], dtype=int16) . a.strides . (2,) . Finally, let&#39;s look at the strides of the following 3D array of size (3,3,2) but with the data-type int8 (so that 1 byte = 1 element which makes interpreting strides a little easier). Visualizing 3D arrays starts to get a bit tricker, but I think of them as matrices stacked together like slices in a loaf of bread, or multiple chessboards stacked on top of each other. . a = np.arange(18, dtype=&#39;int8&#39;).reshape(3, 3, 2) a . array([[[ 0, 1], [ 2, 3], [ 4, 5]], [[ 6, 7], [ 8, 9], [10, 11]], [[12, 13], [14, 15], [16, 17]]], dtype=int8) . a.strides . (6, 2, 1) . Now things are getting a little more confusing! The above is saying that every 6 elements we increment a &quot;depth dimension&quot;, every 2 elements we increment a &quot;row dimension&quot; and every 1 element we increment a &quot;column dimension&quot;. Using our chessboard analogy, every 6 elements in memory we move to the next chessboard in the stack, every 2 elements we move down one row of a chessboard and every element we move across 1 column of our chessboard. I&#39;ll talk more about this 3D example in the next section. . 4. Why Do We Care? . Reshaping . One sometimes confusing topic in NumPy is understanding how ndarrays are reshaped. But now that you understand how ndarrays are represented in memory, you can better understand how reshaping works in NumPy too! Basically, you can think of reshaping as viewing that &quot;chunk&quot; of memory in a different way (reading it into a different shape but preserving the ordering of the data in memory). Consider the same 2D array we just saw earlier, but with the data-type int8 (so that 1 byte = 1 element which makes interpreting strides a little easier): . a = np.array([[1, 2], [3, 4], [5, 6]], dtype=&#39;int8&#39;) a . array([[1, 2], [3, 4], [5, 6]], dtype=int8) . a.strides . (2, 1) . When we reshape the array, think of it as flattening the array out: . a.flatten() . array([1, 2, 3, 4, 5, 6], dtype=int8) . And then reading that data back into a different shape with different strides. Below I&#39;ll change the shape to be (2, 3), which means that we&#39;ll need strides of (3, 1) (every 3 elements, increment a row, every 1 element, increment a column in the array). . a.shape = (2, 3) a.strides = (3, 1) a . array([[1, 2, 3], [4, 5, 6]], dtype=int8) . Above, I didn&#39;t need to do a.strides. When I changed the shape to (2, 3), NumPy already took care of changing the strides for me, but I&#39;m showing it for demonstration purposes. . The same logic applies to reshaping ndarrays of more than 2 dimensions. For example: . a = np.arange(18, dtype=&#39;int8&#39;) a . array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17], dtype=int8) . a = a.reshape(3, 3, 2) a . array([[[ 0, 1], [ 2, 3], [ 4, 5]], [[ 6, 7], [ 8, 9], [10, 11]], [[12, 13], [14, 15], [16, 17]]], dtype=int8) . a.strides . (6, 2, 1) . In the above example, we have three 2D matrix stacked together (I&#39;ll call them &quot;slices&quot;). We use the first 6 elements of our flattened array to fill in the first &quot;slice&quot;, and within that 2D slice, those elements are arranged in rows and columns dictated by the strides (i.e., every 2 elements increment a row and every 1 element increment a column). Once we&#39;ve used the first 6 elements, we traverse a dimensional &quot;slice&quot; and use the next 6 element to fill that 2D slice. We then repeat that one more time for the last &quot;slice&quot;. . You might be wondering two things at this point: . Why is our array above composed of 3 slices of 3x2 matrices and not 2 slices of 3x3 matrices? | how did NumPy decide to fill each 2D matrix slice with elements first, why not fill along the &quot;depth&quot; dimension first? | These questions are related and actually much deeper than you might expect. They are explained in detail in the NumPy documentation and in section 2.3 Memory Layout of ndarray in the book Guide to NumPy by Travis Oliphant, but they are to do with the fundamental implementation of how NumPy reads data from memory. . Briefly, NumPy uses &quot;row-major&quot; indexing when reading data from memory which basically means that &quot;grouping&quot; starts from the left most index. So for a 2D array, the order is (row, column), for a 3D array the order is (depth, row, column), for a 4D array it is (4th dimension, depth, row, column), etc. The way I think about this is that the ndarray is a container, in the 3D case I think of a cube made up of stacked matrices. I can enter this for container (&quot;dimension&quot;) and view a matrix. The next container is a &quot;row&quot; of values which comprises one smaller container for each &quot;column&quot;. There are two overarching styles that dictate the way data is read in from memory, they are the &quot;C style&quot; and &quot;Fortran style&quot;. NumPy uses the &quot;C style&quot; by default which is what we saw above: . a = np.arange(18, dtype=&#39;int8&#39;).reshape(3, 3, 2, order=&quot;C&quot;) a . array([[[ 0, 1], [ 2, 3], [ 4, 5]], [[ 6, 7], [ 8, 9], [10, 11]], [[12, 13], [14, 15], [16, 17]]], dtype=int8) . But there is also the &quot;Fortran style&quot;, which you can see in the example below and can specify using the order argument, which appears to fill the &quot;depth&quot; dimension first: . a = np.arange(18, dtype=&#39;int8&#39;).reshape(3, 3, 2, order=&quot;F&quot;) a . array([[[ 0, 9], [ 3, 12], [ 6, 15]], [[ 1, 10], [ 4, 13], [ 7, 16]], [[ 2, 11], [ 5, 14], [ 8, 17]]], dtype=int8) . These implementation details are not really necessary to know unless your developing algorithms or packages like NumPy that are directly interfacing with a computer&#39;s memory. . Super-speed Code . Knowing about how ndarrays are stored in memory and what strides are can help us leverage some pretty nifty tricks to speed up our numpy code. Consider performing convolution on a 2D image by passing a filter window of &quot;weights&quot; over the image pixels. For example: . . Source: hackernoon.com . There are plenty of ways to solve this problem. The goal is really to apply our filter to windowed segments of our array. One way we can &quot;view&quot; our array as windows is using strides and the numpy.lib.stride_tricks module. Here&#39;s a 400 x 400 pixel image of yours truly: . import time import matplotlib.pyplot as plt from numpy.lib.stride_tricks import as_strided plt.style.use(&#39;ggplot&#39;) plt.rcParams.update({&#39;font.size&#39;: 16, &#39;figure.figsize&#39;: (8,6), &#39;axes.grid&#39;: False}) . image = plt.imread(&#39;img/chapter6/tomas_beuzen.png&#39;)[:,:,0] plt.imshow(image, cmap=&#39;gray&#39;); . NameError Traceback (most recent call last) &lt;ipython-input-1-65e80ed6e478&gt; in &lt;module&gt; -&gt; 1 image = plt.imread(&#39;img/chapter6/tomas_beuzen.png&#39;)[:,:,0] 2 plt.imshow(image, cmap=&#39;gray&#39;); NameError: name &#39;plt&#39; is not defined . This is the filter I want to apply to the image: . f = np.array([[-2, -1, 0], [-1, 1, 1], [0, 1, 2]]) . Now I&#39;ll use strides to view the image as 3 x 3 windows, so I can there just apply the filter to every single window. Basically the goal here is to view our array as a series of 3x3 windows. So think of this as for each pixel in our image, we want to view a 3x3 window around that pixel. We have 400x400 pixels, and if we have a 3x3 window for each pixel, we will have a 4D view of our array with shape (400, 400, 3, 3). In this case, we can&#39;t have a 3x3 window at the edges of the image, so I&#39;m just going to cut those off with our final shape being (398, 398, 3, 3) (but you could just pad the image with 0&#39;s to apply the filter at the edges if you wanted to). Once we have our 4D view, I can just apply the filter to each of those 3x3 windows and sum the numbers in each window. No for loops are complex functions needed! . That is some wicked fast convolution! üöÄ . If that example was a little too much for you right now, Jessica Yung provides a nice simple example of using arrays and strides in the blog post What makes Numpy Arrays Fast: Memory and Strides. .",
            "url": "https://millermuttu.github.io/blog/jupyter/python/data%20science/programming/numpy/2022/11/23/chapter6-numpy-addendum.html",
            "relUrl": "/jupyter/python/data%20science/programming/numpy/2022/11/23/chapter6-numpy-addendum.html",
            "date": " ‚Ä¢ Nov 23, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Data science revision - chapter 3",
            "content": ". Chapter 3: Unit Tests &amp; Classes . Chapter Outline . . 1. Unit Tests | 2. Debugging | 3. Python Classes | . Chapter Learning Objectives . Formulate a test case to prove a function design specification. | Use an assert statement to validate a test case. | Debug Python code with the pdb module, or by using %debug in a Jupyter code cell. | Describe the difference between a class and a function in Python. | Be able to create a class. | Differentiate between instance attributes and class attributes. | Differentiate between methods, class methods and static methods. | Understand and implement subclassing/inheritance with Python classes. | . 1. Unit Tests . Last chapter we discussed Python functions. But how can we be sure that our function is doing exactly what we expect it to do? Unit testing is the process of testing our function to ensure it&#39;s giving us the results we expect. Let&#39;s briefly introduce the concept here. . assert Statements . assert statements are the most common way to test your functions. They cause your program to fail if the tested condition is False. The syntax is: . assert expression, &quot;Error message if expression is False or raises an error.&quot; . assert 1 == 2, &quot;1 is not equal to 2.&quot; . AssertionError Traceback (most recent call last) &lt;ipython-input-1-74b71e52d7cf&gt; in &lt;module&gt; -&gt; 1 assert 1 == 2, &#34;1 is not equal to 2.&#34; AssertionError: 1 is not equal to 2. . Asserting that two numbers are approximately equal can also be helpful. Due to the limitations of floating-point arithmetic in computers, numbers we expect to be equal are sometimes not: . assert 0.1 + 0.2 == 0.3, &quot;Not equal!&quot; . AssertionError Traceback (most recent call last) &lt;ipython-input-2-be33c16f53c8&gt; in &lt;module&gt; -&gt; 1 assert 0.1 + 0.2 == 0.3, &#34;Not equal!&#34; AssertionError: Not equal! . import math # we&#39;ll learn about importing modules next chapter assert math.isclose(0.1 + 0.2, 0.3), &quot;Not equal!&quot; . You can test any statement that evaluates to a boolean: . assert &#39;varada&#39; in [&#39;mike&#39;, &#39;tom&#39;, &#39;tiffany&#39;], &quot;Instructor not present!&quot; . AssertionError Traceback (most recent call last) &lt;ipython-input-5-ff5cfa3cbd54&gt; in &lt;module&gt; -&gt; 1 assert &#39;varada&#39; in [&#39;mike&#39;, &#39;tom&#39;, &#39;tiffany&#39;], &#34;Instructor not present!&#34; AssertionError: Instructor not present! . Test Driven Development . Test Driven Development (TDD) is where you write your tests before your actual function (more here). This may seem a little counter-intuitive, but you&#39;re creating the expectations of your function before the actual function. This can be helpful for several reasons: . you will better understand exactly what code you need to write; | you are forced to write tests upfront; | you won‚Äôt encounter large time-consuming bugs down the line; and, | it helps to keep your workflow manageable by focussing on small, incremental code improvements and additions. | . In general, the approach is as follows: . Write a stub: a function that does nothing but accept all input parameters and return the correct datatype. | Write tests to satisfy your design specifications. | Outline the program with pseudo-code. | Write code and test frequently. | Write documentation. | EAFP vs LBYL . Somewhat related to testing and function design are the philosophies EAFP and LBYL. EAFP = &quot;Easier to ask for fogiveness than permission&quot;. In coding lingo: try doing something, and if it doesn&#39;t work, catch the error. LBYL = &quot;Look before you leap&quot;. In coding lingo: check that you can do something before trying to do it. These two acronyms refer to coding philosophies about how to write your code. Let&#39;s see an example: . d = {&#39;name&#39;: &#39;Doctor Python&#39;, &#39;superpower&#39;: &#39;programming&#39;, &#39;weakness&#39;: &#39;mountain dew&#39;, &#39;enemies&#39;: 10} . try: d[&#39;address&#39;] except KeyError: print(&#39;Please forgive me!&#39;) . Please forgive me! . if &#39;address&#39; in d.keys(): d[&#39;address&#39;] else: print(&#39;Saved you before you leapt!&#39;) . Saved you before you leapt! . While EAFP is often vouched for in Python, there&#39;s no right and wrong way to code and it&#39;s often context-specific. I personally mix the two philosophies most of the time. . 2. Debugging . So if your Python code doesn&#39;t work: what do you do? At the moment, most of you probably do &quot;manual testing&quot; or &quot;exploratory testing&quot;. You keep changing your code until it works, maybe add some print statements around the place to isolate any problems. For example, consider the random_walker code below, which is adapted with permission from COS 126, Conditionals and Loops: . from random import random def random_walker(T): &quot;&quot;&quot; Simulates T steps of a 2D random walk, and prints the result after each step. Returns the squared distance from the origin. Parameters - T : int The number of steps to take. Returns - float The squared distance from the origin to the endpoint, rounded to 2 decimals. Examples -- &gt;&gt;&gt; random_walker(3) (0, -1) (0, 0) (0, -1) 1.0 &quot;&quot;&quot; x = 0 y = 0 for i in range(T): rand = random() if rand &lt; 0.25: x += 1 if rand &lt; 0.5: x -= 1 if rand &lt; 0.75: y += 1 else: y -= 1 print((x, y)) return round((x ** 2 + y ** 2) ** 0.5, 2) random_walker(5) . (-1, 1) (-2, 2) (-3, 3) (-3, 4) (-3, 3) . 4.24 . If we re-run the code above, our random walker never goes right (the x-coordinate is never +ve). We might try to add some print statements here to see what&#39;s going on: . def random_walker(T): &quot;&quot;&quot; Simulates T steps of a 2D random walk, and prints the result after each step. Returns the squared distance from the origin. Parameters - T : int The number of steps to take. Returns - float The squared distance from the origin to the endpoint, rounded to 2 decimals. Examples -- &gt;&gt;&gt; random_walker(3) (0, -1) (0, 0) (0, -1) 1.0 &quot;&quot;&quot; x = 0 y = 0 for i in range(T): rand = random() print(rand) if rand &lt; 0.25: print(&quot;I&#39;m going right!&quot;) x += 1 if rand &lt; 0.5: print(&quot;I&#39;m going left!&quot;) x -= 1 if rand &lt; 0.75: y += 1 else: y -= 1 print((x, y)) return round((x ** 2 + y ** 2) ** 0.5, 2) random_walker(5) . 0.9667856645891864 (0, -1) 0.4521273416101914 I&#39;m going left! (-1, 0) 0.545326215986303 (-1, 1) 0.837732608088001 (-1, 0) 0.4505836355971342 I&#39;m going left! (-2, 1) . 2.24 . Ah! We see that even every time after a &quot;I&#39;m going right!&quot; we immediately get a &quot;I&#39;m going left!&quot;. The problem is in our if statements, we should be using elif for each statement after the intial if, otherwise multiple conditions may be met each time. . This was a pretty simple debugging case, adding print statements is not always helpful or efficient. Alternatively we can use the module pdb. pdb is the Python Debugger included with the standard library. We can use breakpoint() to leverage pdb and set a &quot;break point&quot; at any point in our code and then inspect our variables. See the pdb docs here and this cheatsheet for help interacting with the debugger console. . def random_walker(T): &quot;&quot;&quot; Simulates T steps of a 2D random walk, and prints the result after each step. Returns the squared distance from the origin. Parameters - T : int The number of steps to take. Returns - float The squared distance from the origin to the endpoint, rounded to 2 decimals. Examples -- &gt;&gt;&gt; random_walker(3) (0, -1) (0, 0) (0, -1) 1.0 &quot;&quot;&quot; x = 0 y = 0 for i in range(T): rand = random() breakpoint() if rand &lt; 0.25: print(&quot;I&#39;m going right!&quot;) x += 1 if rand &lt; 0.5: print(&quot;I&#39;m going left!&quot;) x -= 1 if rand &lt; 0.75: y += 1 else: y -= 1 print((x, y)) return round((x ** 2 + y ** 2) ** 0.5, 2) random_walker(5) . &gt; &lt;ipython-input-11-005ed635a05e&gt;(31)random_walker() 29 rand = random() 30 breakpoint() &gt; 31 if rand &lt; 0.25: 32 print(&#34;I&#39;m going right!&#34;) 33 x += 1 ipdb&gt; rand 0.23867380970947405 ipdb&gt; rand &lt; 0.25 True ipdb&gt; q . BdbQuit Traceback (most recent call last) &lt;ipython-input-11-005ed635a05e&gt; in &lt;module&gt; 42 return round((x ** 2 + y ** 2) ** 0.5, 2) 43 &gt; 44 random_walker(5) &lt;ipython-input-11-005ed635a05e&gt; in random_walker(T) 29 rand = random() 30 breakpoint() &gt; 31 if rand &lt; 0.25: 32 print(&#34;I&#39;m going right!&#34;) 33 x += 1 &lt;ipython-input-11-005ed635a05e&gt; in random_walker(T) 29 rand = random() 30 breakpoint() &gt; 31 if rand &lt; 0.25: 32 print(&#34;I&#39;m going right!&#34;) 33 x += 1 /opt/miniconda3/lib/python3.7/bdb.py in trace_dispatch(self, frame, event, arg) 86 return # None 87 if event == &#39;line&#39;: &gt; 88 return self.dispatch_line(frame) 89 if event == &#39;call&#39;: 90 return self.dispatch_call(frame, arg) /opt/miniconda3/lib/python3.7/bdb.py in dispatch_line(self, frame) 111 if self.stop_here(frame) or self.break_here(frame): 112 self.user_line(frame) --&gt; 113 if self.quitting: raise BdbQuit 114 return self.trace_dispatch 115 BdbQuit: . So the correct code should be: . def random_walker(T): &quot;&quot;&quot; Simulates T steps of a 2D random walk, and prints the result after each step. Returns the squared distance from the origin. Parameters - T : int The number of steps to take. Returns - float The squared distance from the origin to the endpoint, rounded to 2 decimals. Examples -- &gt;&gt;&gt; random_walker(3) (0, -1) (0, 0) (0, -1) 1.0 &quot;&quot;&quot; x = 0 y = 0 for i in range(T): rand = random() if rand &lt; 0.25: x += 1 elif rand &lt; 0.5: x -= 1 elif rand &lt; 0.75: y += 1 else: y -= 1 print((x, y)) return round((x ** 2 + y ** 2) ** 0.5, 2) random_walker(5) . (-1, 0) (-1, -1) (-2, -1) (-3, -1) (-3, -2) . 3.61 . I wanted to show pdb because it&#39;s the standard Python debugger. Most Python IDE&#39;s also have their own debugging workflow, for example, here&#39;s a tutorial on debugging in VSCode. Within Jupyter, there is some &quot;magic&quot; commands that you can use. The one we are interested in here is %debug. There are a few ways you can use it, but the easiest is if a cell raises an error, we can create a new cell underneath and just write %debug and run that cell to debug our previous error. . x = 1 x + &#39;string&#39; . TypeError Traceback (most recent call last) &lt;ipython-input-13-496b359b128e&gt; in &lt;module&gt; 1 x = 1 -&gt; 2 x + &#39;string&#39; TypeError: unsupported operand type(s) for +: &#39;int&#39; and &#39;str&#39; . %debug . &gt; &lt;ipython-input-13-496b359b128e&gt;(2)&lt;module&gt;() 1 x = 1 -&gt; 2 x + &#39;string&#39; ipdb&gt; x 1 ipdb&gt; q . The JupyterLab variable inspector extension is another related helpful tool. . 3. Python Classes . We&#39;ve seen data types like dict and list which are built into Python. We can also create our own data types. These are called classes and an instance of a class is called an object (classes documentation here). The general approach to programming using classes and objects is called object-oriented programming . d = dict() . Here, d is an object, whereas dict is a type . type(d) . dict . type(dict) . type . We say d is an instance of the type dict. Hence: . isinstance(d, dict) . True . Why Create Your Own Types/Classes? . &quot;Classes provide a means of bundling data and functionality together&quot; (from the Python docs), in a way that&#39;s easy to use, reuse and build upon. It&#39;s easiest to discover the utility of classes through an example so let&#39;s get started! . Say we want to start storing information about students and instructors in the University of British Columbia&#39;s Master of Data Science Program (MDS). . {note} Recall that the content of this site is adapted from material I used to teach the 2020/2021 offering of the course &quot;DSCI 511 Python Programming for Data Science&quot; for the University of British Columbia&#39;s Master of Data Science Program. . We&#39;ll start with first name, last name, and email address in a dictionary: . mds_1 = {&#39;first&#39;: &#39;Tom&#39;, &#39;last&#39;: &#39;Beuzen&#39;, &#39;email&#39;: &#39;tom.beuzen@mds.com&#39;} . We also want to be able to extract a member&#39;s full name from their first and last name, but don&#39;t want to have to write out this information again. A function could be good for this: . def full_name(first, last): &quot;&quot;&quot;Concatenate first and last with a space.&quot;&quot;&quot; return f&quot;{first} {last}&quot; . full_name(mds_1[&#39;first&#39;], mds_1[&#39;last&#39;]) . &#39;Tom Beuzen&#39; . We can just copy-paste the same code to create new members: . mds_2 = {&#39;first&#39;: &#39;Tiffany&#39;, &#39;last&#39;: &#39;Timbers&#39;, &#39;email&#39;: &#39;tiffany.timbers@mds.com&#39;} full_name(mds_2[&#39;first&#39;], mds_2[&#39;last&#39;]) . &#39;Tiffany Timbers&#39; . Creating a Class . The above was pretty inefficient. You can imagine that the more objects we want and the more complicated the objects get (more data, more functions) the worse this problem becomes! However, this is a perfect use case for a class! A class can be thought of as a blueprint for creating objects, in this case MDS members. . Terminology alert: . Class data = &quot;Attributes&quot; | Class functions = &quot;Methods&quot; | . Syntax alert: . We define a class with the class keyword, followed by a name and a colon (:): | . class mds_member: pass . mds_1 = mds_member() type(mds_1) . __main__.mds_member . We can add an __init__ method to our class which will be run every time we create a new instance, for example, to add data to the instance. Let&#39;s add an __init__ method to our mds_member class. self refers to the instance of a class and should always be passed to class methods as the first argument. . class mds_member: def __init__(self, first, last): # the below are called &quot;attributes&quot; self.first = first self.last = last self.email = first.lower() + &quot;.&quot; + last.lower() + &quot;@mds.com&quot; . mds_1 = mds_member(&#39;Varada&#39;, &#39;Kolhatkar&#39;) print(mds_1.first) print(mds_1.last) print(mds_1.email) . Varada Kolhatkar varada.kolhatkar@mds.com . To get the full name, we can use the function we defined earlier: . full_name(mds_1.first, mds_1.last) . &#39;Varada Kolhatkar&#39; . But a better way to do this is to integrate this function into our class as a method: . class mds_member: def __init__(self, first, last): self.first = first self.last = last self.email = first.lower() + &quot;.&quot; + last.lower() + &quot;@mds.com&quot; def full_name(self): return f&quot;{self.first} {self.last}&quot; . mds_1 = mds_member(&#39;Varada&#39;, &#39;Kolhatkar&#39;) print(mds_1.first) print(mds_1.last) print(mds_1.email) print(mds_1.full_name()) . Varada Kolhatkar varada.kolhatkar@mds.com Varada Kolhatkar . Notice that we need the parentheses above because we are calling a method (think of it as a function), not an attribute. . Instance &amp; Class Attributes . Attributes like mds_1.first are sometimes called instance attributes. They are specific to the object we have created. But we can also set class attributes which are the same amongst all instances of a class, they are defined outside of the __init__ method. . class mds_member: role = &quot;MDS member&quot; # class attributes campus = &quot;UBC&quot; def __init__(self, first, last): self.first = first self.last = last self.email = first.lower() + &quot;.&quot; + last.lower() + &quot;@mds.com&quot; def full_name(self): return f&quot;{self.first} {self.last}&quot; . All instances of our class share the class attribute: . mds_1 = mds_member(&#39;Tom&#39;, &#39;Beuzen&#39;) mds_2 = mds_member(&#39;Joel&#39;, &#39;Ostblom&#39;) print(f&quot;{mds_1.first} is at campus {mds_1.campus}.&quot;) print(f&quot;{mds_2.first} is at campus {mds_2.campus}.&quot;) . Tom is at campus UBC. Joel is at campus UBC. . We can even change the class attribute after our instances have been created. This will affect all of our created instances: . mds_1 = mds_member(&#39;Tom&#39;, &#39;Beuzen&#39;) mds_2 = mds_member(&#39;Mike&#39;, &#39;Gelbart&#39;) mds_member.campus = &#39;UBC Okanagan&#39; print(f&quot;{mds_1.first} is at campus {mds_1.campus}.&quot;) print(f&quot;{mds_2.first} is at campus {mds_2.campus}.&quot;) . Tom is at campus UBC Okanagan. Mike is at campus UBC Okanagan. . You can also change the class attribute for just a single instance. But this is typically not recommended because if you want differing attributes for instances, you should probably use instance attributes. . class mds_member: role = &quot;MDS member&quot; campus = &quot;UBC&quot; def __init__(self, first, last): self.first = first self.last = last self.email = first.lower() + &quot;.&quot; + last.lower() + &quot;@mds.com&quot; def full_name(self): return f&quot;{self.first} {self.last}&quot; . mds_1 = mds_member(&#39;Tom&#39;, &#39;Beuzen&#39;) mds_2 = mds_member(&#39;Mike&#39;, &#39;Gelbart&#39;) mds_1.campus = &#39;UBC Okanagan&#39; print(f&quot;{mds_1.first} is at campus {mds_1.campus}.&quot;) print(f&quot;{mds_2.first} is at campus {mds_2.campus}.&quot;) . Tom is at campus UBC Okanagan. Mike is at campus UBC. . Methods, Class Methods &amp; Static Methods . The methods we&#39;ve seen so far are sometimes calles &quot;regular&quot; methods, they act on an instance of the class (i.e., take self as an argument). We also have class methods that act on the actual class. class methods are often used as &quot;alternative constructors&quot;. As an example, let&#39;s say that somebody commonly wants to use our class with comma-separated names like the following: . name = &#39;Tom,Beuzen&#39; . Unfortunately, those users can&#39;t do this: . mds_member(name) . TypeError Traceback (most recent call last) &lt;ipython-input-39-aba2b627390d&gt; in &lt;module&gt; -&gt; 1 mds_member(name) TypeError: __init__() missing 1 required positional argument: &#39;last&#39; . To use our class, they would need to parse this string into first and last: . first, last = name.split(&#39;,&#39;) print(first) print(last) . Tom Beuzen . Then they could make an instance of our class: . mds_1 = mds_member(first, last) . If this is a common use case for the users of our code, we don&#39;t want them to have to coerce the data every time before using our class. Instead, we can facilitate their use-case with a class method. There are two things we need to do to use a class method: . Identify our method as class method using the decorator @classmethod (more on decorators in a bit); | Pass cls instead of self as the first argument. | class mds_member: role = &quot;MDS member&quot; campus = &quot;UBC&quot; def __init__(self, first, last): self.first = first self.last = last self.email = first.lower() + &quot;.&quot; + last.lower() + &quot;@mds.com&quot; def full_name(self): return f&quot;{self.first} {self.last}&quot; @classmethod def from_csv(cls, csv_name): first, last = csv_name.split(&#39;,&#39;) return cls(first, last) . Now we can use our comma-separated values directly! . mds_1 = mds_member.from_csv(&#39;Tom,Beuzen&#39;) mds_1.full_name() . &#39;Tom Beuzen&#39; . There is a third kind of method called a static method. static methods do not operate on either the instance or the class, they are just simple functions. But we might want to include them in our class because they are somehow related to our class. They are defined using the @staticmethod decorator: . class mds_member: role = &quot;MDS member&quot; campus = &quot;UBC&quot; def __init__(self, first, last): self.first = first self.last = last self.email = first.lower() + &quot;.&quot; + last.lower() + &quot;@mds.com&quot; def full_name(self): return f&quot;{self.first} {self.last}&quot; @classmethod def from_csv(cls, csv_name): first, last = csv_name.split(&#39;,&#39;) return cls(first, last) @staticmethod def is_quizweek(week): return True if week in [3, 5] else False . Note that the method is_quizweek() does not accept or use the self argument. But it is still MDS-related, so we might want to include it here. . mds_1 = mds_member.from_csv(&#39;Tom,Beuzen&#39;) print(f&quot;Is week 1 a quiz week? {mds_1.is_quizweek(1)}&quot;) print(f&quot;Is week 3 a quiz week? {mds_1.is_quizweek(3)}&quot;) . Is week 1 a quiz week? False Is week 3 a quiz week? True . Decorators . Decorators can be quite a complex topic, you can read more about them here. Briefly, they are what they sounds like, they &quot;decorate&quot; functions/methods with additional functionality. You can think of a decorator as a function that takes another function and adds functionality. . Let&#39;s create a decorator as an example. Recall that functions are data types in Python, they can be passed to other functions. So a decorator simply takes a function as an argument, adds some more functionality to it, and returns a &quot;decorated function&quot; that can be executed. . def original_func(): print(&quot;I&#39;m the original function!&quot;) # a decorator def my_decorator(original_func): # takes our original function as input def wrapper(): # wraps our original function with some extra functionality print(f&quot;A decoration before {original_func.__name__}.&quot;) result = original_func() print(f&quot;A decoration after {original_func.__name__}.&quot;) return result return wrapper # returns the unexecuted wrapper function which we can can excute later . The my_decorator() function will return to us a function which is the decorated version of our original function. . my_decorator(original_func) . &lt;function __main__.my_decorator.&lt;locals&gt;.wrapper()&gt; . As a function was returned to us, we can execute it by adding parentheses: . my_decorator(original_func)() . A decoration before original_func. I&#39;m the original function! A decoration after original_func. . We can decorate any arbitrary function with our decorator: . def another_func(): print(&quot;I&#39;m a different function!&quot;) my_decorator(another_func)() . A decoration before another_func. I&#39;m a different function! A decoration after another_func. . The syntax of calling our decorator is not that readable. Instead, we can use the @ symbol as &quot;syntactic sugar&quot; to improve readability and reuseability of decorators: . @my_decorator def one_more_func(): print(&quot;One more function...&quot;) one_more_func() . A decoration before one_more_func. One more function... A decoration after one_more_func. . Okay, let&#39;s make something a little more useful. We will create a decorator that times the execution time of any arbitrary function: . import time # import the time module, we&#39;ll learn about imports next chapter def timer(my_function): # the decorator def wrapper(): # the added functionality t1 = time.time() result = my_function() # the original function t2 = time.time() print(f&quot;{my_function.__name__} ran in {t2 - t1:.3f} sec&quot;) # print the execution time return result return wrapper . @timer def silly_function(): for i in range(10_000_000): if (i % 1_000_000) == 0: print(i) else: pass silly_function() . 0 1000000 2000000 3000000 4000000 5000000 6000000 7000000 8000000 9000000 silly_function ran in 0.601 sec . Python&#39;s built-in decorators like classmethod and staticmethod are coded in C so I&#39;m not showing them here. I don&#39;t often create my own decorators, but I use the built-in decorators all the time. . Inheritance &amp; Subclasses . Just like it sounds, inheritance allows us to &quot;inherit&quot; methods and attributes from another class. So far, we&#39;ve been working with an mds_member class. But let&#39;s get more specific and create a mds_student and mds_instructor class. Recall this was mds_member: . class mds_member: role = &quot;MDS member&quot; campus = &quot;UBC&quot; def __init__(self, first, last): self.first = first self.last = last self.email = first.lower() + &quot;.&quot; + last.lower() + &quot;@mds.com&quot; def full_name(self): return f&quot;{self.first} {self.last}&quot; @classmethod def from_csv(cls, csv): first, last = csv_name.split(&#39;,&#39;) return cls(first, last) @staticmethod def is_quizweek(week): return True if week in [3, 5] else False . We can create an mds_student class that inherits all of the attributes and methods from our mds_member class by by simply passing the mds_member class as an argument to an mds_student class definition: . class mds_student(mds_member): pass . student_1 = mds_student(&#39;Craig&#39;, &#39;Smith&#39;) student_2 = mds_student(&#39;Megan&#39;, &#39;Scott&#39;) print(student_1.full_name()) print(student_2.full_name()) . Craig Smith Megan Scott . What happened here is that our mds_student instance first looked in the mds_student class for an __init__ method, which it didn&#39;t find. It then looked for the __init__ method in the inherited mds_member class and found something to use! This order is called the &quot;method resolution order&quot;. We can inspect it directly using the help() function: . help(mds_student) . Help on class mds_student in module __main__: class mds_student(mds_member) | mds_student(first, last) | | Method resolution order: | mds_student | mds_member | builtins.object | | Methods inherited from mds_member: | | __init__(self, first, last) | Initialize self. See help(type(self)) for accurate signature. | | full_name(self) | | - | Class methods inherited from mds_member: | | from_csv(csv) from builtins.type | | - | Static methods inherited from mds_member: | | is_quizweek(week) | | - | Data descriptors inherited from mds_member: | | __dict__ | dictionary for instance variables (if defined) | | __weakref__ | list of weak references to the object (if defined) | | - | Data and other attributes inherited from mds_member: | | campus = &#39;UBC&#39; | | role = &#39;MDS member&#39; . Okay, let&#39;s fine-tune our mds_student class. The first thing we might want to do is change the role of the student instances to &quot;MDS Student&quot;. We can do that by simply adding a class attribute to our mds_student class. Any attributes or methods not &quot;over-ridden&quot; in the mds_student class will just be inherited from the mds_member class. . class mds_student(mds_member): role = &quot;MDS student&quot; . student_1 = mds_student(&#39;John&#39;, &#39;Smith&#39;) print(student_1.role) print(student_1.campus) print(student_1.full_name()) . MDS student UBC John Smith . Now let&#39;s add an instance attribute to our class called grade. You might be tempted to do something like this: . class mds_student(mds_member): role = &quot;MDS student&quot; def __init__(self, first, last, grade): self.first = first self.last = last self.email = first.lower() + &quot;.&quot; + last.lower() + &quot;@mds.com&quot; self.grade = grade student_1 = mds_student(&#39;John&#39;, &#39;Smith&#39;, &#39;B+&#39;) print(student_1.email) print(student_1.grade) . john.smith@mds.com B+ . But this is not DRY code, remember that we&#39;ve already typed most of this in our mds_member class. So what we can do is let the mds_member class handle our first and last argument and we&#39;ll just worry about grade. We can do this easily with the super() function. Things can get pretty complicated with super(), you can read more here, but all you really need to know is that super() allows you to inherit attributes/methods from other classes. . class mds_student(mds_member): role = &quot;MDS student&quot; def __init__(self, first, last, grade): super().__init__(first, last) self.grade = grade student_1 = mds_student(&#39;John&#39;, &#39;Smith&#39;, &#39;B+&#39;) print(student_1.email) print(student_1.grade) . john.smith@mds.com B+ . Amazing! Hopefully you can start to see how powerful inheritance can be. Let&#39;s create another subclass called mds_instructor, which has two new methods add_course() and remove_course(). . class mds_instructor(mds_member): role = &quot;MDS instructor&quot; def __init__(self, first, last, courses=None): super().__init__(first, last) self.courses = ([] if courses is None else courses) def add_course(self, course): self.courses.append(course) def remove_course(self, course): self.courses.remove(course) . instructor_1 = mds_instructor(&#39;Tom&#39;, &#39;Beuzen&#39;, [&#39;511&#39;, &#39;561&#39;, &#39;513&#39;]) print(instructor_1.full_name()) print(instructor_1.courses) . Tom Beuzen [&#39;511&#39;, &#39;561&#39;, &#39;513&#39;] . instructor_1.add_course(&#39;591&#39;) instructor_1.remove_course(&#39;513&#39;) instructor_1.courses . [&#39;511&#39;, &#39;561&#39;, &#39;591&#39;] . Getters/Setters/Deleters . There&#39;s one more import topic to talk about with Python classes and that is getters/setters/deleters. The necessity for these actions is best illustrated by example. Here&#39;s a stripped down version of the mds_member class from earlier: . class mds_member: def __init__(self, first, last): self.first = first self.last = last self.email = first.lower() + &quot;.&quot; + last.lower() + &quot;@mds.com&quot; def full_name(self): return f&quot;{self.first} {self.last}&quot; . mds_1 = mds_member(&#39;Tom&#39;, &#39;Beuzen&#39;) print(mds_1.first) print(mds_1.last) print(mds_1.email) print(mds_1.full_name()) . Tom Beuzen tom.beuzen@mds.com Tom Beuzen . Imagine that I mis-spelled the name of this class instance and wanted to correct it. Watch what happens... . mds_1.first = &#39;Tomas&#39; print(mds_1.first) print(mds_1.last) print(mds_1.email) print(mds_1.full_name()) . Tomas Beuzen tom.beuzen@mds.com Tomas Beuzen . Uh oh... the email didn&#39;t update with the new first name! We didn&#39;t have this problem with the full_name() method because it just calls the current first and last name. You might think that the best thing to do here is to create a method for email() like we have for full_name(). But this is bad coding for a variety of reasons, for example it means that users of your code will have to change every call to the email attribute to a call to the email() method. We&#39;d call that a breaking change to our software and we want to avoid that where possible. What we can do instead, is define our email like a method, but keep it as an attribute using the @property decorator. . class mds_member: def __init__(self, first, last): self.first = first self.last = last def full_name(self): return f&quot;{self.first} {self.last}&quot; @property def email(self): return self.first.lower() + &quot;.&quot; + self.last.lower() + &quot;@mds.com&quot; . mds_1 = mds_member(&#39;Tom&#39;, &#39;Beuzen&#39;) mds_1.first = &#39;Tomas&#39; print(mds_1.first) print(mds_1.last) print(mds_1.email) print(mds_1.full_name()) . Tomas Beuzen tomas.beuzen@mds.com Tomas Beuzen . We could do the same with the full_name() method if we wanted too... . class mds_member: def __init__(self, first, last): self.first = first self.last = last @property def full_name(self): return f&quot;{self.first} {self.last}&quot; @property def email(self): return self.first.lower() + &quot;.&quot; + self.last.lower() + &quot;@mds.com&quot; . mds_1 = mds_member(&#39;Tom&#39;, &#39;Beuzen&#39;) mds_1.full_name . &#39;Tom Beuzen&#39; . But what happens if we instead want to make a change to the full name now? . mds_1.full_name = &#39;Thomas Beuzen&#39; . AttributeError Traceback (most recent call last) &lt;ipython-input-71-74e4ce79e805&gt; in &lt;module&gt; -&gt; 1 mds_1.full_name = &#39;Thomas Beuzen&#39; AttributeError: can&#39;t set attribute . We get an error... Our class instance doesn&#39;t know what to do with the value it was passed. Ideally, we&#39;d like our class instance to use this full name information to update self.first and self.last. To handle this action, we need a setter, defined using the decorator @&lt;attribute&gt;.setter: . class mds_member: def __init__(self, first, last): self.first = first self.last = last @property def full_name(self): return f&quot;{self.first} {self.last}&quot; @full_name.setter def full_name(self, name): first, last = name.split(&#39; &#39;) self.first = first self.last = last @property def email(self): return self.first.lower() + &quot;.&quot; + self.last.lower() + &quot;@mds.com&quot; . mds_1 = mds_member(&#39;Tom&#39;, &#39;Beuzen&#39;) mds_1.full_name = &#39;Thomas Beuzen&#39; print(mds_1.first) print(mds_1.last) print(mds_1.email) print(mds_1.full_name) . Thomas Beuzen thomas.beuzen@mds.com Thomas Beuzen . Almost there! We&#39;ve talked about getting information and setting information, but what about deletting information? This is typically used to do some clean up and is defined with the @&lt;attribute&gt;.deleter decorator. I rarely use this method but I want you to see it: . class mds_member: def __init__(self, first, last): self.first = first self.last = last @property def full_name(self): return f&quot;{self.first} {self.last}&quot; @full_name.setter def full_name(self, name): first, last = name.split(&#39; &#39;) self.first = first self.last = last @full_name.deleter def full_name(self): print(&#39;Name deleted!&#39;) self.first = None self.last = None @property def email(self): return self.first.lower() + &quot;.&quot; + self.last.lower() + &quot;@mds.com&quot; . mds_1 = mds_member(&#39;Tom&#39;, &#39;Beuzen&#39;) delattr(mds_1, &quot;full_name&quot;) print(mds_1.first) print(mds_1.last) . Name deleted! None None . Congrats for making it to the end, that was a lot of content and some tough topics to get through, so well done!! .",
            "url": "https://millermuttu.github.io/blog/jupyter/python/data%20science/programming/2022/11/23/chapter3-tests-classes.html",
            "relUrl": "/jupyter/python/data%20science/programming/2022/11/23/chapter3-tests-classes.html",
            "date": " ‚Ä¢ Nov 23, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Understanding FTIR with Michelson interferometer | Theory + Codes explainedüíØ",
            "content": "If you have seen all those bulky FTIR instruments in the physics or chemistry lab during your science or engineering courses you might be wondering what technology lies inside those bulky and very expensive instruments right. when I say expensive i really mean it FTIR instruments are very expensive don&#39;t mess around and blew them away!!üòÖ . The design of many interferometers used for infrared spectrometry in modern era is based on that of the two-beam interferometer originally designed by Michelson way back in 18th century. you might be wondering what is ‚Äòspectroscopy‚Äô and what is ‚Äòinfrared‚Äô, I know that feeling, this is what even I felt when I started researching about Spectroscopy. . Half the battle in learning any new field is understanding the jargon. To aid you in learning about Michelson interferometer, a number of the terms used in the field of infrared spectroscopy are defined below, . Spectroscopy‚Ää‚Äî‚Ääthe study of the interaction of light with matter. . Infrared Spectroscopy ‚Äî the study of the interaction of infrared light with matter. . Mid-Infrared‚Ää‚Äî‚Äälight from 4000 to 400 wavenumbers (cm-1). or in wavelength 0.7um to 1000um. . Spectrum‚Ää‚Äî‚Ääa plot of measured light intensity versus some property of light such as wavelength or wavenumber. . Spectrometer‚Ää‚Äî‚Ääan instrument that measures a spectrum. . Infrared Spectrometer‚Ää‚Äî‚Ääan instrument that measures an infrared spectrum . FTIR‚Ää‚Äî‚ÄäFourier Transform Infrared, a specific type of infrared spectrometer. . Hope now you are sounding good with mostly used jargons of spectroscopy in particularly infrared spectroscopy. coming back to our topic of interest, The Michelson interferometer is a device that can divide a beam of radiation into two paths and then recombine the two beams after a path difference has been introduced. A condition is thereby created under which interference between the beams can occur. The variation of intensity of the beam emerging from the interferometer is measured as a function of path difference by a detector. a simplest form of Michelson interferometer is as shown in the below figure. . . A simple representation of Michelson interferometer. . As shown in the figure it consists of a IR source at the left, Two mutually perpendicular placed mirrors, out of the two one mirror is static(F) and other one is movable(M) along the axis that is perpendicular to its plane. at the heart of the interferometer is a optical device called beam splitter and a detector at the bottom. . The light emitted from the source travels to the beam splitter and partially reflected to the fixed mirror and partially transmitted to movable mirror. this transmission and reflection depends on the characteristics of beam splitter for sake of simplicity let us assume that beam splitter splits the light perfectly that makes it half light transmitted and half a light is reflected. the transmitted and reflected light travels the respective path and get reflected upon hitting the mirrors and travels back towards the beam splitter. when the beams returns to the beam splitter they interfere (here we are considering property of wave which can be added or substracted based on their phase when they interfere) and are again partially reflected and partially transmitted. Because of effect of interference the intensity of each beam passing to the detector depends on the difference of path travelled by beams. . As the fixed mirror is always static and moving mirror moves toward and away from beam splitter. the path difference created will result in interferogram generation. this path difference is called as optical path difference denoted by Œ¥. when the fixed and movable mirror are equidistance from the beam splitter path difference is Zero this is also called as Zero path difference(ZPD) the two beams are in prefect phase and undergo constructive interference ad the intensity of the beam passing to the detector is sum of the intensity of the two light passing to fixed and movable mirrors. based on the Œ¥ we can say either beam has undergone constructive or destructive interference. . if $Œ¥ = nŒª$ . Where Œ¥ = Optical path difference, Œª = Wavelength, n = 0, 1, 2, ‚Ä¶ (any integer) the beam undergo constructive interference. . if $Œ¥ = (n + 1/2)Œª$ . Where Œ¥ = Optical path difference, Œª = Wavelength, n = 0, 1, 2, ‚Ä¶ (any integer) the beam undergo destructive interference. . Since we got some understanding on the theory part of Michelson interferometer, you might be wondering how the initial setup of Michelson interferometer would have looked like!! well it was very huge instrument at the early stage of development here is the visualization of early Michelson interferogram device. . 18th century initial prototype of Michelson interferometer device. . . Albert Michelson was the first American to win a Nobel prize . . Well with all respect to Michelson and his grate work in the physics, lets get our hand dirty with some codes to understand the working principle of Michelson interferogram even better. please find attached python code here. copy and paste it your fav editor and play around with variables to see different interferogram patterns . import numpy as np import matplotlib.pyplot as plt start_wn = 4000 # starting wavelength number end_wn = 4500 # ending wavelength number number_wn = end_wn-start_wn +1 wn_vector = np.arange(start_wn,end_wn+1,1) wn_intensity = np.ones(number_wn) # chose two wavenumbers and make their intensity 0.5 wn_1 = 4002 wn_2 = 4004 wn_intensity[wn_1-start_wn+1] = 0.5 wn_intensity[wn_2-start_wn+1] = 0.5 M1 = 10 # 20 cm distance of M1 from BS start_M2 = 10 # 10 cm starting distance of M2 from BS end_M2 = 10.2 # ending distance of M2 from BS mirror_velocity = 0.1 # velocity of moving mirror, cm/s sample_rate = 100000 # 1 MSPS step_dist = mirror_velocity/sample_rate # # step of each movement num_steps = (end_M2-start_M2)/step_dist # total number of steps total_intf = [] for i in np.arange(num_steps): opd_vector = i*step_dist temp_intf = np.multiply(wn_intensity,np.cos(2*np.pi*opd_vector*wn_vector)) total_intf.append(np.sum(temp_intf)) plt.figure(3) plt.plot(total_intf) plt.xlabel(&#39;Number of steps&#39;) plt.ylabel(&#39;Intensity&#39;) plt.title(&#39;Interferogram&#39;) plt.show() . Here we are selecting the light source from 4000 to 4500 cm-1 wavenumbers. and to see the effect of absorption of the light by the sample placed in between source and beam splitter, we are reducing intensity of 4002 and 4004 wavenumbers as 0.5 and rest all beams intensity is set to 1. then we define the distance of fixed mirror at 10cm (M1) and distance of moving mirror between 10 and 10.2 cm. our moving mirror travels a distance of 0.2 cm. we also set other variables like mirror speed and sampling rate. please find the output of interferogram obtained from above code snippet. . Now we have got the interferogram signal, but its very difficult understand the interferogram generated. hence better and most practical way to understand the interferogram is generation of fringe patterns. we will use lightpipes package to generate fringe pattern by changing the distance of moving mirror. . Note: Open up a jupyter notebook and paste the below code to run it as we are using ipywidget to visualize the result . from ipywidgets import interact, interactive, fixed, interact_manual import ipywidgets as widgets import matplotlib.pyplot as plt from LightPipes import * import math import matplotlib.image as mpimg # wavelength=632.8*nm #wavelength of HeNe laser # size=10*mm # size of the grid # N=300 # number (NxN) of grid pixels # R=3*mm # laser beam radius # z1=8*cm # length of arm 1 # z2=7*cm # length of arm 2 # z3=3*cm # distance laser to beamsplitter # z4=5*cm # distance beamsplitter to screen # Rbs=0.5 # reflection beam splitter # tx=1*mrad; ty=0.0*mrad # tilt of mirror 1 # f=50*cm # focal length of positive lens # fig = plt.figure() # ax = fig.add_subplot(111) def MI(wavelength,size,N,R,z1,z2,z3,z4,Rbs,tx,ty,f): z1 = z1*cm z2 = z2 *cm z3 = z3*cm z4 = z4*cm #Generate a weak converging laser beam using a weak positive lens: F=Begin(size*mm,wavelength*nm,int(N)) #F=GaussBeam(F, R) #F=GaussHermite(F,R,0,0,1) #new style #F=GaussHermite(F,R) #new style F=GaussHermite(0,0,1,R*mm,F) #old style F=Lens(f*cm,0,0,F) #Propagate to the beamsplitter: F=Forvard(z3,F) #Split the beam and propagate to mirror #2: F2=IntAttenuator(1-Rbs,F) F2=Forvard(z2,F2) #Introduce tilt and propagate back to the beamsplitter: F2=Tilt(tx*mrad,ty*mrad,F2) F2=Forvard(z2,F2) F2=IntAttenuator(Rbs,F2) #Split off the second beam and propagate to- and back from the mirror #1: F10=IntAttenuator(Rbs,F) F1=Forvard(z1*2,F10) F1=IntAttenuator(1-Rbs,F1) #Recombine the two beams and propagate to the screen: F=BeamMix(F1,F2) F=Forvard(z4,F) I=Intensity(1,F) # ax.format_coord = lambda x, y: &#39;x = %2.2f mm, y = %2.2f mm, Intensity = %2.4f a.u.&#39; % ((-size/2 + x*size/N)/mm, (-size/2 + y*size/N)/mm, I[int(x)][int(y)]) # plt.show() plt.imshow(MI,cmap=&#39;jet&#39;); plt.axis(&#39;off&#39;);plt.title(&#39;intensity pattern&#39;) plt.show() w=interact(MI, wavelength=widgets.FloatSlider(description=&#39;wavelength of HeNe laser [nm]&#39;,min=400,max=1500,step=1,value=632.0,continuous_update=False,readout_format=&#39;.3f&#39;), size=widgets.FloatSlider(description=&#39;size of the grid [mm] &#39;,min=10,max=50,step=10,value=10,continuous_update=False), N=widgets.FloatSlider(description=&#39;number (NxN) of grid pixels &#39;,min=1,max=500,step=10,value=300,continuous_update=False), R=widgets.FloatSlider(description=&#39;Laser beam Radius [mm] &#39;,min=1,max=5,step=0.1,value=3,continuous_update=False), z1=widgets.FloatSlider(description=&#39;Distance between M1(fixed) and Beam spiltter [cm] &#39;,min=3,max=12,step=0.1,value=8,continuous_update=False), z2=widgets.FloatSlider(description=&#39;Distance between M2(Moving) and Beam spiltter [cm] &#39;,min=3,max=12,step=0.1,value=8,continuous_update=False), z3=widgets.FloatSlider(description=&#39;Distance between Laser and Beam spiltter [cm] &#39;,min=3,max=10,step=0.1,value=3,continuous_update=False), z4=widgets.FloatSlider(description=&#39;Distance between Screen and Beam spiltter [cm] &#39;,min=3,max=10,step=0.1,value=3,continuous_update=False), Rbs=widgets.FloatSlider(description=&#39;Reflection coeff of beam splitter [%] &#39;,min=0.1,max=1,step=0.1,value=0.5,continuous_update=False), tx=widgets.FloatSlider(description=&#39;M2 tilt in x direction [0 to 1] &#39;,min=0,max=1,step=0.1,value=0,continuous_update=False), ty=widgets.FloatSlider(description=&#39;M2 tilt in y direction [0 to 1] &#39;,min=0,max=1,step=0.1,value=0,continuous_update=False), f=widgets.FloatSlider(description=&#39;focal length of positive lens [cm] &#39;,min=10,max=100,step=5,value=50,continuous_update=False), ); . . You will get this widget where you can play around the settings of mirror movement to visualize the fringe pattern. . Here is some of the fringe pattern results when we move the moving mirror. . . That&#39;s a nice view of fringe pattern isn&#39;t it. well hope you have enjoyed the theory and some code practice of most fascinating Michelson interferometer. . Hope u enjoyed the information‚Ä¶. happy reading‚Ä¶.. And, Don‚Äôt forget to give your üëè ! .",
            "url": "https://millermuttu.github.io/blog/jupyter/python/ftir/spectroscopy/basics/2022/10/01/Understanding-FTIR-with-Michelson-interferometer-Theory-Codes-explained.html",
            "relUrl": "/jupyter/python/ftir/spectroscopy/basics/2022/10/01/Understanding-FTIR-with-Michelson-interferometer-Theory-Codes-explained.html",
            "date": " ‚Ä¢ Oct 1, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Data science revision - chapter 9",
            "content": ". Chapter 9: Advanced Data Wrangling With Pandas . Chapter Outline . . 1. Working With Strings | 2. Working With Datetimes | 3. Hierachical Indexing | 4. Visualizing DataFrames | 5. Pandas Profiling | . Chapter Learning Objectives . Manipulate strings in Pandas by accessing methods from the Series.str attribute. | Understand how to use regular expressions in Pandas for wrangling strings. | Differentiate between datetime object in Pandas such as Timestamp, Timedelta, Period, DateOffset. | Create these datetime objects with functions like pd.Timestamp(), pd.Period(), pd.date_range(), pd.period_range(). | Index a datetime index with partial string indexing. | Perform basic datetime operations like splitting a datetime into constituent parts (e.g., year, weekday, second, etc), apply offsets, change timezones, and resample with .resample(). | Make basic plots in Pandas by accessing the .plot attribute or importing functions from pandas.plotting. | . 1. Working With Strings . import pandas as pd import numpy as np pd.set_option(&quot;display.max_rows&quot;, 20) . Working with text data is common in data science. Luckily, Pandas Series and Index objects are equipped with a set of string processing methods which we&#39;ll explore here. . String dtype . String data is represented in pandas using the object dtype, which is a generic dtype for representing mixed data or data of unknown size. It would be better to have a dedicated dtype and Pandas has just introduced this: the StringDtype. object remains the default dtype for strings however, as Pandas looks to continue testing and improving the string dtype. You can read more about the StringDtype in the Pandas documentation here. . String Methods . We&#39;ve seen how libraries like NumPy and Pandas can vectorise operations for increased speed and useability: . x = np.array([1, 2, 3, 4, 5]) x * 2 . array([ 2, 4, 6, 8, 10]) . This is not the case for arrays of strings however: . x = np.array([&#39;Tom&#39;, &#39;Mike&#39;, &#39;Tiffany&#39;, &#39;Joel&#39;, &#39;Varada&#39;]) x.upper() . AttributeError Traceback (most recent call last) &lt;ipython-input-3-acaefb05bf10&gt; in &lt;module&gt; 1 x = np.array([&#39;Tom&#39;, &#39;Mike&#39;, &#39;Tiffany&#39;, &#39;Joel&#39;, &#39;Varada&#39;]) -&gt; 2 x.upper() AttributeError: &#39;numpy.ndarray&#39; object has no attribute &#39;upper&#39; . Instead, you would have to operate on each string object one at a time, using a loop for example: . [name.upper() for name in x] . [&#39;TOM&#39;, &#39;MIKE&#39;, &#39;TIFFANY&#39;, &#39;JOEL&#39;, &#39;VARADA&#39;] . But even this will fail if your array contains a missing value: . x = np.array([&#39;Tom&#39;, &#39;Mike&#39;, None, &#39;Tiffany&#39;, &#39;Joel&#39;, &#39;Varada&#39;]) [name.upper() for name in x] . AttributeError Traceback (most recent call last) &lt;ipython-input-5-b687bbdcc894&gt; in &lt;module&gt; 1 x = np.array([&#39;Tom&#39;, &#39;Mike&#39;, None, &#39;Tiffany&#39;, &#39;Joel&#39;, &#39;Varada&#39;]) -&gt; 2 [name.upper() for name in x] &lt;ipython-input-5-b687bbdcc894&gt; in &lt;listcomp&gt;(.0) 1 x = np.array([&#39;Tom&#39;, &#39;Mike&#39;, None, &#39;Tiffany&#39;, &#39;Joel&#39;, &#39;Varada&#39;]) -&gt; 2 [name.upper() for name in x] AttributeError: &#39;NoneType&#39; object has no attribute &#39;upper&#39; . Pandas addresses both of these issues (vectorization and missing values) with its string methods. String methods can be accessed by the .str attribute of Pandas Series and Index objects. Pretty much all built-in string operations (.upper(), .lower(), .split(), etc) and more are available. . s = pd.Series(x) s . 0 Tom 1 Mike 2 None 3 Tiffany 4 Joel 5 Varada dtype: object . s.str.upper() . 0 TOM 1 MIKE 2 None 3 TIFFANY 4 JOEL 5 VARADA dtype: object . s.str.split(&quot;ff&quot;, expand=True) . 0 1 . 0 Tom | None | . 1 Mike | None | . 2 None | None | . 3 Ti | any | . 4 Joel | None | . 5 Varada | None | . s.str.len() . 0 3.0 1 4.0 2 NaN 3 7.0 4 4.0 5 6.0 dtype: float64 . We can also operate on Index objects (i.e., index or column labels): . df = pd.DataFrame(np.random.rand(5, 3), columns = [&#39;Measured Feature&#39;, &#39;recorded feature&#39;, &#39;PredictedFeature&#39;], index = [f&quot;ROW{_}&quot; for _ in range(5)]) df . Measured Feature recorded feature PredictedFeature . ROW0 0.200112 | 0.790722 | 0.655438 | . ROW1 0.866312 | 0.941067 | 0.179676 | . ROW2 0.478350 | 0.844712 | 0.983463 | . ROW3 0.028143 | 0.120413 | 0.396831 | . ROW4 0.941455 | 0.526084 | 0.731475 | . type(df.columns) . pandas.core.indexes.base.Index . Let&#39;s clean up those labels by: . Removing the word &quot;feature&quot; and &quot;Feature&quot; | Lowercase the &quot;ROW&quot; and add an underscore between the digit and letters | df.columns = df.columns.str.capitalize().str.replace(&quot;feature&quot;, &quot;&quot;).str.strip() . df.index = df.index.str.lower().str.replace(&quot;w&quot;, &quot;w_&quot;) . df . Measured Recorded Predicted . row_0 0.200112 | 0.790722 | 0.655438 | . row_1 0.866312 | 0.941067 | 0.179676 | . row_2 0.478350 | 0.844712 | 0.983463 | . row_3 0.028143 | 0.120413 | 0.396831 | . row_4 0.941455 | 0.526084 | 0.731475 | . Great that worked! There are so many string operations you can use in Pandas. Here&#39;s a full list of all the string methods available in Pandas that I pulled from the documentation: . Method Description . Series.str.cat | Concatenate strings | . Series.str.split | Split strings on delimiter | . Series.str.rsplit | Split strings on delimiter working from the end of the string | . Series.str.get | Index into each element (retrieve i-th element) | . Series.str.join | Join strings in each element of the Series with passed separator | . Series.str.get_dummies | Split strings on the delimiter returning DataFrame of dummy variables | . Series.str.contains | Return boolean array if each string contains pattern/regex | . Series.str.replace | Replace occurrences of pattern/regex/string with some other string or the return value of a callable given the occurrence | . Series.str.repeat | Duplicate values (s.str.repeat(3) equivalent to x * 3) | . Series.str.pad | &quot;Add whitespace to left, right, or both sides of strings&quot; | . Series.str.center | Equivalent to str.center | . Series.str.ljust | Equivalent to str.ljust | . Series.str.rjust | Equivalent to str.rjust | . Series.str.zfill | Equivalent to str.zfill | . Series.str.wrap | Split long strings into lines with length less than a given width | . Series.str.slice | Slice each string in the Series | . Series.str.slice_replace | Replace slice in each string with passed value | . Series.str.count | Count occurrences of pattern | . Series.str.startswith | Equivalent to str.startswith(pat) for each element | . Series.str.endswith | Equivalent to str.endswith(pat) for each element | . Series.str.findall | Compute list of all occurrences of pattern/regex for each string | . Series.str.match | &quot;Call re.match on each element, returning matched groups as list&quot; | . Series.str.extract | &quot;Call re.search on each element, returning DataFrame with one row for each element and one column for each regex capture group&quot; | . Series.str.extractall | &quot;Call re.findall on each element, returning DataFrame with one row for each match and one column for each regex capture group&quot; | . Series.str.len | Compute string lengths | . Series.str.strip | Equivalent to str.strip | . Series.str.rstrip | Equivalent to str.rstrip | . Series.str.lstrip | Equivalent to str.lstrip | . Series.str.partition | Equivalent to str.partition | . Series.str.rpartition | Equivalent to str.rpartition | . Series.str.lower | Equivalent to str.lower | . Series.str.casefold | Equivalent to str.casefold | . Series.str.upper | Equivalent to str.upper | . Series.str.find | Equivalent to str.find | . Series.str.rfind | Equivalent to str.rfind | . Series.str.index | Equivalent to str.index | . Series.str.rindex | Equivalent to str.rindex | . Series.str.capitalize | Equivalent to str.capitalize | . Series.str.swapcase | Equivalent to str.swapcase | . Series.str.normalize | Return Unicode normal form. Equivalent to unicodedata.normalize | . Series.str.translate | Equivalent to str.translate | . Series.str.isalnum | Equivalent to str.isalnum | . Series.str.isalpha | Equivalent to str.isalpha | . Series.str.isdigit | Equivalent to str.isdigit | . Series.str.isspace | Equivalent to str.isspace | . Series.str.islower | Equivalent to str.islower | . Series.str.isupper | Equivalent to str.isupper | . Series.str.istitle | Equivalent to str.istitle | . Series.str.isnumeric | Equivalent to str.isnumeric | . Series.str.isdecimal | Equivalent to str.isdecimal | . I will also mention that I often use the dataframe method df.replace() to do string replacements: . df = pd.DataFrame({&#39;col1&#39;: [&#39;replace me&#39;, &#39;b&#39;, &#39;c&#39;], &#39;col2&#39;: [1, 99999, 3]}) df . col1 col2 . 0 replace me | 1 | . 1 b | 99999 | . 2 c | 3 | . df.replace({&#39;replace me&#39;: &#39;a&#39;, 99999: 2}) . col1 col2 . 0 a | 1 | . 1 b | 2 | . 2 c | 3 | . Regular Expressions . A regular expression (regex) is a sequence of characters that defines a search pattern. For more complex string operations, you&#39;ll definitely want to use regex. Here&#39;s a great cheatsheet of regular expression syntax. I am self-admittedly not a regex expert, I usually jump over to RegExr.com and play around until I find the expression I want. Many Pandas string functions accept regular expressions as input, these are the ones I use most often: . Method Description . match() | Call re.match() on each element, returning a boolean. | . extract() | Call re.match() on each element, returning matched groups as strings. | . findall() | Call re.findall() on each element | . replace() | Replace occurrences of pattern with some other string | . contains() | Call re.search() on each element, returning a boolean | . count() | Count occurrences of pattern | . split() | Equivalent to str.split(), but accepts regexps | . rsplit() | Equivalent to str.rsplit(), but accepts regexps | . For example, we can easily find all names in our Series that start and end with a consonant: . s = pd.Series([&#39;Tom&#39;, &#39;Mike&#39;, None, &#39;Tiffany&#39;, &#39;Joel&#39;, &#39;Varada&#39;]) s . 0 Tom 1 Mike 2 None 3 Tiffany 4 Joel 5 Varada dtype: object . s.str.findall(r&#39;^[^AEIOU].*[^aeiou]$&#39;) . 0 [Tom] 1 [] 2 None 3 [Tiffany] 4 [Joel] 5 [] dtype: object . Let&#39;s break down that regex: . Part Description . ^ | Specifies the start of a string | . [^AEIOU] | Square brackets match a single character. When ^ is used inside square brackets it means &quot;not&quot;, so we are are saying, &quot;the first character of the string should not be A, E, I, O, or U (i.e., a vowel)&quot; | . .* | . matches any character and * means &quot;0 or more time&quot;, this is basically saying that we can have any number of characters in the middle of our string | . [^aeiou]$ | $ matches the end of the string, so we are saying, we don&#39;t want the last character to be a lowercase vowel | . Regex can do some truly magical things so keep it in mind when you&#39;re doing complicated text wrangling. Let&#39;s see one more example on the cycling dataset: . df = pd.read_csv(&#39;data/cycling_data.csv&#39;, index_col=0) df . Name Type Time Distance Comments . Date . 10 Sep 2019, 00:13:04 Afternoon Ride | Ride | 2084 | 12.62 | Rain | . 10 Sep 2019, 13:52:18 Morning Ride | Ride | 2531 | 13.03 | rain | . 11 Sep 2019, 00:23:50 Afternoon Ride | Ride | 1863 | 12.52 | Wet road but nice weather | . 11 Sep 2019, 14:06:19 Morning Ride | Ride | 2192 | 12.84 | Stopped for photo of sunrise | . 12 Sep 2019, 00:28:05 Afternoon Ride | Ride | 1891 | 12.48 | Tired by the end of the week | . ... ... | ... | ... | ... | ... | . 4 Oct 2019, 01:08:08 Afternoon Ride | Ride | 1870 | 12.63 | Very tired, riding into the wind | . 9 Oct 2019, 13:55:40 Morning Ride | Ride | 2149 | 12.70 | Really cold! But feeling good | . 10 Oct 2019, 00:10:31 Afternoon Ride | Ride | 1841 | 12.59 | Feeling good after a holiday break! | . 10 Oct 2019, 13:47:14 Morning Ride | Ride | 2463 | 12.79 | Stopped for photo of sunrise | . 11 Oct 2019, 00:16:57 Afternoon Ride | Ride | 1843 | 11.79 | Bike feeling tight, needs an oil and pump | . 33 rows √ó 5 columns . We could find all the comments that contains the string &quot;Rain&quot; or &quot;rain&quot;: . df.loc[df[&#39;Comments&#39;].str.contains(r&quot;[Rr]ain&quot;)] . Name Type Time Distance Comments . Date . 10 Sep 2019, 00:13:04 Afternoon Ride | Ride | 2084 | 12.62 | Rain | . 10 Sep 2019, 13:52:18 Morning Ride | Ride | 2531 | 13.03 | rain | . 17 Sep 2019, 13:43:34 Morning Ride | Ride | 2285 | 12.60 | Raining | . 18 Sep 2019, 13:49:53 Morning Ride | Ride | 2903 | 14.57 | Raining today | . 26 Sep 2019, 00:13:33 Afternoon Ride | Ride | 1860 | 12.52 | raining | . If we didn&#39;t want to include &quot;Raining&quot; or &quot;raining&quot;, we could do: . df.loc[df[&#39;Comments&#39;].str.contains(r&quot;^[Rr]ain$&quot;)] . Name Type Time Distance Comments . Date . 10 Sep 2019, 00:13:04 Afternoon Ride | Ride | 2084 | 12.62 | Rain | . 10 Sep 2019, 13:52:18 Morning Ride | Ride | 2531 | 13.03 | rain | . We can even split strings and separate them into new columns, for example, based on punctuation: . df[&#39;Comments&#39;].str.split(r&quot;[.,!]&quot;, expand=True) . 0 1 . Date . 10 Sep 2019, 00:13:04 Rain | None | . 10 Sep 2019, 13:52:18 rain | None | . 11 Sep 2019, 00:23:50 Wet road but nice weather | None | . 11 Sep 2019, 14:06:19 Stopped for photo of sunrise | None | . 12 Sep 2019, 00:28:05 Tired by the end of the week | None | . ... ... | ... | . 4 Oct 2019, 01:08:08 Very tired | riding into the wind | . 9 Oct 2019, 13:55:40 Really cold | But feeling good | . 10 Oct 2019, 00:10:31 Feeling good after a holiday break | | . 10 Oct 2019, 13:47:14 Stopped for photo of sunrise | None | . 11 Oct 2019, 00:16:57 Bike feeling tight | needs an oil and pump | . 33 rows √ó 2 columns . My point being here that you can pretty much do anything your heart desires! . 2. Working With Datetimes . Just like with strings, Pandas has extensive functionality for working with time series data. . Datetime dtype and Motivation for Using Pandas . Python has built-in support for datetime format, that is, an object that contains time and date information, in the datetime module. . from datetime import datetime, timedelta . date = datetime(year=2005, month=7, day=9, hour=13, minute=54) date . datetime.datetime(2005, 7, 9, 13, 54) . We can also parse directly from a string, see format codes here: . date = datetime.strptime(&quot;July 9 2005, 13:54&quot;, &quot;%B %d %Y, %H:%M&quot;) date . datetime.datetime(2005, 7, 9, 13, 54) . We can then extract specific information from our data: . print(f&quot;Year: {date.strftime(&#39;%Y&#39;)}&quot;) print(f&quot;Month: {date.strftime(&#39;%B&#39;)}&quot;) print(f&quot;Day: {date.strftime(&#39;%d&#39;)}&quot;) print(f&quot;Day name: {date.strftime(&#39;%A&#39;)}&quot;) print(f&quot;Day of year: {date.strftime(&#39;%j&#39;)}&quot;) print(f&quot;Time of day: {date.strftime(&#39;%p&#39;)}&quot;) . Year: 2005 Month: July Day: 09 Day name: Saturday Day of year: 190 Time of day: PM . And perform basic operations, like adding a week: . date + timedelta(days=7) . datetime.datetime(2005, 7, 16, 13, 54) . But as with strings, working with arrays of datetimes in Python can be difficult and inefficient. NumPy, therefore included a new datetime object to work more effectively with dates: . dates = np.array([&quot;2020-07-09&quot;, &quot;2020-08-10&quot;], dtype=&quot;datetime64&quot;) dates . array([&#39;2020-07-09&#39;, &#39;2020-08-10&#39;], dtype=&#39;datetime64[D]&#39;) . We can create arrays using other built-in functions like np.arange() too: . dates = np.arange(&quot;2020-07&quot;, &quot;2020-12&quot;, dtype=&#39;datetime64[M]&#39;) dates . array([&#39;2020-07&#39;, &#39;2020-08&#39;, &#39;2020-09&#39;, &#39;2020-10&#39;, &#39;2020-11&#39;], dtype=&#39;datetime64[M]&#39;) . Now we can easily do operations on arrays of time. You can check out all the datetime units and their format in the documentation here. . dates + np.timedelta64(2, &#39;M&#39;) . array([&#39;2020-09&#39;, &#39;2020-10&#39;, &#39;2020-11&#39;, &#39;2020-12&#39;, &#39;2021-01&#39;], dtype=&#39;datetime64[M]&#39;) . But while numpy helps bring datetimes into the array world, it&#39;s missing a lot of functionality that we would commonly want/need for wrangling tasks. This is where Pandas comes in. Pandas consolidates and extends functionality from the datetime module, numpy, and other libraries like scikits.timeseries into a single place. Pandas provides 4 key datetime objects which we&#39;ll explore in the following sections: . Timestamp (like np.datetime64) | Timedelta (like np.timedelta64) | Period (custom object for regular ranges of datetimes) | DateOffset (custom object like timedelta but factoring in calendar rules) | Creating Datetimes . From scratch . Most commonly you&#39;ll want to: . Create a single point in time with pd.Timestamp(), e.g., 2005-07-09 00:00:00 | Create a span of time with pd.Period(), e.g., 2020 Jan | Create an array of datetimes with pd.date_range() or pd.period_range() | print(pd.Timestamp(&#39;2005-07-09&#39;)) # parsed from string print(pd.Timestamp(year=2005, month=7, day=9)) # pass data directly print(pd.Timestamp(datetime(year=2005, month=7, day=9))) # from datetime object . 2005-07-09 00:00:00 2005-07-09 00:00:00 2005-07-09 00:00:00 . The above is a specific point in time. Below, we can use pd.Period() to specify a span of time (like a day): . span = pd.Period(&#39;2005-07-09&#39;) print(span) print(span.start_time) print(span.end_time) . 2005-07-09 2005-07-09 00:00:00 2005-07-09 23:59:59.999999999 . point = pd.Timestamp(&#39;2005-07-09 12:00&#39;) span = pd.Period(&#39;2005-07-09&#39;) print(f&quot;Point: {point}&quot;) print(f&quot; Span: {span}&quot;) print(f&quot;Point in span? {span.start_time &lt; point &lt; span.end_time}&quot;) . Point: 2005-07-09 12:00:00 Span: 2005-07-09 Point in span? True . Often, you&#39;ll want to create arrays of datetimes, not just single values. Arrays of datetimes are of the class DatetimeIndex/PeriodIndex/TimedeltaIndex: . pd.date_range(&#39;2020-09-01 12:00&#39;, &#39;2020-09-11 12:00&#39;, freq=&#39;D&#39;) . DatetimeIndex([&#39;2020-09-01 12:00:00&#39;, &#39;2020-09-02 12:00:00&#39;, &#39;2020-09-03 12:00:00&#39;, &#39;2020-09-04 12:00:00&#39;, &#39;2020-09-05 12:00:00&#39;, &#39;2020-09-06 12:00:00&#39;, &#39;2020-09-07 12:00:00&#39;, &#39;2020-09-08 12:00:00&#39;, &#39;2020-09-09 12:00:00&#39;, &#39;2020-09-10 12:00:00&#39;, &#39;2020-09-11 12:00:00&#39;], dtype=&#39;datetime64[ns]&#39;, freq=&#39;D&#39;) . pd.period_range(&#39;2020-09-01&#39;, &#39;2020-09-11&#39;, freq=&#39;D&#39;) . PeriodIndex([&#39;2020-09-01&#39;, &#39;2020-09-02&#39;, &#39;2020-09-03&#39;, &#39;2020-09-04&#39;, &#39;2020-09-05&#39;, &#39;2020-09-06&#39;, &#39;2020-09-07&#39;, &#39;2020-09-08&#39;, &#39;2020-09-09&#39;, &#39;2020-09-10&#39;, &#39;2020-09-11&#39;], dtype=&#39;period[D]&#39;, freq=&#39;D&#39;) . We can use Timedelta objects to perform temporal operations like adding or subtracting time: . pd.date_range(&#39;2020-09-01 12:00&#39;, &#39;2020-09-11 12:00&#39;, freq=&#39;D&#39;) + pd.Timedelta(&#39;1.5 hour&#39;) . DatetimeIndex([&#39;2020-09-01 13:30:00&#39;, &#39;2020-09-02 13:30:00&#39;, &#39;2020-09-03 13:30:00&#39;, &#39;2020-09-04 13:30:00&#39;, &#39;2020-09-05 13:30:00&#39;, &#39;2020-09-06 13:30:00&#39;, &#39;2020-09-07 13:30:00&#39;, &#39;2020-09-08 13:30:00&#39;, &#39;2020-09-09 13:30:00&#39;, &#39;2020-09-10 13:30:00&#39;, &#39;2020-09-11 13:30:00&#39;], dtype=&#39;datetime64[ns]&#39;, freq=&#39;D&#39;) . Finally, Pandas represents missing datetimes with NaT, which is just like np.nan: . pd.Timestamp(pd.NaT) . NaT . By converting existing data . It&#39;s fairly common to have an array of dates as strings. We can use pd.to_datetime() to convert these to datetime: . string_dates = [&#39;July 9, 2020&#39;, &#39;August 1, 2020&#39;, &#39;August 28, 2020&#39;] string_dates . [&#39;July 9, 2020&#39;, &#39;August 1, 2020&#39;, &#39;August 28, 2020&#39;] . pd.to_datetime(string_dates) . DatetimeIndex([&#39;2020-07-09&#39;, &#39;2020-08-01&#39;, &#39;2020-08-28&#39;], dtype=&#39;datetime64[ns]&#39;, freq=None) . For more complex datetime format, use the format argument (see Python Format Codes for help): . string_dates = [&#39;2020 9 July&#39;, &#39;2020 1 August&#39;, &#39;2020 28 August&#39;] pd.to_datetime(string_dates, format=&quot;%Y %d %B&quot;) . DatetimeIndex([&#39;2020-07-09&#39;, &#39;2020-08-01&#39;, &#39;2020-08-28&#39;], dtype=&#39;datetime64[ns]&#39;, freq=None) . Or use a dictionary: . dict_dates = pd.to_datetime({&quot;year&quot;: [2020, 2020, 2020], &quot;month&quot;: [7, 8, 8], &quot;day&quot;: [9, 1, 28]}) # note this is a series, not an index! dict_dates . 0 2020-07-09 1 2020-08-01 2 2020-08-28 dtype: datetime64[ns] . pd.Index(dict_dates) . DatetimeIndex([&#39;2020-07-09&#39;, &#39;2020-08-01&#39;, &#39;2020-08-28&#39;], dtype=&#39;datetime64[ns]&#39;, freq=None) . By reading directly from an external source . Let&#39;s practice by reading in our favourite cycling dataset: . df = pd.read_csv(&#39;data/cycling_data.csv&#39;, index_col=0) df . Name Type Time Distance Comments . Date . 10 Sep 2019, 00:13:04 Afternoon Ride | Ride | 2084 | 12.62 | Rain | . 10 Sep 2019, 13:52:18 Morning Ride | Ride | 2531 | 13.03 | rain | . 11 Sep 2019, 00:23:50 Afternoon Ride | Ride | 1863 | 12.52 | Wet road but nice weather | . 11 Sep 2019, 14:06:19 Morning Ride | Ride | 2192 | 12.84 | Stopped for photo of sunrise | . 12 Sep 2019, 00:28:05 Afternoon Ride | Ride | 1891 | 12.48 | Tired by the end of the week | . ... ... | ... | ... | ... | ... | . 4 Oct 2019, 01:08:08 Afternoon Ride | Ride | 1870 | 12.63 | Very tired, riding into the wind | . 9 Oct 2019, 13:55:40 Morning Ride | Ride | 2149 | 12.70 | Really cold! But feeling good | . 10 Oct 2019, 00:10:31 Afternoon Ride | Ride | 1841 | 12.59 | Feeling good after a holiday break! | . 10 Oct 2019, 13:47:14 Morning Ride | Ride | 2463 | 12.79 | Stopped for photo of sunrise | . 11 Oct 2019, 00:16:57 Afternoon Ride | Ride | 1843 | 11.79 | Bike feeling tight, needs an oil and pump | . 33 rows √ó 5 columns . Our index is just a plain old index at the moment, with dtype object, full of string dates: . print(df.index.dtype) type(df.index) . object . pandas.core.indexes.base.Index . We could manually convert our index to a datetime using pd.to_datetime(). But even better, pd.read_csv() has an argument parse_dates which can do this automatically when reading the file: . df = pd.read_csv(&#39;data/cycling_data.csv&#39;, index_col=0, parse_dates=True) df . Name Type Time Distance Comments . Date . 2019-09-10 00:13:04 Afternoon Ride | Ride | 2084 | 12.62 | Rain | . 2019-09-10 13:52:18 Morning Ride | Ride | 2531 | 13.03 | rain | . 2019-09-11 00:23:50 Afternoon Ride | Ride | 1863 | 12.52 | Wet road but nice weather | . 2019-09-11 14:06:19 Morning Ride | Ride | 2192 | 12.84 | Stopped for photo of sunrise | . 2019-09-12 00:28:05 Afternoon Ride | Ride | 1891 | 12.48 | Tired by the end of the week | . ... ... | ... | ... | ... | ... | . 2019-10-04 01:08:08 Afternoon Ride | Ride | 1870 | 12.63 | Very tired, riding into the wind | . 2019-10-09 13:55:40 Morning Ride | Ride | 2149 | 12.70 | Really cold! But feeling good | . 2019-10-10 00:10:31 Afternoon Ride | Ride | 1841 | 12.59 | Feeling good after a holiday break! | . 2019-10-10 13:47:14 Morning Ride | Ride | 2463 | 12.79 | Stopped for photo of sunrise | . 2019-10-11 00:16:57 Afternoon Ride | Ride | 1843 | 11.79 | Bike feeling tight, needs an oil and pump | . 33 rows √ó 5 columns . type(df.index) . pandas.core.indexes.datetimes.DatetimeIndex . print(df.index.dtype) type(df.index) . datetime64[ns] . pandas.core.indexes.datetimes.DatetimeIndex . The parse_dates argument is very flexible and you can specify the datetime format for harder to read dates. There are other related arguments like date_parser, dayfirst, etc that are also helpful, check out the Pandas documentation for more. . Indexing Datetimes . Datetime index objects are just like regular Index objects and can be selected, sliced, filtered, etc. . df . Name Type Time Distance Comments . Date . 2019-09-10 00:13:04 Afternoon Ride | Ride | 2084 | 12.62 | Rain | . 2019-09-10 13:52:18 Morning Ride | Ride | 2531 | 13.03 | rain | . 2019-09-11 00:23:50 Afternoon Ride | Ride | 1863 | 12.52 | Wet road but nice weather | . 2019-09-11 14:06:19 Morning Ride | Ride | 2192 | 12.84 | Stopped for photo of sunrise | . 2019-09-12 00:28:05 Afternoon Ride | Ride | 1891 | 12.48 | Tired by the end of the week | . ... ... | ... | ... | ... | ... | . 2019-10-04 01:08:08 Afternoon Ride | Ride | 1870 | 12.63 | Very tired, riding into the wind | . 2019-10-09 13:55:40 Morning Ride | Ride | 2149 | 12.70 | Really cold! But feeling good | . 2019-10-10 00:10:31 Afternoon Ride | Ride | 1841 | 12.59 | Feeling good after a holiday break! | . 2019-10-10 13:47:14 Morning Ride | Ride | 2463 | 12.79 | Stopped for photo of sunrise | . 2019-10-11 00:16:57 Afternoon Ride | Ride | 1843 | 11.79 | Bike feeling tight, needs an oil and pump | . 33 rows √ó 5 columns . We can do partial string indexing: . df.loc[&#39;2019-09&#39;] . Name Type Time Distance Comments . Date . 2019-09-10 00:13:04 Afternoon Ride | Ride | 2084 | 12.62 | Rain | . 2019-09-10 13:52:18 Morning Ride | Ride | 2531 | 13.03 | rain | . 2019-09-11 00:23:50 Afternoon Ride | Ride | 1863 | 12.52 | Wet road but nice weather | . 2019-09-11 14:06:19 Morning Ride | Ride | 2192 | 12.84 | Stopped for photo of sunrise | . 2019-09-12 00:28:05 Afternoon Ride | Ride | 1891 | 12.48 | Tired by the end of the week | . ... ... | ... | ... | ... | ... | . 2019-09-25 13:35:41 Morning Ride | Ride | 2124 | 12.65 | Stopped for photo of sunrise | . 2019-09-26 00:13:33 Afternoon Ride | Ride | 1860 | 12.52 | raining | . 2019-09-26 13:42:43 Morning Ride | Ride | 2350 | 12.91 | Detour around trucks at Jericho | . 2019-09-27 01:00:18 Afternoon Ride | Ride | 1712 | 12.47 | Tired by the end of the week | . 2019-09-30 13:53:52 Morning Ride | Ride | 2118 | 12.71 | Rested after the weekend! | . 22 rows √ó 5 columns . Exact matching: . df.loc[&#39;2019-10-10&#39;] . Name Type Time Distance Comments . Date . 2019-10-10 00:10:31 Afternoon Ride | Ride | 1841 | 12.59 | Feeling good after a holiday break! | . 2019-10-10 13:47:14 Morning Ride | Ride | 2463 | 12.79 | Stopped for photo of sunrise | . df.loc[&#39;2019-10-10 13:47:14&#39;] . Name Type Time Distance Comments . Date . 2019-10-10 13:47:14 Morning Ride | Ride | 2463 | 12.79 | Stopped for photo of sunrise | . And slicing: . df.loc[&#39;2019-10-01&#39;:&#39;2019-10-13&#39;] . Name Type Time Distance Comments . Date . 2019-10-01 00:15:07 Afternoon Ride | Ride | 1732 | NaN | Legs feeling strong! | . 2019-10-01 13:45:55 Morning Ride | Ride | 2222 | 12.82 | Beautiful morning! Feeling fit | . 2019-10-02 00:13:09 Afternoon Ride | Ride | 1756 | NaN | A little tired today but good weather | . 2019-10-02 13:46:06 Morning Ride | Ride | 2134 | 13.06 | Bit tired today but good weather | . 2019-10-03 00:45:22 Afternoon Ride | Ride | 1724 | 12.52 | Feeling good | . 2019-10-03 13:47:36 Morning Ride | Ride | 2182 | 12.68 | Wet road | . 2019-10-04 01:08:08 Afternoon Ride | Ride | 1870 | 12.63 | Very tired, riding into the wind | . 2019-10-09 13:55:40 Morning Ride | Ride | 2149 | 12.70 | Really cold! But feeling good | . 2019-10-10 00:10:31 Afternoon Ride | Ride | 1841 | 12.59 | Feeling good after a holiday break! | . 2019-10-10 13:47:14 Morning Ride | Ride | 2463 | 12.79 | Stopped for photo of sunrise | . 2019-10-11 00:16:57 Afternoon Ride | Ride | 1843 | 11.79 | Bike feeling tight, needs an oil and pump | . df.query() will also work here: . df.query(&quot;&#39;2019-10-10&#39;&quot;) . Name Type Time Distance Comments . Date . 2019-10-10 00:10:31 Afternoon Ride | Ride | 1841 | 12.59 | Feeling good after a holiday break! | . 2019-10-10 13:47:14 Morning Ride | Ride | 2463 | 12.79 | Stopped for photo of sunrise | . And for getting all results between two times of a day, use df.between_time(): . df.between_time(&#39;00:00&#39;, &#39;01:00&#39;) . Name Type Time Distance Comments . Date . 2019-09-10 00:13:04 Afternoon Ride | Ride | 2084 | 12.62 | Rain | . 2019-09-11 00:23:50 Afternoon Ride | Ride | 1863 | 12.52 | Wet road but nice weather | . 2019-09-12 00:28:05 Afternoon Ride | Ride | 1891 | 12.48 | Tired by the end of the week | . 2019-09-17 00:15:47 Afternoon Ride | Ride | 1973 | 12.45 | Legs feeling strong! | . 2019-09-18 00:15:52 Afternoon Ride | Ride | 2101 | 12.48 | Pumped up tires | . 2019-09-19 00:30:01 Afternoon Ride | Ride | 48062 | 12.48 | Feeling good | . 2019-09-24 00:35:42 Afternoon Ride | Ride | 2076 | 12.47 | Oiled chain, bike feels smooth | . 2019-09-25 00:07:21 Afternoon Ride | Ride | 1775 | 12.10 | Feeling really tired | . 2019-09-26 00:13:33 Afternoon Ride | Ride | 1860 | 12.52 | raining | . 2019-10-01 00:15:07 Afternoon Ride | Ride | 1732 | NaN | Legs feeling strong! | . 2019-10-02 00:13:09 Afternoon Ride | Ride | 1756 | NaN | A little tired today but good weather | . 2019-10-03 00:45:22 Afternoon Ride | Ride | 1724 | 12.52 | Feeling good | . 2019-10-10 00:10:31 Afternoon Ride | Ride | 1841 | 12.59 | Feeling good after a holiday break! | . 2019-10-11 00:16:57 Afternoon Ride | Ride | 1843 | 11.79 | Bike feeling tight, needs an oil and pump | . For more complicated filtering, we may have to decompose our timeseries, as we&#39;ll shown in the next section. . Manipulating Datetimes . Decomposition . We can easily decompose our timeseries into its constituent components. There are many attributes that define these constituents. . df.index.year . Int64Index([2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019], dtype=&#39;int64&#39;, name=&#39;Date&#39;) . df.index.second . Int64Index([ 4, 18, 50, 19, 5, 48, 47, 34, 53, 52, 1, 9, 5, 41, 42, 24, 21, 41, 33, 43, 18, 52, 7, 55, 9, 6, 22, 36, 8, 40, 31, 14, 57], dtype=&#39;int64&#39;, name=&#39;Date&#39;) . df.index.weekday . Int64Index([1, 1, 2, 2, 3, 0, 1, 1, 2, 2, 3, 3, 4, 0, 1, 1, 2, 2, 3, 3, 4, 0, 1, 1, 2, 2, 3, 3, 4, 2, 3, 3, 4], dtype=&#39;int64&#39;, name=&#39;Date&#39;) . As well as methods we can use: . df.index.day_name() . Index([&#39;Tuesday&#39;, &#39;Tuesday&#39;, &#39;Wednesday&#39;, &#39;Wednesday&#39;, &#39;Thursday&#39;, &#39;Monday&#39;, &#39;Tuesday&#39;, &#39;Tuesday&#39;, &#39;Wednesday&#39;, &#39;Wednesday&#39;, &#39;Thursday&#39;, &#39;Thursday&#39;, &#39;Friday&#39;, &#39;Monday&#39;, &#39;Tuesday&#39;, &#39;Tuesday&#39;, &#39;Wednesday&#39;, &#39;Wednesday&#39;, &#39;Thursday&#39;, &#39;Thursday&#39;, &#39;Friday&#39;, &#39;Monday&#39;, &#39;Tuesday&#39;, &#39;Tuesday&#39;, &#39;Wednesday&#39;, &#39;Wednesday&#39;, &#39;Thursday&#39;, &#39;Thursday&#39;, &#39;Friday&#39;, &#39;Wednesday&#39;, &#39;Thursday&#39;, &#39;Thursday&#39;, &#39;Friday&#39;], dtype=&#39;object&#39;, name=&#39;Date&#39;) . df.index.month_name() . Index([&#39;September&#39;, &#39;September&#39;, &#39;September&#39;, &#39;September&#39;, &#39;September&#39;, &#39;September&#39;, &#39;September&#39;, &#39;September&#39;, &#39;September&#39;, &#39;September&#39;, &#39;September&#39;, &#39;September&#39;, &#39;September&#39;, &#39;September&#39;, &#39;September&#39;, &#39;September&#39;, &#39;September&#39;, &#39;September&#39;, &#39;September&#39;, &#39;September&#39;, &#39;September&#39;, &#39;September&#39;, &#39;October&#39;, &#39;October&#39;, &#39;October&#39;, &#39;October&#39;, &#39;October&#39;, &#39;October&#39;, &#39;October&#39;, &#39;October&#39;, &#39;October&#39;, &#39;October&#39;, &#39;October&#39;], dtype=&#39;object&#39;, name=&#39;Date&#39;) . Note that if you&#39;re operating on a Series rather than a DatetimeIndex object, you can access this functionality through the .dt attribute: . s = pd.Series(pd.date_range(&#39;2011-12-29&#39;, &#39;2011-12-31&#39;)) s.year # raises error . AttributeError Traceback (most recent call last) &lt;ipython-input-60-e24fea3644a8&gt; in &lt;module&gt; 1 s = pd.Series(pd.date_range(&#39;2011-12-29&#39;, &#39;2011-12-31&#39;)) -&gt; 2 s.year # raises error /opt/miniconda3/lib/python3.7/site-packages/pandas/core/generic.py in __getattr__(self, name) 5128 if self._info_axis._can_hold_identifiers_and_holds_name(name): 5129 return self[name] -&gt; 5130 return object.__getattribute__(self, name) 5131 5132 def __setattr__(self, name: str, value) -&gt; None: AttributeError: &#39;Series&#39; object has no attribute &#39;year&#39; . s.dt.year # works . 0 2011 1 2011 2 2011 dtype: int64 . Offsets and Timezones . We saw before how we can use Timedelta to add/subtract time to our datetimes. Timedelta respects absolute time, which can be problematic in some cases, where time is not regular. For example, on March 8, Canada daylight savings started and clocks moved forward 1 hour. This extra &quot;calendar hour&quot; is not accounted for in absolute time: . t1 = pd.Timestamp(&#39;2020-03-07 12:00:00&#39;, tz=&#39;Canada/Pacific&#39;) t2 = t1 + pd.Timedelta(&quot;1 day&quot;) print(f&quot;Original time: {t1}&quot;) print(f&quot; Plus one day: {t2}&quot;) # note that time has moved from 12:00 -&gt; 13:00 . Original time: 2020-03-07 12:00:00-08:00 Plus one day: 2020-03-08 13:00:00-07:00 . Instead, we&#39;d need to use a Dateoffset: . t3 = t1 + pd.DateOffset(days=1) print(f&quot;Original time: {t1}&quot;) print(f&quot; Plus one day: {t3}&quot;) # note that time has stayed at 12:00 . Original time: 2020-03-07 12:00:00-08:00 Plus one day: 2020-03-08 12:00:00-07:00 . You can see that we started including timezone information above. By default, datetime objects are &quot;timezone unaware&quot;. To associate times with a timezone, we can use the tz argument in construction, or we can use the tz_localize() method: . print(f&quot; No timezone: {pd.Timestamp(&#39;2020-03-07 12:00:00&#39;).tz}&quot;) print(f&quot; tz arg: {pd.Timestamp(&#39;2020-03-07 12:00:00&#39;, tz=&#39;Canada/Pacific&#39;).tz}&quot;) print(f&quot;.tz_localize method: {pd.Timestamp(&#39;2020-03-07 12:00:00&#39;).tz_localize(&#39;Canada/Pacific&#39;).tz}&quot;) . No timezone: None tz arg: Canada/Pacific .tz_localize method: Canada/Pacific . You can convert between timezones using the .tz_convert() method. You might have noticed something funny about the times I&#39;ve been riding to University: . df = pd.read_csv(&#39;data/cycling_data.csv&#39;, index_col=0, parse_dates=True) df . Name Type Time Distance Comments . Date . 2019-09-10 00:13:04 Afternoon Ride | Ride | 2084 | 12.62 | Rain | . 2019-09-10 13:52:18 Morning Ride | Ride | 2531 | 13.03 | rain | . 2019-09-11 00:23:50 Afternoon Ride | Ride | 1863 | 12.52 | Wet road but nice weather | . 2019-09-11 14:06:19 Morning Ride | Ride | 2192 | 12.84 | Stopped for photo of sunrise | . 2019-09-12 00:28:05 Afternoon Ride | Ride | 1891 | 12.48 | Tired by the end of the week | . ... ... | ... | ... | ... | ... | . 2019-10-04 01:08:08 Afternoon Ride | Ride | 1870 | 12.63 | Very tired, riding into the wind | . 2019-10-09 13:55:40 Morning Ride | Ride | 2149 | 12.70 | Really cold! But feeling good | . 2019-10-10 00:10:31 Afternoon Ride | Ride | 1841 | 12.59 | Feeling good after a holiday break! | . 2019-10-10 13:47:14 Morning Ride | Ride | 2463 | 12.79 | Stopped for photo of sunrise | . 2019-10-11 00:16:57 Afternoon Ride | Ride | 1843 | 11.79 | Bike feeling tight, needs an oil and pump | . 33 rows √ó 5 columns . I know for a fact that I haven&#39;t been cycling around midnight... There&#39;s something wrong with the timezone in this dataset. I was using the Strava app to document my rides, it was recording in Canadian time but converting to Australia time. Let&#39;s go ahead and fix that up: . df.index = df.index.tz_localize(&quot;Canada/Pacific&quot;) # first specify the current timezone df.index = df.index.tz_convert(&quot;Australia/Sydney&quot;) # then convert to the proper timezone df . Name Type Time Distance Comments . Date . 2019-09-10 17:13:04+10:00 Afternoon Ride | Ride | 2084 | 12.62 | Rain | . 2019-09-11 06:52:18+10:00 Morning Ride | Ride | 2531 | 13.03 | rain | . 2019-09-11 17:23:50+10:00 Afternoon Ride | Ride | 1863 | 12.52 | Wet road but nice weather | . 2019-09-12 07:06:19+10:00 Morning Ride | Ride | 2192 | 12.84 | Stopped for photo of sunrise | . 2019-09-12 17:28:05+10:00 Afternoon Ride | Ride | 1891 | 12.48 | Tired by the end of the week | . ... ... | ... | ... | ... | ... | . 2019-10-04 18:08:08+10:00 Afternoon Ride | Ride | 1870 | 12.63 | Very tired, riding into the wind | . 2019-10-10 07:55:40+11:00 Morning Ride | Ride | 2149 | 12.70 | Really cold! But feeling good | . 2019-10-10 18:10:31+11:00 Afternoon Ride | Ride | 1841 | 12.59 | Feeling good after a holiday break! | . 2019-10-11 07:47:14+11:00 Morning Ride | Ride | 2463 | 12.79 | Stopped for photo of sunrise | . 2019-10-11 18:16:57+11:00 Afternoon Ride | Ride | 1843 | 11.79 | Bike feeling tight, needs an oil and pump | . 33 rows √ó 5 columns . We could have also used a DateOffset if we knew the offset we wanted to apply, in this case, 7 hours: . df = pd.read_csv(&#39;data/cycling_data.csv&#39;, index_col=0, parse_dates=True) df.index = df.index + pd.DateOffset(hours=-7) df . Name Type Time Distance Comments . Date . 2019-09-09 17:13:04 Afternoon Ride | Ride | 2084 | 12.62 | Rain | . 2019-09-10 06:52:18 Morning Ride | Ride | 2531 | 13.03 | rain | . 2019-09-10 17:23:50 Afternoon Ride | Ride | 1863 | 12.52 | Wet road but nice weather | . 2019-09-11 07:06:19 Morning Ride | Ride | 2192 | 12.84 | Stopped for photo of sunrise | . 2019-09-11 17:28:05 Afternoon Ride | Ride | 1891 | 12.48 | Tired by the end of the week | . ... ... | ... | ... | ... | ... | . 2019-10-03 18:08:08 Afternoon Ride | Ride | 1870 | 12.63 | Very tired, riding into the wind | . 2019-10-09 06:55:40 Morning Ride | Ride | 2149 | 12.70 | Really cold! But feeling good | . 2019-10-09 17:10:31 Afternoon Ride | Ride | 1841 | 12.59 | Feeling good after a holiday break! | . 2019-10-10 06:47:14 Morning Ride | Ride | 2463 | 12.79 | Stopped for photo of sunrise | . 2019-10-10 17:16:57 Afternoon Ride | Ride | 1843 | 11.79 | Bike feeling tight, needs an oil and pump | . 33 rows √ó 5 columns . Resampling and Aggregating . One of the most common operations you will want do when working with time series is resampling the time series to a coarser/finer/regular resolution. For example, you may want to resample daily data to weekly data. We can do that with the .resample() method. For example, let&#39;s resample my irregular cycling timeseries to a regular 12-hourly series: . df.index . DatetimeIndex([&#39;2019-09-09 17:13:04&#39;, &#39;2019-09-10 06:52:18&#39;, &#39;2019-09-10 17:23:50&#39;, &#39;2019-09-11 07:06:19&#39;, &#39;2019-09-11 17:28:05&#39;, &#39;2019-09-16 06:57:48&#39;, &#39;2019-09-16 17:15:47&#39;, &#39;2019-09-17 06:43:34&#39;, &#39;2019-09-18 06:49:53&#39;, &#39;2019-09-17 17:15:52&#39;, &#39;2019-09-18 17:30:01&#39;, &#39;2019-09-19 06:52:09&#39;, &#39;2019-09-19 18:02:05&#39;, &#39;2019-09-23 06:50:41&#39;, &#39;2019-09-23 17:35:42&#39;, &#39;2019-09-24 06:41:24&#39;, &#39;2019-09-24 17:07:21&#39;, &#39;2019-09-25 06:35:41&#39;, &#39;2019-09-25 17:13:33&#39;, &#39;2019-09-26 06:42:43&#39;, &#39;2019-09-26 18:00:18&#39;, &#39;2019-09-30 06:53:52&#39;, &#39;2019-09-30 17:15:07&#39;, &#39;2019-10-01 06:45:55&#39;, &#39;2019-10-01 17:13:09&#39;, &#39;2019-10-02 06:46:06&#39;, &#39;2019-10-02 17:45:22&#39;, &#39;2019-10-03 06:47:36&#39;, &#39;2019-10-03 18:08:08&#39;, &#39;2019-10-09 06:55:40&#39;, &#39;2019-10-09 17:10:31&#39;, &#39;2019-10-10 06:47:14&#39;, &#39;2019-10-10 17:16:57&#39;], dtype=&#39;datetime64[ns]&#39;, name=&#39;Date&#39;, freq=None) . df.resample(&quot;1D&quot;) . &lt;pandas.core.resample.DatetimeIndexResampler object at 0x153840d90&gt; . Resampler objects are very similar to the groupby objects we saw in the previous chapter. We need to apply an aggregating function on our grouped timeseries, just like we did with groupby objects: . dfr = df.resample(&quot;1D&quot;).mean() dfr . Time Distance . Date . 2019-09-09 2084.0 | 12.620 | . 2019-09-10 2197.0 | 12.775 | . 2019-09-11 2041.5 | 12.660 | . 2019-09-12 NaN | NaN | . 2019-09-13 NaN | NaN | . ... ... | ... | . 2019-10-06 NaN | NaN | . 2019-10-07 NaN | NaN | . 2019-10-08 NaN | NaN | . 2019-10-09 1995.0 | 12.645 | . 2019-10-10 2153.0 | 12.290 | . 32 rows √ó 2 columns . There&#39;s quite a few NaNs in there? Some days I didn&#39;t ride, but some might by weekends too... . dfr[&#39;Weekday&#39;] = dfr.index.day_name() dfr.head(10) . Time Distance Weekday . Date . 2019-09-09 2084.0 | 12.620 | Monday | . 2019-09-10 2197.0 | 12.775 | Tuesday | . 2019-09-11 2041.5 | 12.660 | Wednesday | . 2019-09-12 NaN | NaN | Thursday | . 2019-09-13 NaN | NaN | Friday | . 2019-09-14 NaN | NaN | Saturday | . 2019-09-15 NaN | NaN | Sunday | . 2019-09-16 2122.5 | 12.450 | Monday | . 2019-09-17 2193.0 | 12.540 | Tuesday | . 2019-09-18 25482.5 | 13.525 | Wednesday | . Pandas support &quot;business time&quot; operations and format codes in all the timeseries functions we&#39;ve seen so far. You can check out the documentation for more info, but let&#39;s specify business days here to get rid of those weekends: . dfr = df.resample(&quot;1B&quot;).mean() # &quot;B&quot; is business day dfr[&#39;Weekday&#39;] = dfr.index.day_name() dfr.head(10) . Time Distance Weekday . Date . 2019-09-09 2084.0 | 12.620 | Monday | . 2019-09-10 2197.0 | 12.775 | Tuesday | . 2019-09-11 2041.5 | 12.660 | Wednesday | . 2019-09-12 NaN | NaN | Thursday | . 2019-09-13 NaN | NaN | Friday | . 2019-09-16 2122.5 | 12.450 | Monday | . 2019-09-17 2193.0 | 12.540 | Tuesday | . 2019-09-18 25482.5 | 13.525 | Wednesday | . 2019-09-19 2525.5 | 12.700 | Thursday | . 2019-09-20 NaN | NaN | Friday | . 3. Hierachical Indexing . Hierachical indexing, sometimes called &quot;multi-indexing&quot; or &quot;stacked indexing&quot;, is how Pandas &quot;nests&quot; data. The idea is to facilitate the storage of high dimensional data in a 2D dataframe. . . Source: Giphy . Creating a Hierachical Index . Let&#39;s start with a motivating example. Say you want to track how many courses each Master of Data Science instructor taught over the years in a Pandas Series. . {note} Recall that the content of this site is adapted from material I used to teach the 2020/2021 offering of the course &quot;DSCI 511 Python Programming for Data Science&quot; for the University of British Columbia&#39;s Master of Data Science Program. . We could use a tuple to make an appropriate index: . index = [(&#39;Tom&#39;, 2019), (&#39;Tom&#39;, 2020), (&#39;Mike&#39;, 2019), (&#39;Mike&#39;, 2020), (&#39;Tiffany&#39;, 2019), (&#39;Tiffany&#39;, 2020)] courses = [4, 6, 5, 5, 6, 3] s = pd.Series(courses, index) s . (Tom, 2019) 4 (Tom, 2020) 6 (Mike, 2019) 5 (Mike, 2020) 5 (Tiffany, 2019) 6 (Tiffany, 2020) 3 dtype: int64 . We can still kind of index this series: . s.loc[(&quot;Tom&quot;, 2019):(&quot;Tom&quot;, 2019)] . (Tom, 2019) 4 dtype: int64 . But if we wanted to get all of the values for 2019, we&#39;d need to do some messy looping: . s[[i for i in s.index if i[1] == 2019]] . (Tom, 2019) 4 (Mike, 2019) 5 (Tiffany, 2019) 6 dtype: int64 . The better way to set up this problem is with a multi-index (&quot;hierachical index&quot;). We can create a multi-index with pd.MultiIndex.from_tuple(). There are other variations of .from_X but tuple is most common. . mi = pd.MultiIndex.from_tuples(index) mi . MultiIndex([( &#39;Tom&#39;, 2019), ( &#39;Tom&#39;, 2020), ( &#39;Mike&#39;, 2019), ( &#39;Mike&#39;, 2020), (&#39;Tiffany&#39;, 2019), (&#39;Tiffany&#39;, 2020)], ) . s = pd.Series(courses, mi) s . Tom 2019 4 2020 6 Mike 2019 5 2020 5 Tiffany 2019 6 2020 3 dtype: int64 . Now we can do more efficient and logical indexing: . s.loc[&#39;Tom&#39;] . 2019 4 2020 6 dtype: int64 . s.loc[:, 2019] . Tom 4 Mike 5 Tiffany 6 dtype: int64 . s.loc[&quot;Tom&quot;, 2019] . 4 . We could also create the index by passing iterables like a list of lists directly to the index argument, but I feel it&#39;s not as explicit or intutitive as using pd.MultIndex: . index = [[&#39;Tom&#39;, &#39;Tom&#39;, &#39;Mike&#39;, &#39;Mike&#39;, &#39;Tiffany&#39;, &#39;Tiffany&#39;], [2019, 2020, 2019, 2020, 2019, 2020]] courses = [4, 6, 5, 5, 6, 3] s = pd.Series(courses, index) s . Tom 2019 4 2020 6 Mike 2019 5 2020 5 Tiffany 2019 6 2020 3 dtype: int64 . Stacking / Unstacking . You might have noticed that we could also represent our multi-index series as a dataframe. Pandas noticed this too and provides the .stack() and .unstack() methods for switching between dataframes and multi-index series: . s = s.unstack() s . 2019 2020 . Mike 5 | 5 | . Tiffany 6 | 3 | . Tom 4 | 6 | . s.stack() . Mike 2019 5 2020 5 Tiffany 2019 6 2020 3 Tom 2019 4 2020 6 dtype: int64 . Using a Hierachical Index . Observing the multi-index &lt;-&gt; dataframe equivalence above, you might wonder why we would even want multi-indices. Above, we were only dealing with 2D data, but a multi-index allows us to store any arbitrary number of dimensions: . index = [[&#39;Tom&#39;, &#39;Tom&#39;, &#39;Mike&#39;, &#39;Mike&#39;, &#39;Tiffany&#39;, &#39;Tiffany&#39;], [2019, 2020, 2019, 2020, 2019, 2020]] courses = [4, 6, 5, 5, 6, 3] s = pd.Series(courses, index) s . Tom 2019 4 2020 6 Mike 2019 5 2020 5 Tiffany 2019 6 2020 3 dtype: int64 . pd.DataFrame(s).stack() . Tom 2019 0 4 2020 0 6 Mike 2019 0 5 2020 0 5 Tiffany 2019 0 6 2020 0 3 dtype: int64 . s.loc[&#39;Tom&#39;] . 2019 4 2020 6 dtype: int64 . tom = pd.DataFrame({&quot;Courses&quot;: [4, 6], &quot;Students&quot;: [273, 342]}, index = [2019, 2020]) mike = pd.DataFrame({&quot;Courses&quot;: [5, 5], &quot;Students&quot;: [293, 420]}, index = [2019, 2020]) tiff = pd.DataFrame({&quot;Courses&quot;: [6, 3], &quot;Students&quot;: [363, 190]}, index = [2019, 2020]) . Here I have three 2D dataframes that I&#39;d like to join together. There are so many ways you can do this, but I&#39;m going to use pd.concat() and then specify the keys argument: . s3 = pd.concat((tom, mike, tiff), keys= [&#39;Tom&#39;, &#39;Mike&#39;, &#39;Tiff&#39;], axis=0) s3 . Courses Students . Tom 2019 4 | 273 | . 2020 6 | 342 | . Mike 2019 5 | 293 | . 2020 5 | 420 | . Tiff 2019 6 | 363 | . 2020 3 | 190 | . Now we have 3 dimensions of information in a single structure! . s3.stack() . Tom 2019 Courses 4 Students 273 2020 Courses 6 Students 342 Mike 2019 Courses 5 Students 293 2020 Courses 5 Students 420 Tiff 2019 Courses 6 Students 363 2020 Courses 3 Students 190 dtype: int64 . s3.loc[&#39;Tom&#39;] . Courses Students . 2019 4 | 273 | . 2020 6 | 342 | . s3.loc[&#39;Tom&#39;, 2019] . Courses 4 Students 273 Name: (Tom, 2019), dtype: int64 . We can access deeper levels in various ways: . s3.loc[&#39;Tom&#39;, 2019][&#39;Courses&#39;] . 4 . s3.loc[(&#39;Tom&#39;, 2019), &#39;Courses&#39;] . 4 . s3.loc[(&#39;Tom&#39;, 2019), &#39;Courses&#39;] . 4 . If we name our index columns, we can also use .query(): . s3 = s3.rename_axis(index=[&quot;Name&quot;, &quot;Year&quot;]) s3 . Courses Students . Name Year . Tom 2019 4 | 273 | . 2020 6 | 342 | . Mike 2019 5 | 293 | . 2020 5 | 420 | . Tiff 2019 6 | 363 | . 2020 3 | 190 | . s3.query(&quot;Year == 2019&quot;) . Courses Students . Name Year . Tom 2019 4 | 273 | . Mike 2019 5 | 293 | . Tiff 2019 6 | 363 | . Or you might prefer the &quot;stacked&quot; version of our hierachical index: . s3.stack() . Name Year Tom 2019 Courses 4 Students 273 2020 Courses 6 Students 342 Mike 2019 Courses 5 Students 293 2020 Courses 5 Students 420 Tiff 2019 Courses 6 Students 363 2020 Courses 3 Students 190 dtype: int64 . s3.stack().loc[(&#39;Tom&#39;, 2019, &#39;Courses&#39;)] . 4 . By the way, we can also use all the previous methods we&#39;ve learned about on hierachical dataframes: . s3.sort_index(ascending=False) . Courses Students . Name Year . Tom 2020 6 | 342 | . 2019 4 | 273 | . Tiff 2020 3 | 190 | . 2019 6 | 363 | . Mike 2020 5 | 420 | . 2019 5 | 293 | . s3.sort_values(by=&#39;Students&#39;) . Courses Students . Name Year . Tiff 2020 3 | 190 | . Tom 2019 4 | 273 | . Mike 2019 5 | 293 | . Tom 2020 6 | 342 | . Tiff 2019 6 | 363 | . Mike 2020 5 | 420 | . There&#39;s one important exception! We can now specify a level argument to chose which level of our multi-index to apply the function to: . s3.mean() . Courses 4.833333 Students 313.500000 dtype: float64 . s3.mean(level=&#39;Year&#39;) . Courses Students . Year . 2019 5.000000 | 309.666667 | . 2020 4.666667 | 317.333333 | . 4. Visualizing DataFrames . Pandas provides a .plot() method on Series and DataFrames which I wanted to show briefly here. . Simple Plots . df = pd.read_csv(&#39;data/cycling_data.csv&#39;, index_col=0, parse_dates=True).dropna() . Let&#39;s go ahead and make a plot of the distances I&#39;ve ridden: . df[&#39;Distance&#39;].plot.line(); . Cumulative distance might be more informative: . df[&#39;Distance&#39;].cumsum().plot.line(); . There are many configuration options for these plots which build of the matplotlib library: . df[&#39;Distance&#39;].cumsum().plot.line(fontsize=14, linewidth = 2, color = &#39;r&#39;, ylabel=&quot;km&quot;); . I actually usually use built-in themes for my plots which do a lot of the colour and text formatting for you: . import matplotlib.pyplot as plt plt.style.use(&#39;ggplot&#39;) plt.rcParams.update({&#39;font.size&#39;: 16, &#39;axes.labelweight&#39;: &#39;bold&#39;, &#39;figure.figsize&#39;: (8,6)}) . df[&#39;Distance&#39;].dropna().cumsum().plot.line(ylabel=&quot;km&quot;); . Some people have also made custom themes, like this fun cyberpunk theme: . import mplcyberpunk plt.style.use(&quot;cyberpunk&quot;) df[&#39;Distance&#39;].plot.line(ylabel=&quot;km&quot;) mplcyberpunk.add_glow_effects() . There are many other kinds of plots you can make too: . Method Plot Type . bar or barh | bar plots | . hist | histogram | . box | boxplot | . kde or density | density plots | . area | area plots | . scatter | scatter plots | . hexbin | hexagonal bin plots | . pie | pie plots | . plt.style.use(&#39;ggplot&#39;) plt.rcParams.update({&#39;font.size&#39;: 16, &#39;axes.labelweight&#39;: &#39;bold&#39;, &#39;figure.figsize&#39;: (8,6)}) df[&#39;Distance&#39;].plot.hist(); . df[&#39;Distance&#39;].plot.density(); . Pandas Plotting . Pandas also supports a few more advanced plotting functions in the pandas.plotting module. You can view them in the Pandas documentation. . from pandas.plotting import scatter_matrix . scatter_matrix(df); . We have an outlier time in the data above, a time value of ~48,000. Let&#39;s remove it and re-plot. . scatter_matrix(df.query(&#39;Time &lt; 4000&#39;), alpha=1); . 5. Pandas Profiling . Pandas profiling is a nifty tool for generating summary reports and doing exploratory data analysis on dataframes. Pandas profiling is not part of base Pandas but you can install with: . $ conda install -c conda-forge pandas-profiling . import pandas_profiling df = pd.read_csv(&#39;data/cycling_data.csv&#39;) df.profile_report(progress_bar=False) . .",
            "url": "https://millermuttu.github.io/blog/jupyter/python/data%20science/programming/pandas/2022/06/15/chapter9-wrangling-advanced.html",
            "relUrl": "/jupyter/python/data%20science/programming/pandas/2022/06/15/chapter9-wrangling-advanced.html",
            "date": " ‚Ä¢ Jun 15, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Data science revision - chapter 8",
            "content": ". Chapter 8: Basic Data Wrangling With Pandas . Chapter Outline . . 1. DataFrame Characteristics | 2. Basic DataFrame Manipulations | 3. DataFrame Reshaping | 4. Working with Multiple DataFrames | 5. More DataFrame Operations | . Chapter Learning Objectives . Inspect a dataframe with df.head(), df.tail(), df.info(), df.describe(). | Obtain dataframe summaries with df.info() and df.describe(). | Manipulate how a dataframe displays in Jupyter by modifying Pandas configuration options such as pd.set_option(&quot;display.max_rows&quot;, n). | Rename columns of a dataframe using the df.rename() function or by accessing the df.columns attribute. | Modify the index name and index values of a dataframe using .set_index(), .reset_index() , df.index.name, .index. | Use df.melt() and df.pivot() to reshape dataframes, specifically to make tidy dataframes. | Combine dataframes using df.merge() and pd.concat() and know when to use these different methods. | Apply functions to a dataframe df.apply() and df.applymap() | Perform grouping and aggregating operations using df.groupby() and df.agg(). | Perform aggregating methods on grouped or ungrouped objects such as finding the minimum, maximum and sum of values in a dataframe using df.agg(). | Remove or fill missing values in a dataframe with df.dropna() and df.fillna(). | . 1. DataFrame Characteristics . Last chapter we looked at how we can create dataframes. Let&#39;s now look at some helpful ways we can view our dataframe. . import numpy as np import pandas as pd . Head/Tail . The .head() and .tail() methods allow you to view the top/bottom n (default 5) rows of a dataframe. Let&#39;s load in the cycling data set from last chapter and try them out: . df = pd.read_csv(&#39;data/cycling_data.csv&#39;) df.head() . Date Name Type Time Distance Comments . 0 10 Sep 2019, 00:13:04 | Afternoon Ride | Ride | 2084 | 12.62 | Rain | . 1 10 Sep 2019, 13:52:18 | Morning Ride | Ride | 2531 | 13.03 | rain | . 2 11 Sep 2019, 00:23:50 | Afternoon Ride | Ride | 1863 | 12.52 | Wet road but nice weather | . 3 11 Sep 2019, 14:06:19 | Morning Ride | Ride | 2192 | 12.84 | Stopped for photo of sunrise | . 4 12 Sep 2019, 00:28:05 | Afternoon Ride | Ride | 1891 | 12.48 | Tired by the end of the week | . The default return value is 5 rows, but we can pass in any number we like. For example, let&#39;s take a look at the top 10 rows: . df.head(10) . Date Name Type Time Distance Comments . 0 10 Sep 2019, 00:13:04 | Afternoon Ride | Ride | 2084 | 12.62 | Rain | . 1 10 Sep 2019, 13:52:18 | Morning Ride | Ride | 2531 | 13.03 | rain | . 2 11 Sep 2019, 00:23:50 | Afternoon Ride | Ride | 1863 | 12.52 | Wet road but nice weather | . 3 11 Sep 2019, 14:06:19 | Morning Ride | Ride | 2192 | 12.84 | Stopped for photo of sunrise | . 4 12 Sep 2019, 00:28:05 | Afternoon Ride | Ride | 1891 | 12.48 | Tired by the end of the week | . 5 16 Sep 2019, 13:57:48 | Morning Ride | Ride | 2272 | 12.45 | Rested after the weekend! | . 6 17 Sep 2019, 00:15:47 | Afternoon Ride | Ride | 1973 | 12.45 | Legs feeling strong! | . 7 17 Sep 2019, 13:43:34 | Morning Ride | Ride | 2285 | 12.60 | Raining | . 8 18 Sep 2019, 13:49:53 | Morning Ride | Ride | 2903 | 14.57 | Raining today | . 9 18 Sep 2019, 00:15:52 | Afternoon Ride | Ride | 2101 | 12.48 | Pumped up tires | . Or the bottom 5 rows: . df.tail() . Date Name Type Time Distance Comments . 28 4 Oct 2019, 01:08:08 | Afternoon Ride | Ride | 1870 | 12.63 | Very tired, riding into the wind | . 29 9 Oct 2019, 13:55:40 | Morning Ride | Ride | 2149 | 12.70 | Really cold! But feeling good | . 30 10 Oct 2019, 00:10:31 | Afternoon Ride | Ride | 1841 | 12.59 | Feeling good after a holiday break! | . 31 10 Oct 2019, 13:47:14 | Morning Ride | Ride | 2463 | 12.79 | Stopped for photo of sunrise | . 32 11 Oct 2019, 00:16:57 | Afternoon Ride | Ride | 1843 | 11.79 | Bike feeling tight, needs an oil and pump | . DataFrame Summaries . Three very helpful attributes/functions for getting high-level summaries of your dataframe are: . .shape | .info() | .describe() | . .shape is just like the ndarray attribute we&#39;ve seen previously. It gives the shape (rows, cols) of your dataframe: . df.shape . (33, 6) . .info() prints information about the dataframe itself, such as dtypes, memory usages, non-null values, etc: . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 33 entries, 0 to 32 Data columns (total 6 columns): # Column Non-Null Count Dtype -- -- 0 Date 33 non-null object 1 Name 33 non-null object 2 Type 33 non-null object 3 Time 33 non-null int64 4 Distance 31 non-null float64 5 Comments 33 non-null object dtypes: float64(1), int64(1), object(4) memory usage: 1.7+ KB . .describe() provides summary statistics of the values within a dataframe: . df.describe() . Time Distance . count 33.000000 | 31.000000 | . mean 3512.787879 | 12.667419 | . std 8003.309233 | 0.428618 | . min 1712.000000 | 11.790000 | . 25% 1863.000000 | 12.480000 | . 50% 2118.000000 | 12.620000 | . 75% 2285.000000 | 12.750000 | . max 48062.000000 | 14.570000 | . By default, .describe() only print summaries of numeric features. We can force it to give summaries on all features using the argument include=&#39;all&#39; (although they may not make sense!): . df.describe(include=&#39;all&#39;) . Date Name Type Time Distance Comments . count 33 | 33 | 33 | 33.000000 | 31.000000 | 33 | . unique 33 | 2 | 1 | NaN | NaN | 25 | . top 20 Sep 2019, 01:02:05 | Afternoon Ride | Ride | NaN | NaN | Rested after the weekend! | . freq 1 | 17 | 33 | NaN | NaN | 3 | . mean NaN | NaN | NaN | 3512.787879 | 12.667419 | NaN | . std NaN | NaN | NaN | 8003.309233 | 0.428618 | NaN | . min NaN | NaN | NaN | 1712.000000 | 11.790000 | NaN | . 25% NaN | NaN | NaN | 1863.000000 | 12.480000 | NaN | . 50% NaN | NaN | NaN | 2118.000000 | 12.620000 | NaN | . 75% NaN | NaN | NaN | 2285.000000 | 12.750000 | NaN | . max NaN | NaN | NaN | 48062.000000 | 14.570000 | NaN | . Displaying DataFrames . Displaying your dataframes effectively can be an important part of your workflow. If a dataframe has more than 60 rows, Pandas will only display the first 5 and last 5 rows: . pd.DataFrame(np.random.rand(100)) . 0 . 0 0.643224 | . 1 0.617756 | . 2 0.650490 | . 3 0.289595 | . 4 0.469394 | . ... ... | . 95 0.549403 | . 96 0.187836 | . 97 0.016904 | . 98 0.733392 | . 99 0.875343 | . 100 rows √ó 1 columns . For dataframes of less than 60 rows, Pandas will print the whole dataframe: . df . Date Name Type Time Distance Comments . 0 10 Sep 2019, 00:13:04 | Afternoon Ride | Ride | 2084 | 12.62 | Rain | . 1 10 Sep 2019, 13:52:18 | Morning Ride | Ride | 2531 | 13.03 | rain | . 2 11 Sep 2019, 00:23:50 | Afternoon Ride | Ride | 1863 | 12.52 | Wet road but nice weather | . 3 11 Sep 2019, 14:06:19 | Morning Ride | Ride | 2192 | 12.84 | Stopped for photo of sunrise | . 4 12 Sep 2019, 00:28:05 | Afternoon Ride | Ride | 1891 | 12.48 | Tired by the end of the week | . 5 16 Sep 2019, 13:57:48 | Morning Ride | Ride | 2272 | 12.45 | Rested after the weekend! | . 6 17 Sep 2019, 00:15:47 | Afternoon Ride | Ride | 1973 | 12.45 | Legs feeling strong! | . 7 17 Sep 2019, 13:43:34 | Morning Ride | Ride | 2285 | 12.60 | Raining | . 8 18 Sep 2019, 13:49:53 | Morning Ride | Ride | 2903 | 14.57 | Raining today | . 9 18 Sep 2019, 00:15:52 | Afternoon Ride | Ride | 2101 | 12.48 | Pumped up tires | . 10 19 Sep 2019, 00:30:01 | Afternoon Ride | Ride | 48062 | 12.48 | Feeling good | . 11 19 Sep 2019, 13:52:09 | Morning Ride | Ride | 2090 | 12.59 | Getting colder which is nice | . 12 20 Sep 2019, 01:02:05 | Afternoon Ride | Ride | 2961 | 12.81 | Feeling good | . 13 23 Sep 2019, 13:50:41 | Morning Ride | Ride | 2462 | 12.68 | Rested after the weekend! | . 14 24 Sep 2019, 00:35:42 | Afternoon Ride | Ride | 2076 | 12.47 | Oiled chain, bike feels smooth | . 15 24 Sep 2019, 13:41:24 | Morning Ride | Ride | 2321 | 12.68 | Bike feeling much smoother | . 16 25 Sep 2019, 00:07:21 | Afternoon Ride | Ride | 1775 | 12.10 | Feeling really tired | . 17 25 Sep 2019, 13:35:41 | Morning Ride | Ride | 2124 | 12.65 | Stopped for photo of sunrise | . 18 26 Sep 2019, 00:13:33 | Afternoon Ride | Ride | 1860 | 12.52 | raining | . 19 26 Sep 2019, 13:42:43 | Morning Ride | Ride | 2350 | 12.91 | Detour around trucks at Jericho | . 20 27 Sep 2019, 01:00:18 | Afternoon Ride | Ride | 1712 | 12.47 | Tired by the end of the week | . 21 30 Sep 2019, 13:53:52 | Morning Ride | Ride | 2118 | 12.71 | Rested after the weekend! | . 22 1 Oct 2019, 00:15:07 | Afternoon Ride | Ride | 1732 | NaN | Legs feeling strong! | . 23 1 Oct 2019, 13:45:55 | Morning Ride | Ride | 2222 | 12.82 | Beautiful morning! Feeling fit | . 24 2 Oct 2019, 00:13:09 | Afternoon Ride | Ride | 1756 | NaN | A little tired today but good weather | . 25 2 Oct 2019, 13:46:06 | Morning Ride | Ride | 2134 | 13.06 | Bit tired today but good weather | . 26 3 Oct 2019, 00:45:22 | Afternoon Ride | Ride | 1724 | 12.52 | Feeling good | . 27 3 Oct 2019, 13:47:36 | Morning Ride | Ride | 2182 | 12.68 | Wet road | . 28 4 Oct 2019, 01:08:08 | Afternoon Ride | Ride | 1870 | 12.63 | Very tired, riding into the wind | . 29 9 Oct 2019, 13:55:40 | Morning Ride | Ride | 2149 | 12.70 | Really cold! But feeling good | . 30 10 Oct 2019, 00:10:31 | Afternoon Ride | Ride | 1841 | 12.59 | Feeling good after a holiday break! | . 31 10 Oct 2019, 13:47:14 | Morning Ride | Ride | 2463 | 12.79 | Stopped for photo of sunrise | . 32 11 Oct 2019, 00:16:57 | Afternoon Ride | Ride | 1843 | 11.79 | Bike feeling tight, needs an oil and pump | . I find the 60 row threshold to be a little too much, I prefer something more like 20. You can change the setting using pd.set_option(&quot;display.max_rows&quot;, 20) so that anything with more than 20 rows will be summarised by the first and last 5 rows as before: . pd.set_option(&quot;display.max_rows&quot;, 20) df . Date Name Type Time Distance Comments . 0 10 Sep 2019, 00:13:04 | Afternoon Ride | Ride | 2084 | 12.62 | Rain | . 1 10 Sep 2019, 13:52:18 | Morning Ride | Ride | 2531 | 13.03 | rain | . 2 11 Sep 2019, 00:23:50 | Afternoon Ride | Ride | 1863 | 12.52 | Wet road but nice weather | . 3 11 Sep 2019, 14:06:19 | Morning Ride | Ride | 2192 | 12.84 | Stopped for photo of sunrise | . 4 12 Sep 2019, 00:28:05 | Afternoon Ride | Ride | 1891 | 12.48 | Tired by the end of the week | . ... ... | ... | ... | ... | ... | ... | . 28 4 Oct 2019, 01:08:08 | Afternoon Ride | Ride | 1870 | 12.63 | Very tired, riding into the wind | . 29 9 Oct 2019, 13:55:40 | Morning Ride | Ride | 2149 | 12.70 | Really cold! But feeling good | . 30 10 Oct 2019, 00:10:31 | Afternoon Ride | Ride | 1841 | 12.59 | Feeling good after a holiday break! | . 31 10 Oct 2019, 13:47:14 | Morning Ride | Ride | 2463 | 12.79 | Stopped for photo of sunrise | . 32 11 Oct 2019, 00:16:57 | Afternoon Ride | Ride | 1843 | 11.79 | Bike feeling tight, needs an oil and pump | . 33 rows √ó 6 columns . There are also other display options you can change, such as how many columns are shown, how numbers are formatted, etc. See the official documentation for more. . One display option I will point out is that Pandas allows you to style your tables, for example by highlighting negative values, or adding conditional colour maps to your dataframe. Below I&#39;ll style values based on their value ranging from negative (purple) to postive (yellow) but you can see the styling documentation for more examples. . test = pd.DataFrame(np.random.randn(5, 5), index = [f&quot;row_{_}&quot; for _ in range(5)], columns = [f&quot;feature_{_}&quot; for _ in range(5)]) test.style.background_gradient(cmap=&#39;plasma&#39;) . feature_0 feature_1 feature_2 feature_3 feature_4 . row_0 -0.519284 | -0.553179 | -1.768810 | 1.023342 | -0.668625 | . row_1 -1.064582 | 0.205538 | 0.350555 | -1.942761 | 1.594635 | . row_2 1.057886 | 0.074919 | -0.998969 | -0.030725 | 0.677628 | . row_3 0.898228 | -0.697138 | -0.134935 | -1.592920 | -2.127225 | . row_4 0.039582 | 0.372877 | -0.299539 | -2.132972 | 2.194797 | . Views vs Copies . In previous chapters we&#39;ve discussed views (&quot;looking&quot; at a part of an existing object) and copies (making a new copy of the object in memory). These things get a little abstract with Pandas and &quot;...it‚Äôs very hard to predict whether it will return a view or a copy&quot; (that&#39;s a quote straight from a dedicated section in the Pandas documentation). . Basically, it depends on the operation you are trying to perform, your dataframe&#39;s structure and the memory layout of the underlying array. But don&#39;t worry, let me tell you all you need to know. Firstly, the most common warning you&#39;ll encounter in Pandas is the SettingWithCopy, Pandas raises it as a warning that you might not be doing what you think you&#39;re doing. Let&#39;s see an example. You may recall there is one outlier Time in our dataframe: . df[df[&#39;Time&#39;] &gt; 4000] . Date Name Type Time Distance Comments . 10 19 Sep 2019, 00:30:01 | Afternoon Ride | Ride | 48062 | 12.48 | Feeling good | . Imagine we wanted to change this to 2000. You&#39;d probably do the following: . df[df[&#39;Time&#39;] &gt; 4000][&#39;Time&#39;] = 2000 . /opt/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy &#34;&#34;&#34;Entry point for launching an IPython kernel. . Ah, there&#39;s that warning. Did our dataframe get changed? . df[df[&#39;Time&#39;] &gt; 4000] . Date Name Type Time Distance Comments . 10 19 Sep 2019, 00:30:01 | Afternoon Ride | Ride | 48062 | 12.48 | Feeling good | . No it didn&#39;t, even though you probably thought it did. What happened above is that df[df[&#39;Time&#39;] &gt; 4000] was executed first and returned a copy of the dataframe, we can confirm by using id(): . print(f&quot;The id of the original dataframe is: {id(df)}&quot;) print(f&quot; The id of the indexed dataframe is: {id(df[df[&#39;Time&#39;] &gt; 4000])}&quot;) . The id of the original dataframe is: 5762156560 The id of the indexed dataframe is: 5781171152 . We then tried to set a value on this new object by appending [&#39;Time&#39;] = 2000. Pandas is warning us that we are doing that operation on a copy of the original dataframe, which is probably not what we want. To fix this, you need to index in a single go, using .loc[] for example: . df.loc[df[&#39;Time&#39;] &gt; 4000, &#39;Time&#39;] = 2000 . No error this time! And let&#39;s confirm the change: . df[df[&#39;Time&#39;] &gt; 4000] . Date Name Type Time Distance Comments . The second thing you need to know is that if you&#39;re ever in doubt about whether something is a view or a copy, you can just use the .copy() method to force a copy of a dataframe. Just like this: . df2 = df[df[&#39;Time&#39;] &gt; 4000].copy() . That way, your guaranteed a copy that you can modify as you wish. . 2. Basic DataFrame Manipulations . Renaming Columns . We can rename columns two ways: . Using .rename() (to selectively change column names) | By setting the .columns attribute (to change all column names at once) | df . Date Name Type Time Distance Comments . 0 10 Sep 2019, 00:13:04 | Afternoon Ride | Ride | 2084 | 12.62 | Rain | . 1 10 Sep 2019, 13:52:18 | Morning Ride | Ride | 2531 | 13.03 | rain | . 2 11 Sep 2019, 00:23:50 | Afternoon Ride | Ride | 1863 | 12.52 | Wet road but nice weather | . 3 11 Sep 2019, 14:06:19 | Morning Ride | Ride | 2192 | 12.84 | Stopped for photo of sunrise | . 4 12 Sep 2019, 00:28:05 | Afternoon Ride | Ride | 1891 | 12.48 | Tired by the end of the week | . ... ... | ... | ... | ... | ... | ... | . 28 4 Oct 2019, 01:08:08 | Afternoon Ride | Ride | 1870 | 12.63 | Very tired, riding into the wind | . 29 9 Oct 2019, 13:55:40 | Morning Ride | Ride | 2149 | 12.70 | Really cold! But feeling good | . 30 10 Oct 2019, 00:10:31 | Afternoon Ride | Ride | 1841 | 12.59 | Feeling good after a holiday break! | . 31 10 Oct 2019, 13:47:14 | Morning Ride | Ride | 2463 | 12.79 | Stopped for photo of sunrise | . 32 11 Oct 2019, 00:16:57 | Afternoon Ride | Ride | 1843 | 11.79 | Bike feeling tight, needs an oil and pump | . 33 rows √ó 6 columns . Let&#39;s give it a go: . df.rename(columns={&quot;Date&quot;: &quot;Datetime&quot;, &quot;Comments&quot;: &quot;Notes&quot;}) df . Date Name Type Time Distance Comments . 0 10 Sep 2019, 00:13:04 | Afternoon Ride | Ride | 2084 | 12.62 | Rain | . 1 10 Sep 2019, 13:52:18 | Morning Ride | Ride | 2531 | 13.03 | rain | . 2 11 Sep 2019, 00:23:50 | Afternoon Ride | Ride | 1863 | 12.52 | Wet road but nice weather | . 3 11 Sep 2019, 14:06:19 | Morning Ride | Ride | 2192 | 12.84 | Stopped for photo of sunrise | . 4 12 Sep 2019, 00:28:05 | Afternoon Ride | Ride | 1891 | 12.48 | Tired by the end of the week | . ... ... | ... | ... | ... | ... | ... | . 28 4 Oct 2019, 01:08:08 | Afternoon Ride | Ride | 1870 | 12.63 | Very tired, riding into the wind | . 29 9 Oct 2019, 13:55:40 | Morning Ride | Ride | 2149 | 12.70 | Really cold! But feeling good | . 30 10 Oct 2019, 00:10:31 | Afternoon Ride | Ride | 1841 | 12.59 | Feeling good after a holiday break! | . 31 10 Oct 2019, 13:47:14 | Morning Ride | Ride | 2463 | 12.79 | Stopped for photo of sunrise | . 32 11 Oct 2019, 00:16:57 | Afternoon Ride | Ride | 1843 | 11.79 | Bike feeling tight, needs an oil and pump | . 33 rows √ó 6 columns . Wait? What happened? Nothing changed? In the code above we did actually rename columns of our dataframe but we didn&#39;t modify the dataframe inplace, we made a copy of it. There are generally two options for making permanent dataframe changes: . Use the argument inplace=True, e.g., df.rename(..., inplace=True), available in most functions/methods | | Re-assign, e.g., df = df.rename(...) The Pandas team recommends Method 2 (re-assign), for a few reasons (mostly to do with how memory is allocated under the hood). | | . df = df.rename(columns={&quot;Date&quot;: &quot;Datetime&quot;, &quot;Comments&quot;: &quot;Notes&quot;}) df . Datetime Name Type Time Distance Notes . 0 10 Sep 2019, 00:13:04 | Afternoon Ride | Ride | 2084 | 12.62 | Rain | . 1 10 Sep 2019, 13:52:18 | Morning Ride | Ride | 2531 | 13.03 | rain | . 2 11 Sep 2019, 00:23:50 | Afternoon Ride | Ride | 1863 | 12.52 | Wet road but nice weather | . 3 11 Sep 2019, 14:06:19 | Morning Ride | Ride | 2192 | 12.84 | Stopped for photo of sunrise | . 4 12 Sep 2019, 00:28:05 | Afternoon Ride | Ride | 1891 | 12.48 | Tired by the end of the week | . ... ... | ... | ... | ... | ... | ... | . 28 4 Oct 2019, 01:08:08 | Afternoon Ride | Ride | 1870 | 12.63 | Very tired, riding into the wind | . 29 9 Oct 2019, 13:55:40 | Morning Ride | Ride | 2149 | 12.70 | Really cold! But feeling good | . 30 10 Oct 2019, 00:10:31 | Afternoon Ride | Ride | 1841 | 12.59 | Feeling good after a holiday break! | . 31 10 Oct 2019, 13:47:14 | Morning Ride | Ride | 2463 | 12.79 | Stopped for photo of sunrise | . 32 11 Oct 2019, 00:16:57 | Afternoon Ride | Ride | 1843 | 11.79 | Bike feeling tight, needs an oil and pump | . 33 rows √ó 6 columns . If you wish to change all of the columns of a dataframe, you can do so by setting the .columns attribute: . df.columns = [f&quot;Column {_}&quot; for _ in range(1, 7)] df . Column 1 Column 2 Column 3 Column 4 Column 5 Column 6 . 0 10 Sep 2019, 00:13:04 | Afternoon Ride | Ride | 2084 | 12.62 | Rain | . 1 10 Sep 2019, 13:52:18 | Morning Ride | Ride | 2531 | 13.03 | rain | . 2 11 Sep 2019, 00:23:50 | Afternoon Ride | Ride | 1863 | 12.52 | Wet road but nice weather | . 3 11 Sep 2019, 14:06:19 | Morning Ride | Ride | 2192 | 12.84 | Stopped for photo of sunrise | . 4 12 Sep 2019, 00:28:05 | Afternoon Ride | Ride | 1891 | 12.48 | Tired by the end of the week | . ... ... | ... | ... | ... | ... | ... | . 28 4 Oct 2019, 01:08:08 | Afternoon Ride | Ride | 1870 | 12.63 | Very tired, riding into the wind | . 29 9 Oct 2019, 13:55:40 | Morning Ride | Ride | 2149 | 12.70 | Really cold! But feeling good | . 30 10 Oct 2019, 00:10:31 | Afternoon Ride | Ride | 1841 | 12.59 | Feeling good after a holiday break! | . 31 10 Oct 2019, 13:47:14 | Morning Ride | Ride | 2463 | 12.79 | Stopped for photo of sunrise | . 32 11 Oct 2019, 00:16:57 | Afternoon Ride | Ride | 1843 | 11.79 | Bike feeling tight, needs an oil and pump | . 33 rows √ó 6 columns . Changing the Index . You can change the index labels of a dataframe in 3 main ways: . .set_index() to make one of the columns of the dataframe the index | Directly modify df.index.name to change the index name | .reset_index() to move the current index as a column and to reset the index with integer labels starting from 0 | Directly modify the .index() attribute | df . Column 1 Column 2 Column 3 Column 4 Column 5 Column 6 . 0 10 Sep 2019, 00:13:04 | Afternoon Ride | Ride | 2084 | 12.62 | Rain | . 1 10 Sep 2019, 13:52:18 | Morning Ride | Ride | 2531 | 13.03 | rain | . 2 11 Sep 2019, 00:23:50 | Afternoon Ride | Ride | 1863 | 12.52 | Wet road but nice weather | . 3 11 Sep 2019, 14:06:19 | Morning Ride | Ride | 2192 | 12.84 | Stopped for photo of sunrise | . 4 12 Sep 2019, 00:28:05 | Afternoon Ride | Ride | 1891 | 12.48 | Tired by the end of the week | . ... ... | ... | ... | ... | ... | ... | . 28 4 Oct 2019, 01:08:08 | Afternoon Ride | Ride | 1870 | 12.63 | Very tired, riding into the wind | . 29 9 Oct 2019, 13:55:40 | Morning Ride | Ride | 2149 | 12.70 | Really cold! But feeling good | . 30 10 Oct 2019, 00:10:31 | Afternoon Ride | Ride | 1841 | 12.59 | Feeling good after a holiday break! | . 31 10 Oct 2019, 13:47:14 | Morning Ride | Ride | 2463 | 12.79 | Stopped for photo of sunrise | . 32 11 Oct 2019, 00:16:57 | Afternoon Ride | Ride | 1843 | 11.79 | Bike feeling tight, needs an oil and pump | . 33 rows √ó 6 columns . Below I will set the index as Column 1 and rename the index to &quot;New Index&quot;: . df = df.set_index(&quot;Column 1&quot;) df.index.name = &quot;New Index&quot; df . Column 2 Column 3 Column 4 Column 5 Column 6 . New Index . 10 Sep 2019, 00:13:04 Afternoon Ride | Ride | 2084 | 12.62 | Rain | . 10 Sep 2019, 13:52:18 Morning Ride | Ride | 2531 | 13.03 | rain | . 11 Sep 2019, 00:23:50 Afternoon Ride | Ride | 1863 | 12.52 | Wet road but nice weather | . 11 Sep 2019, 14:06:19 Morning Ride | Ride | 2192 | 12.84 | Stopped for photo of sunrise | . 12 Sep 2019, 00:28:05 Afternoon Ride | Ride | 1891 | 12.48 | Tired by the end of the week | . ... ... | ... | ... | ... | ... | . 4 Oct 2019, 01:08:08 Afternoon Ride | Ride | 1870 | 12.63 | Very tired, riding into the wind | . 9 Oct 2019, 13:55:40 Morning Ride | Ride | 2149 | 12.70 | Really cold! But feeling good | . 10 Oct 2019, 00:10:31 Afternoon Ride | Ride | 1841 | 12.59 | Feeling good after a holiday break! | . 10 Oct 2019, 13:47:14 Morning Ride | Ride | 2463 | 12.79 | Stopped for photo of sunrise | . 11 Oct 2019, 00:16:57 Afternoon Ride | Ride | 1843 | 11.79 | Bike feeling tight, needs an oil and pump | . 33 rows √ó 5 columns . I can send the index back to a column and have a default integer index using .reset_index(): . df = df.reset_index() df . New Index Column 2 Column 3 Column 4 Column 5 Column 6 . 0 10 Sep 2019, 00:13:04 | Afternoon Ride | Ride | 2084 | 12.62 | Rain | . 1 10 Sep 2019, 13:52:18 | Morning Ride | Ride | 2531 | 13.03 | rain | . 2 11 Sep 2019, 00:23:50 | Afternoon Ride | Ride | 1863 | 12.52 | Wet road but nice weather | . 3 11 Sep 2019, 14:06:19 | Morning Ride | Ride | 2192 | 12.84 | Stopped for photo of sunrise | . 4 12 Sep 2019, 00:28:05 | Afternoon Ride | Ride | 1891 | 12.48 | Tired by the end of the week | . ... ... | ... | ... | ... | ... | ... | . 28 4 Oct 2019, 01:08:08 | Afternoon Ride | Ride | 1870 | 12.63 | Very tired, riding into the wind | . 29 9 Oct 2019, 13:55:40 | Morning Ride | Ride | 2149 | 12.70 | Really cold! But feeling good | . 30 10 Oct 2019, 00:10:31 | Afternoon Ride | Ride | 1841 | 12.59 | Feeling good after a holiday break! | . 31 10 Oct 2019, 13:47:14 | Morning Ride | Ride | 2463 | 12.79 | Stopped for photo of sunrise | . 32 11 Oct 2019, 00:16:57 | Afternoon Ride | Ride | 1843 | 11.79 | Bike feeling tight, needs an oil and pump | . 33 rows √ó 6 columns . Like with column names, we can also modify the index directly, but I can&#39;t remember ever doing this, usually I&#39;ll use .set_index(): . df.index . RangeIndex(start=0, stop=33, step=1) . df.index = range(100, 133, 1) df . New Index Column 2 Column 3 Column 4 Column 5 Column 6 . 100 10 Sep 2019, 00:13:04 | Afternoon Ride | Ride | 2084 | 12.62 | Rain | . 101 10 Sep 2019, 13:52:18 | Morning Ride | Ride | 2531 | 13.03 | rain | . 102 11 Sep 2019, 00:23:50 | Afternoon Ride | Ride | 1863 | 12.52 | Wet road but nice weather | . 103 11 Sep 2019, 14:06:19 | Morning Ride | Ride | 2192 | 12.84 | Stopped for photo of sunrise | . 104 12 Sep 2019, 00:28:05 | Afternoon Ride | Ride | 1891 | 12.48 | Tired by the end of the week | . ... ... | ... | ... | ... | ... | ... | . 128 4 Oct 2019, 01:08:08 | Afternoon Ride | Ride | 1870 | 12.63 | Very tired, riding into the wind | . 129 9 Oct 2019, 13:55:40 | Morning Ride | Ride | 2149 | 12.70 | Really cold! But feeling good | . 130 10 Oct 2019, 00:10:31 | Afternoon Ride | Ride | 1841 | 12.59 | Feeling good after a holiday break! | . 131 10 Oct 2019, 13:47:14 | Morning Ride | Ride | 2463 | 12.79 | Stopped for photo of sunrise | . 132 11 Oct 2019, 00:16:57 | Afternoon Ride | Ride | 1843 | 11.79 | Bike feeling tight, needs an oil and pump | . 33 rows √ó 6 columns . Adding/Removing Columns . There are two main ways to add/remove columns of a dataframe: . Use [] to add columns | Use .drop() to drop columns | Let&#39;s re-read in a fresh copy of the cycling dataset. . df = pd.read_csv(&#39;data/cycling_data.csv&#39;) df . Date Name Type Time Distance Comments . 0 10 Sep 2019, 00:13:04 | Afternoon Ride | Ride | 2084 | 12.62 | Rain | . 1 10 Sep 2019, 13:52:18 | Morning Ride | Ride | 2531 | 13.03 | rain | . 2 11 Sep 2019, 00:23:50 | Afternoon Ride | Ride | 1863 | 12.52 | Wet road but nice weather | . 3 11 Sep 2019, 14:06:19 | Morning Ride | Ride | 2192 | 12.84 | Stopped for photo of sunrise | . 4 12 Sep 2019, 00:28:05 | Afternoon Ride | Ride | 1891 | 12.48 | Tired by the end of the week | . ... ... | ... | ... | ... | ... | ... | . 28 4 Oct 2019, 01:08:08 | Afternoon Ride | Ride | 1870 | 12.63 | Very tired, riding into the wind | . 29 9 Oct 2019, 13:55:40 | Morning Ride | Ride | 2149 | 12.70 | Really cold! But feeling good | . 30 10 Oct 2019, 00:10:31 | Afternoon Ride | Ride | 1841 | 12.59 | Feeling good after a holiday break! | . 31 10 Oct 2019, 13:47:14 | Morning Ride | Ride | 2463 | 12.79 | Stopped for photo of sunrise | . 32 11 Oct 2019, 00:16:57 | Afternoon Ride | Ride | 1843 | 11.79 | Bike feeling tight, needs an oil and pump | . 33 rows √ó 6 columns . We can add a new column to a dataframe by simply using [] with a new column name and value(s): . df[&#39;Rider&#39;] = &#39;Tom Beuzen&#39; df[&#39;Avg Speed&#39;] = df[&#39;Distance&#39;] * 1000 / df[&#39;Time&#39;] # avg. speed in m/s df . Date Name Type Time Distance Comments Rider Avg Speed . 0 10 Sep 2019, 00:13:04 | Afternoon Ride | Ride | 2084 | 12.62 | Rain | Tom Beuzen | 6.055662 | . 1 10 Sep 2019, 13:52:18 | Morning Ride | Ride | 2531 | 13.03 | rain | Tom Beuzen | 5.148163 | . 2 11 Sep 2019, 00:23:50 | Afternoon Ride | Ride | 1863 | 12.52 | Wet road but nice weather | Tom Beuzen | 6.720344 | . 3 11 Sep 2019, 14:06:19 | Morning Ride | Ride | 2192 | 12.84 | Stopped for photo of sunrise | Tom Beuzen | 5.857664 | . 4 12 Sep 2019, 00:28:05 | Afternoon Ride | Ride | 1891 | 12.48 | Tired by the end of the week | Tom Beuzen | 6.599683 | . ... ... | ... | ... | ... | ... | ... | ... | ... | . 28 4 Oct 2019, 01:08:08 | Afternoon Ride | Ride | 1870 | 12.63 | Very tired, riding into the wind | Tom Beuzen | 6.754011 | . 29 9 Oct 2019, 13:55:40 | Morning Ride | Ride | 2149 | 12.70 | Really cold! But feeling good | Tom Beuzen | 5.909725 | . 30 10 Oct 2019, 00:10:31 | Afternoon Ride | Ride | 1841 | 12.59 | Feeling good after a holiday break! | Tom Beuzen | 6.838675 | . 31 10 Oct 2019, 13:47:14 | Morning Ride | Ride | 2463 | 12.79 | Stopped for photo of sunrise | Tom Beuzen | 5.192854 | . 32 11 Oct 2019, 00:16:57 | Afternoon Ride | Ride | 1843 | 11.79 | Bike feeling tight, needs an oil and pump | Tom Beuzen | 6.397179 | . 33 rows √ó 8 columns . df = df.drop(columns=[&#39;Rider&#39;, &#39;Avg Speed&#39;]) df . Date Name Type Time Distance Comments . 0 10 Sep 2019, 00:13:04 | Afternoon Ride | Ride | 2084 | 12.62 | Rain | . 1 10 Sep 2019, 13:52:18 | Morning Ride | Ride | 2531 | 13.03 | rain | . 2 11 Sep 2019, 00:23:50 | Afternoon Ride | Ride | 1863 | 12.52 | Wet road but nice weather | . 3 11 Sep 2019, 14:06:19 | Morning Ride | Ride | 2192 | 12.84 | Stopped for photo of sunrise | . 4 12 Sep 2019, 00:28:05 | Afternoon Ride | Ride | 1891 | 12.48 | Tired by the end of the week | . ... ... | ... | ... | ... | ... | ... | . 28 4 Oct 2019, 01:08:08 | Afternoon Ride | Ride | 1870 | 12.63 | Very tired, riding into the wind | . 29 9 Oct 2019, 13:55:40 | Morning Ride | Ride | 2149 | 12.70 | Really cold! But feeling good | . 30 10 Oct 2019, 00:10:31 | Afternoon Ride | Ride | 1841 | 12.59 | Feeling good after a holiday break! | . 31 10 Oct 2019, 13:47:14 | Morning Ride | Ride | 2463 | 12.79 | Stopped for photo of sunrise | . 32 11 Oct 2019, 00:16:57 | Afternoon Ride | Ride | 1843 | 11.79 | Bike feeling tight, needs an oil and pump | . 33 rows √ó 6 columns . Adding/Removing Rows . You won&#39;t often be adding rows to a dataframe manually (you&#39;ll usually add rows through concatenating/joining - that&#39;s coming up next). You can add/remove rows of a dataframe in two ways: . Use .append() to add rows | Use .drop() to drop rows | df . Date Name Type Time Distance Comments . 0 10 Sep 2019, 00:13:04 | Afternoon Ride | Ride | 2084 | 12.62 | Rain | . 1 10 Sep 2019, 13:52:18 | Morning Ride | Ride | 2531 | 13.03 | rain | . 2 11 Sep 2019, 00:23:50 | Afternoon Ride | Ride | 1863 | 12.52 | Wet road but nice weather | . 3 11 Sep 2019, 14:06:19 | Morning Ride | Ride | 2192 | 12.84 | Stopped for photo of sunrise | . 4 12 Sep 2019, 00:28:05 | Afternoon Ride | Ride | 1891 | 12.48 | Tired by the end of the week | . ... ... | ... | ... | ... | ... | ... | . 28 4 Oct 2019, 01:08:08 | Afternoon Ride | Ride | 1870 | 12.63 | Very tired, riding into the wind | . 29 9 Oct 2019, 13:55:40 | Morning Ride | Ride | 2149 | 12.70 | Really cold! But feeling good | . 30 10 Oct 2019, 00:10:31 | Afternoon Ride | Ride | 1841 | 12.59 | Feeling good after a holiday break! | . 31 10 Oct 2019, 13:47:14 | Morning Ride | Ride | 2463 | 12.79 | Stopped for photo of sunrise | . 32 11 Oct 2019, 00:16:57 | Afternoon Ride | Ride | 1843 | 11.79 | Bike feeling tight, needs an oil and pump | . 33 rows √ó 6 columns . Let&#39;s add a new row to the bottom of this dataframe: . another_row = pd.DataFrame([[&quot;12 Oct 2019, 00:10:57&quot;, &quot;Morning Ride&quot;, &quot;Ride&quot;, 2331, 12.67, &quot;Washed and oiled bike last night&quot;]], columns = df.columns, index = [33]) df = df.append(another_row) df . Date Name Type Time Distance Comments . 0 10 Sep 2019, 00:13:04 | Afternoon Ride | Ride | 2084 | 12.62 | Rain | . 1 10 Sep 2019, 13:52:18 | Morning Ride | Ride | 2531 | 13.03 | rain | . 2 11 Sep 2019, 00:23:50 | Afternoon Ride | Ride | 1863 | 12.52 | Wet road but nice weather | . 3 11 Sep 2019, 14:06:19 | Morning Ride | Ride | 2192 | 12.84 | Stopped for photo of sunrise | . 4 12 Sep 2019, 00:28:05 | Afternoon Ride | Ride | 1891 | 12.48 | Tired by the end of the week | . ... ... | ... | ... | ... | ... | ... | . 29 9 Oct 2019, 13:55:40 | Morning Ride | Ride | 2149 | 12.70 | Really cold! But feeling good | . 30 10 Oct 2019, 00:10:31 | Afternoon Ride | Ride | 1841 | 12.59 | Feeling good after a holiday break! | . 31 10 Oct 2019, 13:47:14 | Morning Ride | Ride | 2463 | 12.79 | Stopped for photo of sunrise | . 32 11 Oct 2019, 00:16:57 | Afternoon Ride | Ride | 1843 | 11.79 | Bike feeling tight, needs an oil and pump | . 33 12 Oct 2019, 00:10:57 | Morning Ride | Ride | 2331 | 12.67 | Washed and oiled bike last night | . 34 rows √ó 6 columns . We can drop all rows above index 30 using .drop(): . df.drop(index=range(30, 34)) . Date Name Type Time Distance Comments . 0 10 Sep 2019, 00:13:04 | Afternoon Ride | Ride | 2084 | 12.62 | Rain | . 1 10 Sep 2019, 13:52:18 | Morning Ride | Ride | 2531 | 13.03 | rain | . 2 11 Sep 2019, 00:23:50 | Afternoon Ride | Ride | 1863 | 12.52 | Wet road but nice weather | . 3 11 Sep 2019, 14:06:19 | Morning Ride | Ride | 2192 | 12.84 | Stopped for photo of sunrise | . 4 12 Sep 2019, 00:28:05 | Afternoon Ride | Ride | 1891 | 12.48 | Tired by the end of the week | . ... ... | ... | ... | ... | ... | ... | . 25 2 Oct 2019, 13:46:06 | Morning Ride | Ride | 2134 | 13.06 | Bit tired today but good weather | . 26 3 Oct 2019, 00:45:22 | Afternoon Ride | Ride | 1724 | 12.52 | Feeling good | . 27 3 Oct 2019, 13:47:36 | Morning Ride | Ride | 2182 | 12.68 | Wet road | . 28 4 Oct 2019, 01:08:08 | Afternoon Ride | Ride | 1870 | 12.63 | Very tired, riding into the wind | . 29 9 Oct 2019, 13:55:40 | Morning Ride | Ride | 2149 | 12.70 | Really cold! But feeling good | . 30 rows √ó 6 columns . 3. DataFrame Reshaping . Tidy data is about &quot;linking the structure of a dataset with its semantics (its meaning)&quot;. It is defined by: . Each variable forms a column | Each observation forms a row | Each type of observational unit forms a table | Often you&#39;ll need to reshape a dataframe to make it tidy (or for some other purpose). . . Source: r4ds . Melt and Pivot . Pandas .melt(), .pivot() and .pivot_table() can help reshape dataframes . .melt(): make wide data long. | .pivot(): make long data width. | .pivot_table(): same as .pivot() but can handle multiple indexes. | . . Source: Garrick Aden-Buie&#39;s GitHub . The below data shows how many courses different instructors taught across different years. If the question you want to answer is something like: &quot;Does the number of courses taught vary depending on year?&quot; then the below would probably not be considered tidy because there are multiple observations of courses taught in a year per row (i.e., there is data for 2018, 2019 and 2020 in a single row): . df = pd.DataFrame({&quot;Name&quot;: [&quot;Tom&quot;, &quot;Mike&quot;, &quot;Tiffany&quot;, &quot;Varada&quot;, &quot;Joel&quot;], &quot;2018&quot;: [1, 3, 4, 5, 3], &quot;2019&quot;: [2, 4, 3, 2, 1], &quot;2020&quot;: [5, 2, 4, 4, 3]}) df . Name 2018 2019 2020 . 0 Tom | 1 | 2 | 5 | . 1 Mike | 3 | 4 | 2 | . 2 Tiffany | 4 | 3 | 4 | . 3 Varada | 5 | 2 | 4 | . 4 Joel | 3 | 1 | 3 | . Let&#39;s make it tidy with .melt(). .melt() takes a few arguments, most important is the id_vars which indicated which column should be the &quot;identifier&quot;. . df_melt = df.melt(id_vars=&quot;Name&quot;, var_name=&quot;Year&quot;, value_name=&quot;Courses&quot;) df_melt . Name Year Courses . 0 Tom | 2018 | 1 | . 1 Mike | 2018 | 3 | . 2 Tiffany | 2018 | 4 | . 3 Varada | 2018 | 5 | . 4 Joel | 2018 | 3 | . 5 Tom | 2019 | 2 | . 6 Mike | 2019 | 4 | . 7 Tiffany | 2019 | 3 | . 8 Varada | 2019 | 2 | . 9 Joel | 2019 | 1 | . 10 Tom | 2020 | 5 | . 11 Mike | 2020 | 2 | . 12 Tiffany | 2020 | 4 | . 13 Varada | 2020 | 4 | . 14 Joel | 2020 | 3 | . The value_vars argument allows us to select which specific variables we want to &quot;melt&quot; (if you don&#39;t specify value_vars, all non-identifier columns will be used). For example, below I&#39;m omitting the 2018 column: . df.melt(id_vars=&quot;Name&quot;, value_vars=[&quot;2019&quot;, &quot;2020&quot;], var_name=&quot;Year&quot;, value_name=&quot;Courses&quot;) . Name Year Courses . 0 Tom | 2019 | 2 | . 1 Mike | 2019 | 4 | . 2 Tiffany | 2019 | 3 | . 3 Varada | 2019 | 2 | . 4 Joel | 2019 | 1 | . 5 Tom | 2020 | 5 | . 6 Mike | 2020 | 2 | . 7 Tiffany | 2020 | 4 | . 8 Varada | 2020 | 4 | . 9 Joel | 2020 | 3 | . Sometimes, you want to make long data wide, which we can do with .pivot(). When using .pivot() we need to specify the index to pivot on, and the columns that will be used to make the new columns of the wider dataframe: . df_pivot = df_melt.pivot(index=&quot;Name&quot;, columns=&quot;Year&quot;, values=&quot;Courses&quot;) df_pivot . Year 2018 2019 2020 . Name . Joel 3 | 1 | 3 | . Mike 3 | 4 | 2 | . Tiffany 4 | 3 | 4 | . Tom 1 | 2 | 5 | . Varada 5 | 2 | 4 | . You&#39;ll notice that Pandas set our specified index as the index of the new dataframe and preserved the label of the columns. We can easily remove these names and reset the index to make our dataframe look like it originally did: . df_pivot = df_pivot.reset_index() df_pivot.columns.name = None df_pivot . Name 2018 2019 2020 . 0 Joel | 3 | 1 | 3 | . 1 Mike | 3 | 4 | 2 | . 2 Tiffany | 4 | 3 | 4 | . 3 Tom | 1 | 2 | 5 | . 4 Varada | 5 | 2 | 4 | . .pivot() will often get you what you want, but it won&#39;t work if you want to: . Use multiple indexes (next chapter), or | Have duplicate index/column labels | . In these cases you&#39;ll have to use .pivot_table(). I won&#39;t focus on it too much here because I&#39;d rather you learn about pivot() first. . df = pd.DataFrame({&quot;Name&quot;: [&quot;Tom&quot;, &quot;Tom&quot;, &quot;Mike&quot;, &quot;Mike&quot;], &quot;Department&quot;: [&quot;CS&quot;, &quot;STATS&quot;, &quot;CS&quot;, &quot;STATS&quot;], &quot;2018&quot;: [1, 2, 3, 1], &quot;2019&quot;: [2, 3, 4, 2], &quot;2020&quot;: [5, 1, 2, 2]}).melt(id_vars=[&quot;Name&quot;, &quot;Department&quot;], var_name=&quot;Year&quot;, value_name=&quot;Courses&quot;) df . Name Department Year Courses . 0 Tom | CS | 2018 | 1 | . 1 Tom | STATS | 2018 | 2 | . 2 Mike | CS | 2018 | 3 | . 3 Mike | STATS | 2018 | 1 | . 4 Tom | CS | 2019 | 2 | . 5 Tom | STATS | 2019 | 3 | . 6 Mike | CS | 2019 | 4 | . 7 Mike | STATS | 2019 | 2 | . 8 Tom | CS | 2020 | 5 | . 9 Tom | STATS | 2020 | 1 | . 10 Mike | CS | 2020 | 2 | . 11 Mike | STATS | 2020 | 2 | . In the above case, we have duplicates in Name, so pivot() won&#39;t work. It will throw us a ValueError: Index contains duplicate entries, cannot reshape: . df.pivot(index=&quot;Name&quot;, columns=&quot;Year&quot;, values=&quot;Courses&quot;) . ValueError Traceback (most recent call last) &lt;ipython-input-41-585e29950052&gt; in &lt;module&gt; 1 df.pivot(index=&#34;Name&#34;, 2 columns=&#34;Year&#34;, -&gt; 3 values=&#34;Courses&#34;) /opt/miniconda3/lib/python3.7/site-packages/pandas/core/frame.py in pivot(self, index, columns, values) 6669 from pandas.core.reshape.pivot import pivot 6670 -&gt; 6671 return pivot(self, index=index, columns=columns, values=values) 6672 6673 _shared_docs[ /opt/miniconda3/lib/python3.7/site-packages/pandas/core/reshape/pivot.py in pivot(data, index, columns, values) 475 else: 476 indexed = data._constructor_sliced(data[values]._values, index=index) --&gt; 477 return indexed.unstack(columns) 478 479 /opt/miniconda3/lib/python3.7/site-packages/pandas/core/series.py in unstack(self, level, fill_value) 3888 from pandas.core.reshape.reshape import unstack 3889 -&gt; 3890 return unstack(self, level, fill_value) 3891 3892 # - /opt/miniconda3/lib/python3.7/site-packages/pandas/core/reshape/reshape.py in unstack(obj, level, fill_value) 423 return _unstack_extension_series(obj, level, fill_value) 424 unstacker = _Unstacker( --&gt; 425 obj.index, level=level, constructor=obj._constructor_expanddim, 426 ) 427 return unstacker.get_result( /opt/miniconda3/lib/python3.7/site-packages/pandas/core/reshape/reshape.py in __init__(self, index, level, constructor) 118 raise ValueError(&#34;Unstacked DataFrame is too big, causing int32 overflow&#34;) 119 --&gt; 120 self._make_selectors() 121 122 @cache_readonly /opt/miniconda3/lib/python3.7/site-packages/pandas/core/reshape/reshape.py in _make_selectors(self) 167 168 if mask.sum() &lt; len(self.index): --&gt; 169 raise ValueError(&#34;Index contains duplicate entries, cannot reshape&#34;) 170 171 self.group_index = comp_index ValueError: Index contains duplicate entries, cannot reshape . In such a case, we&#39;d use .pivot_table(). It will apply an aggregation function to our duplicates, in this case, we&#39;ll sum() them up: . df.pivot_table(index=&quot;Name&quot;, columns=&#39;Year&#39;, values=&#39;Courses&#39;, aggfunc=&#39;sum&#39;) . Year 2018 2019 2020 . Name . Mike 4 | 6 | 4 | . Tom 3 | 5 | 6 | . If we wanted to keep the numbers per department, we could specify both Name and Department as multiple indexes: . df.pivot_table(index=[&quot;Name&quot;, &quot;Department&quot;], columns=&#39;Year&#39;, values=&#39;Courses&#39;) . Year 2018 2019 2020 . Name Department . Mike CS 3 | 4 | 2 | . STATS 1 | 2 | 2 | . Tom CS 1 | 2 | 5 | . STATS 2 | 3 | 1 | . The result above is a mutlti-index or &quot;hierarchically indexed&quot; dataframe (more on those next chapter). If you ever have a need to use it, you can read more about pivot_table() in the documentation. . 4. Working with Multiple DataFrames . Often you&#39;ll work with multiple dataframes that you want to stick together or merge. df.merge() and df.concat() are all you need to know for combining dataframes. The Pandas documentation is very helpful for these functions, but they are pretty easy to grasp. . {note} The example joins shown in this section are inspired by [Chapter 15](https://stat545.com/join-cheatsheet.html) of Jenny Bryan&#39;s STAT 545 materials. . Sticking DataFrames Together with pd.concat() . You can use pd.concat() to stick dataframes together: . Vertically: if they have the same columns, OR | Horizontally: if they have the same rows | . df1 = pd.DataFrame({&#39;A&#39;: [1, 3, 5], &#39;B&#39;: [2, 4, 6]}) df2 = pd.DataFrame({&#39;A&#39;: [7, 9, 11], &#39;B&#39;: [8, 10, 12]}) . df1 . A B . 0 1 | 2 | . 1 3 | 4 | . 2 5 | 6 | . df2 . A B . 0 7 | 8 | . 1 9 | 10 | . 2 11 | 12 | . pd.concat((df1, df2), axis=0) # axis=0 specifies a vertical stick, i.e., on the columns . A B . 0 1 | 2 | . 1 3 | 4 | . 2 5 | 6 | . 0 7 | 8 | . 1 9 | 10 | . 2 11 | 12 | . Notice that the indexes were simply joined together? This may or may not be what you want. To reset the index, you can specify the argument ignore_index=True: . pd.concat((df1, df2), axis=0, ignore_index=True) . A B . 0 1 | 2 | . 1 3 | 4 | . 2 5 | 6 | . 3 7 | 8 | . 4 9 | 10 | . 5 11 | 12 | . Use axis=1 to stick together horizontally: . pd.concat((df1, df2), axis=1, ignore_index=True) . 0 1 2 3 . 0 1 | 2 | 7 | 8 | . 1 3 | 4 | 9 | 10 | . 2 5 | 6 | 11 | 12 | . You are not limited to just two dataframes, you can concatenate as many as you want: . pd.concat((df1, df2, df1, df2), axis=0, ignore_index=True) . A B . 0 1 | 2 | . 1 3 | 4 | . 2 5 | 6 | . 3 7 | 8 | . 4 9 | 10 | . 5 11 | 12 | . 6 1 | 2 | . 7 3 | 4 | . 8 5 | 6 | . 9 7 | 8 | . 10 9 | 10 | . 11 11 | 12 | . Joining DataFrames with pd.merge() . pd.merge() gives you the ability to &quot;join&quot; dataframes using different rules (just like with SQL if you&#39;re familiar with it). You can use df.merge() to join dataframes based on shared key columns. Methods include: . &quot;inner join&quot; | &quot;outer join&quot; | &quot;left join&quot; | &quot;right join&quot; | . See this great cheat sheet and these great animations for more insights. . df1 = pd.DataFrame({&quot;name&quot;: [&#39;Magneto&#39;, &#39;Storm&#39;, &#39;Mystique&#39;, &#39;Batman&#39;, &#39;Joker&#39;, &#39;Catwoman&#39;, &#39;Hellboy&#39;], &#39;alignment&#39;: [&#39;bad&#39;, &#39;good&#39;, &#39;bad&#39;, &#39;good&#39;, &#39;bad&#39;, &#39;bad&#39;, &#39;good&#39;], &#39;gender&#39;: [&#39;male&#39;, &#39;female&#39;, &#39;female&#39;, &#39;male&#39;, &#39;male&#39;, &#39;female&#39;, &#39;male&#39;], &#39;publisher&#39;: [&#39;Marvel&#39;, &#39;Marvel&#39;, &#39;Marvel&#39;, &#39;DC&#39;, &#39;DC&#39;, &#39;DC&#39;, &#39;Dark Horse Comics&#39;]}) df2 = pd.DataFrame({&#39;publisher&#39;: [&#39;DC&#39;, &#39;Marvel&#39;, &#39;Image&#39;], &#39;year_founded&#39;: [1934, 1939, 1992]}) . . An &quot;inner&quot; join will return all rows of df1 where matching values for &quot;publisher&quot; are found in df2: . pd.merge(df1, df2, how=&quot;inner&quot;, on=&quot;publisher&quot;) . name alignment gender publisher year_founded . 0 Magneto | bad | male | Marvel | 1939 | . 1 Storm | good | female | Marvel | 1939 | . 2 Mystique | bad | female | Marvel | 1939 | . 3 Batman | good | male | DC | 1934 | . 4 Joker | bad | male | DC | 1934 | . 5 Catwoman | bad | female | DC | 1934 | . . An &quot;outer&quot; join will return all rows of df1 and df2, placing NaNs where information is unavailable: . pd.merge(df1, df2, how=&quot;outer&quot;, on=&quot;publisher&quot;) . name alignment gender publisher year_founded . 0 Magneto | bad | male | Marvel | 1939.0 | . 1 Storm | good | female | Marvel | 1939.0 | . 2 Mystique | bad | female | Marvel | 1939.0 | . 3 Batman | good | male | DC | 1934.0 | . 4 Joker | bad | male | DC | 1934.0 | . 5 Catwoman | bad | female | DC | 1934.0 | . 6 Hellboy | good | male | Dark Horse Comics | NaN | . 7 NaN | NaN | NaN | Image | 1992.0 | . . Return all rows from df1 and all columns of df1 and df2, populated where matches occur: . pd.merge(df1, df2, how=&quot;left&quot;, on=&quot;publisher&quot;) . name alignment gender publisher year_founded . 0 Magneto | bad | male | Marvel | 1939.0 | . 1 Storm | good | female | Marvel | 1939.0 | . 2 Mystique | bad | female | Marvel | 1939.0 | . 3 Batman | good | male | DC | 1934.0 | . 4 Joker | bad | male | DC | 1934.0 | . 5 Catwoman | bad | female | DC | 1934.0 | . 6 Hellboy | good | male | Dark Horse Comics | NaN | . . pd.merge(df1, df2, how=&quot;right&quot;, on=&quot;publisher&quot;) . name alignment gender publisher year_founded . 0 Batman | good | male | DC | 1934 | . 1 Joker | bad | male | DC | 1934 | . 2 Catwoman | bad | female | DC | 1934 | . 3 Magneto | bad | male | Marvel | 1939 | . 4 Storm | good | female | Marvel | 1939 | . 5 Mystique | bad | female | Marvel | 1939 | . 6 NaN | NaN | NaN | Image | 1992 | . There are many ways to specify the key to join dataframes on, you can join on index values, different, column names, etc. Another helpful argument is the indicator argument which will add a column to the result telling you where matches were found in the dataframes: . pd.merge(df1, df2, how=&quot;outer&quot;, on=&quot;publisher&quot;, indicator=True) . name alignment gender publisher year_founded _merge . 0 Magneto | bad | male | Marvel | 1939.0 | both | . 1 Storm | good | female | Marvel | 1939.0 | both | . 2 Mystique | bad | female | Marvel | 1939.0 | both | . 3 Batman | good | male | DC | 1934.0 | both | . 4 Joker | bad | male | DC | 1934.0 | both | . 5 Catwoman | bad | female | DC | 1934.0 | both | . 6 Hellboy | good | male | Dark Horse Comics | NaN | left_only | . 7 NaN | NaN | NaN | Image | 1992.0 | right_only | . By the way, you can use pd.concat() to do a simple &quot;inner&quot; or &quot;outer&quot; join on multiple datadrames at once. It&#39;s less flexible than merge, but can be useful sometimes. . 5. More DataFrame Operations . Applying Custom Functions . There will be times when you want to apply a function that is not built-in to Pandas. For this, we also have methods: . df.apply(), applies a function column-wise or row-wise across a dataframe (the function must be able to accept/return an array) | df.applymap(), applies a function element-wise (for functions that accept/return single values at a time) | series.apply()/series.map(), same as above but for Pandas series | . For example, say you want to use a numpy function on a column in your dataframe: . df = pd.read_csv(&#39;data/cycling_data.csv&#39;) df[[&#39;Time&#39;, &#39;Distance&#39;]].apply(np.sin) . Time Distance . 0 -0.901866 | 0.053604 | . 1 -0.901697 | 0.447197 | . 2 -0.035549 | -0.046354 | . 3 -0.739059 | 0.270228 | . 4 -0.236515 | -0.086263 | . ... ... | ... | . 28 -0.683372 | 0.063586 | . 29 0.150056 | 0.133232 | . 30 0.026702 | 0.023627 | . 31 -0.008640 | 0.221770 | . 32 0.897861 | -0.700695 | . 33 rows √ó 2 columns . Or you may want to apply your own custom function: . def seconds_to_hours(x): return x / 3600 df[[&#39;Time&#39;]].apply(seconds_to_hours) . Time . 0 0.578889 | . 1 0.703056 | . 2 0.517500 | . 3 0.608889 | . 4 0.525278 | . ... ... | . 28 0.519444 | . 29 0.596944 | . 30 0.511389 | . 31 0.684167 | . 32 0.511944 | . 33 rows √ó 1 columns . This may have been better as a lambda function... . df[[&#39;Time&#39;]].apply(lambda x: x / 3600) . Time . 0 0.578889 | . 1 0.703056 | . 2 0.517500 | . 3 0.608889 | . 4 0.525278 | . ... ... | . 28 0.519444 | . 29 0.596944 | . 30 0.511389 | . 31 0.684167 | . 32 0.511944 | . 33 rows √ó 1 columns . You can even use functions that require additional arguments. Just specify the arguments in .apply(): . def convert_seconds(x, to=&quot;hours&quot;): if to == &quot;hours&quot;: return x / 3600 elif to == &quot;minutes&quot;: return x / 60 df[[&#39;Time&#39;]].apply(convert_seconds, to=&quot;minutes&quot;) . Time . 0 34.733333 | . 1 42.183333 | . 2 31.050000 | . 3 36.533333 | . 4 31.516667 | . ... ... | . 28 31.166667 | . 29 35.816667 | . 30 30.683333 | . 31 41.050000 | . 32 30.716667 | . 33 rows √ó 1 columns . Some functions only accept/return a scalar: . int(3.141) . 3 . float([3.141, 10.345]) . TypeError Traceback (most recent call last) &lt;ipython-input-62-97f9c30c3ff1&gt; in &lt;module&gt; -&gt; 1 float([3.141, 10.345]) TypeError: float() argument must be a string or a number, not &#39;list&#39; . For these, we need .applymap(): . df[[&#39;Time&#39;]].applymap(int) . Time . 0 2084 | . 1 2531 | . 2 1863 | . 3 2192 | . 4 1891 | . ... ... | . 28 1870 | . 29 2149 | . 30 1841 | . 31 2463 | . 32 1843 | . 33 rows √ó 1 columns . However, there are often &quot;vectorized&quot; versions of common functions like this already available, which are much faster. In the case above, we can use .astype() to change the dtype of a whole column quickly: . time_applymap = %timeit -q -o -r 3 df[[&#39;Time&#39;]].applymap(float) time_builtin = %timeit -q -o -r 3 df[[&#39;Time&#39;]].astype(float) print(f&quot;&#39;astype&#39; is {time_applymap.average / time_builtin.average:.2f} faster than &#39;applymap&#39;!&quot;) . &#39;astype&#39; is 1.98 faster than &#39;applymap&#39;! . Grouping . Often we are interested in examining specific groups in our data. df.groupby() allows us to group our data based on a variable(s). . df = pd.read_csv(&#39;data/cycling_data.csv&#39;) df . Date Name Type Time Distance Comments . 0 10 Sep 2019, 00:13:04 | Afternoon Ride | Ride | 2084 | 12.62 | Rain | . 1 10 Sep 2019, 13:52:18 | Morning Ride | Ride | 2531 | 13.03 | rain | . 2 11 Sep 2019, 00:23:50 | Afternoon Ride | Ride | 1863 | 12.52 | Wet road but nice weather | . 3 11 Sep 2019, 14:06:19 | Morning Ride | Ride | 2192 | 12.84 | Stopped for photo of sunrise | . 4 12 Sep 2019, 00:28:05 | Afternoon Ride | Ride | 1891 | 12.48 | Tired by the end of the week | . ... ... | ... | ... | ... | ... | ... | . 28 4 Oct 2019, 01:08:08 | Afternoon Ride | Ride | 1870 | 12.63 | Very tired, riding into the wind | . 29 9 Oct 2019, 13:55:40 | Morning Ride | Ride | 2149 | 12.70 | Really cold! But feeling good | . 30 10 Oct 2019, 00:10:31 | Afternoon Ride | Ride | 1841 | 12.59 | Feeling good after a holiday break! | . 31 10 Oct 2019, 13:47:14 | Morning Ride | Ride | 2463 | 12.79 | Stopped for photo of sunrise | . 32 11 Oct 2019, 00:16:57 | Afternoon Ride | Ride | 1843 | 11.79 | Bike feeling tight, needs an oil and pump | . 33 rows √ó 6 columns . Let&#39;s group this dataframe on the column Name: . dfg = df.groupby(by=&#39;Name&#39;) dfg . &lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x1577bb150&gt; . What is a DataFrameGroupBy object? It contains information about the groups of the dataframe: . . The groupby object is really just a dictionary of index-mappings, which we could look at if we wanted to: . dfg.groups . {&#39;Afternoon Ride&#39;: [0, 2, 4, 6, 9, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32], &#39;Morning Ride&#39;: [1, 3, 5, 7, 8, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31]} . We can also access a group using the .get_group() method: . dfg.get_group(&#39;Afternoon Ride&#39;) . Date Name Type Time Distance Comments . 0 10 Sep 2019, 00:13:04 | Afternoon Ride | Ride | 2084 | 12.62 | Rain | . 2 11 Sep 2019, 00:23:50 | Afternoon Ride | Ride | 1863 | 12.52 | Wet road but nice weather | . 4 12 Sep 2019, 00:28:05 | Afternoon Ride | Ride | 1891 | 12.48 | Tired by the end of the week | . 6 17 Sep 2019, 00:15:47 | Afternoon Ride | Ride | 1973 | 12.45 | Legs feeling strong! | . 9 18 Sep 2019, 00:15:52 | Afternoon Ride | Ride | 2101 | 12.48 | Pumped up tires | . 10 19 Sep 2019, 00:30:01 | Afternoon Ride | Ride | 48062 | 12.48 | Feeling good | . 12 20 Sep 2019, 01:02:05 | Afternoon Ride | Ride | 2961 | 12.81 | Feeling good | . 14 24 Sep 2019, 00:35:42 | Afternoon Ride | Ride | 2076 | 12.47 | Oiled chain, bike feels smooth | . 16 25 Sep 2019, 00:07:21 | Afternoon Ride | Ride | 1775 | 12.10 | Feeling really tired | . 18 26 Sep 2019, 00:13:33 | Afternoon Ride | Ride | 1860 | 12.52 | raining | . 20 27 Sep 2019, 01:00:18 | Afternoon Ride | Ride | 1712 | 12.47 | Tired by the end of the week | . 22 1 Oct 2019, 00:15:07 | Afternoon Ride | Ride | 1732 | NaN | Legs feeling strong! | . 24 2 Oct 2019, 00:13:09 | Afternoon Ride | Ride | 1756 | NaN | A little tired today but good weather | . 26 3 Oct 2019, 00:45:22 | Afternoon Ride | Ride | 1724 | 12.52 | Feeling good | . 28 4 Oct 2019, 01:08:08 | Afternoon Ride | Ride | 1870 | 12.63 | Very tired, riding into the wind | . 30 10 Oct 2019, 00:10:31 | Afternoon Ride | Ride | 1841 | 12.59 | Feeling good after a holiday break! | . 32 11 Oct 2019, 00:16:57 | Afternoon Ride | Ride | 1843 | 11.79 | Bike feeling tight, needs an oil and pump | . The usual thing to do however, is to apply aggregate functions to the groupby object: . . dfg.mean() . Time Distance . Name . Afternoon Ride 4654.352941 | 12.462 | . Morning Ride 2299.875000 | 12.860 | . We can apply multiple functions using .aggregate(): . dfg.aggregate([&#39;mean&#39;, &#39;sum&#39;, &#39;count&#39;]) . Time Distance . mean sum count mean sum count . Name . Afternoon Ride 4654.352941 | 79124 | 17 | 12.462 | 186.93 | 15 | . Morning Ride 2299.875000 | 36798 | 16 | 12.860 | 205.76 | 16 | . And even apply different functions to different columns: . def num_range(x): return x.max() - x.min() dfg.aggregate({&quot;Time&quot;: [&#39;max&#39;, &#39;min&#39;, &#39;mean&#39;, num_range], &quot;Distance&quot;: [&#39;sum&#39;]}) . Time Distance . max min mean num_range sum . Name . Afternoon Ride 48062 | 1712 | 4654.352941 | 46350 | 186.93 | . Morning Ride 2903 | 2090 | 2299.875000 | 813 | 205.76 | . By the way, you can use aggregate for non-grouped dataframes too. This is pretty much what df.describe does under-the-hood: . df.agg([&#39;mean&#39;, &#39;min&#39;, &#39;count&#39;, num_range]) . Date Name Type Time Distance Comments . min 1 Oct 2019, 00:15:07 | Afternoon Ride | Ride | 1712.000000 | 11.790000 | A little tired today but good weather | . count 33 | 33 | 33 | 33.000000 | 31.000000 | 33 | . mean NaN | NaN | NaN | 3512.787879 | 12.667419 | NaN | . num_range NaN | NaN | NaN | 46350.000000 | 2.780000 | NaN | . Dealing with Missing Values . Missing values are typically denoted with NaN. We can use df.isnull() to find missing values in a dataframe. It returns a boolean for each element in the dataframe: . df.isnull() . Date Name Type Time Distance Comments . 0 False | False | False | False | False | False | . 1 False | False | False | False | False | False | . 2 False | False | False | False | False | False | . 3 False | False | False | False | False | False | . 4 False | False | False | False | False | False | . ... ... | ... | ... | ... | ... | ... | . 28 False | False | False | False | False | False | . 29 False | False | False | False | False | False | . 30 False | False | False | False | False | False | . 31 False | False | False | False | False | False | . 32 False | False | False | False | False | False | . 33 rows √ó 6 columns . But it&#39;s usually more helpful to get this information by row or by column using the .any() or .info() method: . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 33 entries, 0 to 32 Data columns (total 6 columns): # Column Non-Null Count Dtype -- -- 0 Date 33 non-null object 1 Name 33 non-null object 2 Type 33 non-null object 3 Time 33 non-null int64 4 Distance 31 non-null float64 5 Comments 33 non-null object dtypes: float64(1), int64(1), object(4) memory usage: 1.7+ KB . df[df.isnull().any(axis=1)] . Date Name Type Time Distance Comments . 22 1 Oct 2019, 00:15:07 | Afternoon Ride | Ride | 1732 | NaN | Legs feeling strong! | . 24 2 Oct 2019, 00:13:09 | Afternoon Ride | Ride | 1756 | NaN | A little tired today but good weather | . When you have missing values, we usually either drop them or impute them.You can drop missing values with df.dropna(): . df.dropna() . Date Name Type Time Distance Comments . 0 10 Sep 2019, 00:13:04 | Afternoon Ride | Ride | 2084 | 12.62 | Rain | . 1 10 Sep 2019, 13:52:18 | Morning Ride | Ride | 2531 | 13.03 | rain | . 2 11 Sep 2019, 00:23:50 | Afternoon Ride | Ride | 1863 | 12.52 | Wet road but nice weather | . 3 11 Sep 2019, 14:06:19 | Morning Ride | Ride | 2192 | 12.84 | Stopped for photo of sunrise | . 4 12 Sep 2019, 00:28:05 | Afternoon Ride | Ride | 1891 | 12.48 | Tired by the end of the week | . ... ... | ... | ... | ... | ... | ... | . 28 4 Oct 2019, 01:08:08 | Afternoon Ride | Ride | 1870 | 12.63 | Very tired, riding into the wind | . 29 9 Oct 2019, 13:55:40 | Morning Ride | Ride | 2149 | 12.70 | Really cold! But feeling good | . 30 10 Oct 2019, 00:10:31 | Afternoon Ride | Ride | 1841 | 12.59 | Feeling good after a holiday break! | . 31 10 Oct 2019, 13:47:14 | Morning Ride | Ride | 2463 | 12.79 | Stopped for photo of sunrise | . 32 11 Oct 2019, 00:16:57 | Afternoon Ride | Ride | 1843 | 11.79 | Bike feeling tight, needs an oil and pump | . 31 rows √ó 6 columns . Or you can impute (&quot;fill&quot;) them using .fillna(). This method has various options for filling, you can use a fixed value, the mean of the column, the previous non-nan value, etc: . df = pd.DataFrame([[np.nan, 2, np.nan, 0], [3, 4, np.nan, 1], [np.nan, np.nan, np.nan, 5], [np.nan, 3, np.nan, 4]], columns=list(&#39;ABCD&#39;)) df . A B C D . 0 NaN | 2.0 | NaN | 0 | . 1 3.0 | 4.0 | NaN | 1 | . 2 NaN | NaN | NaN | 5 | . 3 NaN | 3.0 | NaN | 4 | . df.fillna(0) # fill with 0 . A B C D . 0 0.0 | 2.0 | 0.0 | 0 | . 1 3.0 | 4.0 | 0.0 | 1 | . 2 0.0 | 0.0 | 0.0 | 5 | . 3 0.0 | 3.0 | 0.0 | 4 | . df.fillna(df.mean()) # fill with the mean . A B C D . 0 3.0 | 2.0 | NaN | 0 | . 1 3.0 | 4.0 | NaN | 1 | . 2 3.0 | 3.0 | NaN | 5 | . 3 3.0 | 3.0 | NaN | 4 | . df.fillna(method=&#39;bfill&#39;) # backward (upwards) fill from non-nan values . A B C D . 0 3.0 | 2.0 | NaN | 0 | . 1 3.0 | 4.0 | NaN | 1 | . 2 NaN | 3.0 | NaN | 5 | . 3 NaN | 3.0 | NaN | 4 | . df.fillna(method=&#39;ffill&#39;) # forward (downward) fill from non-nan values . A B C D . 0 NaN | 2.0 | NaN | 0 | . 1 3.0 | 4.0 | NaN | 1 | . 2 3.0 | 4.0 | NaN | 5 | . 3 3.0 | 3.0 | NaN | 4 | . Finally, sometimes I use visualizations to help identify (patterns in) missing values. One thing I often do is print a heatmap of my dataframe to get a feel for where my missing values are. If you want to run this code, you may need to install seaborn: . conda install seaborn . import seaborn as sns sns.set(rc={&#39;figure.figsize&#39;:(7, 7)}) . df . A B C D . 0 NaN | 2.0 | NaN | 0 | . 1 3.0 | 4.0 | NaN | 1 | . 2 NaN | NaN | NaN | 5 | . 3 NaN | 3.0 | NaN | 4 | . sns.heatmap(df.isnull(), cmap=&#39;viridis&#39;, cbar=False); . np.random.seed(2020) npx = np.zeros((100,20)) mask = np.random.choice([True, False], npx.shape, p=[.1, .9]) npx[mask] = np.nan sns.heatmap(pd.DataFrame(npx).isnull(), cmap=&#39;viridis&#39;, cbar=False); .",
            "url": "https://millermuttu.github.io/blog/jupyter/python/data%20science/programming/pandas/2022/05/19/chapter8-wrangling-basics.html",
            "relUrl": "/jupyter/python/data%20science/programming/pandas/2022/05/19/chapter8-wrangling-basics.html",
            "date": " ‚Ä¢ May 19, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Data science revision - chapter 7",
            "content": ". Chapter 7: Introduction to Pandas . Chapter Outline . . 1. Introduction to Pandas | 2. Pandas Series | 3. Pandas DataFrames | 4. Why ndarrays and Series and DataFrames? | . Chapter Learning Objectives . Create Pandas series with pd.Series() and Pandas dataframe with pd.DataFrame() | Be able to access values from a Series/DataFrame by indexing, slicing and boolean indexing using notation such as df[], df.loc[], df.iloc[], df.query[] | Perform basic arithmetic operations between two series and anticipate the result. | Describe how Pandas assigns dtypes to Series and what the object dtype is | Read a standard .csv file from a local path or url using Pandas pd.read_csv(). | Explain the relationship and differences between np.ndarray, pd.Series and pd.DataFrame objects in Python. | . 1. Introduction to Pandas . . Pandas is most popular Python library for tabular data structures. You can think of Pandas as an extremely powerful version of Excel (but free and with a lot more features!) . Pandas can be installed using conda: . conda install pandas . We usually import pandas with the alias pd. You&#39;ll see these two imports at the top of most data science workflows: . import pandas as pd import numpy as np . 2. Pandas Series . What are Series? . A Series is like a NumPy array but with labels. They are strictly 1-dimensional and can contain any data type (integers, strings, floats, objects, etc), including a mix of them. Series can be created from a scalar, a list, ndarray or dictionary using pd.Series() (note the captial &quot;S&quot;). Here are some example series: . . Creating Series . By default, series are labelled with indices starting from 0. For example: . pd.Series(data = [-5, 1.3, 21, 6, 3]) . 0 -5.0 1 1.3 2 21.0 3 6.0 4 3.0 dtype: float64 . But you can add a custom index: . pd.Series(data = [-5, 1.3, 21, 6, 3], index = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;]) . a -5.0 b 1.3 c 21.0 d 6.0 e 3.0 dtype: float64 . You can create a Series from a dictionary: . pd.Series(data = {&#39;a&#39;: 10, &#39;b&#39;: 20, &#39;c&#39;: 30}) . a 10 b 20 c 30 dtype: int64 . Or from an ndarray: . pd.Series(data = np.random.randn(3)) . 0 -0.428301 1 -0.104959 2 0.170835 dtype: float64 . Or even a scalar: . pd.Series(3.141) . 0 3.141 dtype: float64 . pd.Series(data=3.141, index=[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;]) . a 3.141 b 3.141 c 3.141 dtype: float64 . Series Characteristics . Series can be given a name attribute. I almost never use this but it might come up sometimes: . s = pd.Series(data = np.random.randn(5), name=&#39;random_series&#39;) s . 0 -1.363190 1 0.425801 2 -0.048966 3 -0.298172 4 1.899199 Name: random_series, dtype: float64 . s.name . &#39;random_series&#39; . s.rename(&quot;another_name&quot;) . 0 -1.363190 1 0.425801 2 -0.048966 3 -0.298172 4 1.899199 Name: another_name, dtype: float64 . You can access the index labels of your series using the .index attribute: . s.index . RangeIndex(start=0, stop=5, step=1) . You can access the underlying data array using .to_numpy(): . s.to_numpy() . array([-1.36319006, 0.42580052, -0.04896627, -0.29817227, 1.89919866]) . pd.Series([[1, 2, 3], &quot;b&quot;, 1]).to_numpy() . array([list([1, 2, 3]), &#39;b&#39;, 1], dtype=object) . Indexing and Slicing Series . Series are very much like ndarrays (in fact, series can be passed to most NumPy functions!). They can be indexed using square brackets [ ] and sliced using colon : notation: . s = pd.Series(data = range(5), index = [&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;, &#39;E&#39;]) s . A 0 B 1 C 2 D 3 E 4 dtype: int64 . s[0] . 0 . s[[1, 2, 3]] . B 1 C 2 D 3 dtype: int64 . s[0:3] . A 0 B 1 C 2 dtype: int64 . Note above how array-based indexing and slicing also returns the series index. . Series are also like dictionaries, in that we can access values using index labels: . s[&quot;A&quot;] . 0 . s[[&quot;B&quot;, &quot;D&quot;, &quot;C&quot;]] . B 1 D 3 C 2 dtype: int64 . s[&quot;A&quot;:&quot;C&quot;] . A 0 B 1 C 2 dtype: int64 . &quot;A&quot; in s . True . &quot;Z&quot; in s . False . Series do allow for non-unique indexing, but be careful because indexing operations won&#39;t return unique values: . x = pd.Series(data = range(5), index = [&quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;B&quot;, &quot;C&quot;]) x . A 0 A 1 A 2 B 3 C 4 dtype: int64 . x[&quot;A&quot;] . A 0 A 1 A 2 dtype: int64 . Finally, we can also do boolean indexing with series: . s[s &gt;= 1] . B 1 C 2 D 3 E 4 dtype: int64 . s[s &gt; s.mean()] . D 3 E 4 dtype: int64 . (s != 1) . A True B False C True D True E True dtype: bool . Series Operations . Unlike ndarrays operations between Series (+, -, /, *) align values based on their LABELS (not their position in the structure). The resulting index will be the sorted union of the two indexes. This gives you the flexibility to run operations on series regardless of their labels. . s1 = pd.Series(data = range(4), index = [&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;]) s1 . A 0 B 1 C 2 D 3 dtype: int64 . s2 = pd.Series(data = range(10, 14), index = [&quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;]) s2 . B 10 C 11 D 12 E 13 dtype: int64 . s1 + s2 . A NaN B 11.0 C 13.0 D 15.0 E NaN dtype: float64 . As you can see above, indices that match will be operated on. Indices that don&#39;t match will appear in the product but with NaN values: . . We can also perform standard operations on a series, like multiplying or squaring. NumPy also accepts series as an argument to most functions because series are built off numpy arrays (more on that later): . s1 ** 2 . A 0 B 1 C 4 D 9 dtype: int64 . np.exp(s1) . A 1.000000 B 2.718282 C 7.389056 D 20.085537 dtype: float64 . Finally, just like arrays, series have many built-in methods for various operations. You can find them all by running help(pd.Series): . print([_ for _ in dir(pd.Series) if not _.startswith(&quot;_&quot;)]) # print all common methods . [&#39;T&#39;, &#39;abs&#39;, &#39;add&#39;, &#39;add_prefix&#39;, &#39;add_suffix&#39;, &#39;agg&#39;, &#39;aggregate&#39;, &#39;align&#39;, &#39;all&#39;, &#39;any&#39;, &#39;append&#39;, &#39;apply&#39;, &#39;argmax&#39;, &#39;argmin&#39;, &#39;argsort&#39;, &#39;array&#39;, &#39;asfreq&#39;, &#39;asof&#39;, &#39;astype&#39;, &#39;at&#39;, &#39;at_time&#39;, &#39;attrs&#39;, &#39;autocorr&#39;, &#39;axes&#39;, &#39;backfill&#39;, &#39;between&#39;, &#39;between_time&#39;, &#39;bfill&#39;, &#39;bool&#39;, &#39;cat&#39;, &#39;clip&#39;, &#39;combine&#39;, &#39;combine_first&#39;, &#39;compare&#39;, &#39;convert_dtypes&#39;, &#39;copy&#39;, &#39;corr&#39;, &#39;count&#39;, &#39;cov&#39;, &#39;cummax&#39;, &#39;cummin&#39;, &#39;cumprod&#39;, &#39;cumsum&#39;, &#39;describe&#39;, &#39;diff&#39;, &#39;div&#39;, &#39;divide&#39;, &#39;divmod&#39;, &#39;dot&#39;, &#39;drop&#39;, &#39;drop_duplicates&#39;, &#39;droplevel&#39;, &#39;dropna&#39;, &#39;dt&#39;, &#39;dtype&#39;, &#39;dtypes&#39;, &#39;duplicated&#39;, &#39;empty&#39;, &#39;eq&#39;, &#39;equals&#39;, &#39;ewm&#39;, &#39;expanding&#39;, &#39;explode&#39;, &#39;factorize&#39;, &#39;ffill&#39;, &#39;fillna&#39;, &#39;filter&#39;, &#39;first&#39;, &#39;first_valid_index&#39;, &#39;floordiv&#39;, &#39;ge&#39;, &#39;get&#39;, &#39;groupby&#39;, &#39;gt&#39;, &#39;hasnans&#39;, &#39;head&#39;, &#39;hist&#39;, &#39;iat&#39;, &#39;idxmax&#39;, &#39;idxmin&#39;, &#39;iloc&#39;, &#39;index&#39;, &#39;infer_objects&#39;, &#39;interpolate&#39;, &#39;is_monotonic&#39;, &#39;is_monotonic_decreasing&#39;, &#39;is_monotonic_increasing&#39;, &#39;is_unique&#39;, &#39;isin&#39;, &#39;isna&#39;, &#39;isnull&#39;, &#39;item&#39;, &#39;items&#39;, &#39;iteritems&#39;, &#39;keys&#39;, &#39;kurt&#39;, &#39;kurtosis&#39;, &#39;last&#39;, &#39;last_valid_index&#39;, &#39;le&#39;, &#39;loc&#39;, &#39;lt&#39;, &#39;mad&#39;, &#39;map&#39;, &#39;mask&#39;, &#39;max&#39;, &#39;mean&#39;, &#39;median&#39;, &#39;memory_usage&#39;, &#39;min&#39;, &#39;mod&#39;, &#39;mode&#39;, &#39;mul&#39;, &#39;multiply&#39;, &#39;name&#39;, &#39;nbytes&#39;, &#39;ndim&#39;, &#39;ne&#39;, &#39;nlargest&#39;, &#39;notna&#39;, &#39;notnull&#39;, &#39;nsmallest&#39;, &#39;nunique&#39;, &#39;pad&#39;, &#39;pct_change&#39;, &#39;pipe&#39;, &#39;plot&#39;, &#39;pop&#39;, &#39;pow&#39;, &#39;prod&#39;, &#39;product&#39;, &#39;quantile&#39;, &#39;radd&#39;, &#39;rank&#39;, &#39;ravel&#39;, &#39;rdiv&#39;, &#39;rdivmod&#39;, &#39;reindex&#39;, &#39;reindex_like&#39;, &#39;rename&#39;, &#39;rename_axis&#39;, &#39;reorder_levels&#39;, &#39;repeat&#39;, &#39;replace&#39;, &#39;resample&#39;, &#39;reset_index&#39;, &#39;rfloordiv&#39;, &#39;rmod&#39;, &#39;rmul&#39;, &#39;rolling&#39;, &#39;round&#39;, &#39;rpow&#39;, &#39;rsub&#39;, &#39;rtruediv&#39;, &#39;sample&#39;, &#39;searchsorted&#39;, &#39;sem&#39;, &#39;set_axis&#39;, &#39;shape&#39;, &#39;shift&#39;, &#39;size&#39;, &#39;skew&#39;, &#39;slice_shift&#39;, &#39;sort_index&#39;, &#39;sort_values&#39;, &#39;sparse&#39;, &#39;squeeze&#39;, &#39;std&#39;, &#39;str&#39;, &#39;sub&#39;, &#39;subtract&#39;, &#39;sum&#39;, &#39;swapaxes&#39;, &#39;swaplevel&#39;, &#39;tail&#39;, &#39;take&#39;, &#39;to_clipboard&#39;, &#39;to_csv&#39;, &#39;to_dict&#39;, &#39;to_excel&#39;, &#39;to_frame&#39;, &#39;to_hdf&#39;, &#39;to_json&#39;, &#39;to_latex&#39;, &#39;to_list&#39;, &#39;to_markdown&#39;, &#39;to_numpy&#39;, &#39;to_period&#39;, &#39;to_pickle&#39;, &#39;to_sql&#39;, &#39;to_string&#39;, &#39;to_timestamp&#39;, &#39;to_xarray&#39;, &#39;tolist&#39;, &#39;transform&#39;, &#39;transpose&#39;, &#39;truediv&#39;, &#39;truncate&#39;, &#39;tshift&#39;, &#39;tz_convert&#39;, &#39;tz_localize&#39;, &#39;unique&#39;, &#39;unstack&#39;, &#39;update&#39;, &#39;value_counts&#39;, &#39;values&#39;, &#39;var&#39;, &#39;view&#39;, &#39;where&#39;, &#39;xs&#39;] . s1 . A 0 B 1 C 2 D 3 dtype: int64 . s1.mean() . 1.5 . s1.sum() . 6 . s1.astype(float) . A 0.0 B 1.0 C 2.0 D 3.0 dtype: float64 . &quot;Chaining&quot; operations together is also common with pandas: . s1.add(3.141).pow(2).mean().astype(int) . 22 . Data Types . Series can hold all the data types (dtypes) you&#39;re used to, e.g., int, float, bool, etc. There are a few other special data types too (object, DateTime and Categorical) which we&#39;ll talk about in this and later chapters. You can always read more about pandas dtypes in the documentation too. For example, here&#39;s a series of dtype int64: . x = pd.Series(range(5)) x.dtype . dtype(&#39;int64&#39;) . The dtype &quot;object&quot; is used for series of strings or mixed data. Pandas is currently experimenting with a dedicated string dtype StringDtype, but it is still in testing. . x = pd.Series([&#39;A&#39;, &#39;B&#39;]) x . 0 A 1 B dtype: object . x = pd.Series([&#39;A&#39;, 1, [&quot;I&quot;, &quot;AM&quot;, &quot;A&quot;, &quot;LIST&quot;]]) x . 0 A 1 1 2 [I, AM, A, LIST] dtype: object . While flexible, it is recommended to avoid the use of object dtypes because of higher memory requirements. Essentially, in an object dtype series, every single element stores information about its individual dtype. We can inspect the dtypes of all the elements in a mixed series in several ways, below I&#39;ll use the map method: . x.map(type) . 0 &lt;class &#39;str&#39;&gt; 1 &lt;class &#39;int&#39;&gt; 2 &lt;class &#39;list&#39;&gt; dtype: object . We can see that each object in our series has a different dtype. This comes at a cost. Compare the memory usage of the series below: . x1 = pd.Series([1, 2, 3]) print(f&quot;x1 dtype: {x1.dtype}&quot;) print(f&quot;x1 memory usage: {x1.memory_usage(deep=True)} bytes&quot;) print(&quot;&quot;) x2 = pd.Series([1, 2, &quot;3&quot;]) print(f&quot;x2 dtype: {x2.dtype}&quot;) print(f&quot;x2 memory usage: {x2.memory_usage(deep=True)} bytes&quot;) print(&quot;&quot;) x3 = pd.Series([1, 2, &quot;3&quot;]).astype(&#39;int8&#39;) # coerce the object series to int8 print(f&quot;x3 dtype: {x3.dtype}&quot;) print(f&quot;x3 memory usage: {x3.memory_usage(deep=True)} bytes&quot;) . x1 dtype: int64 x1 memory usage: 152 bytes x2 dtype: object x2 memory usage: 258 bytes x3 dtype: int8 x3 memory usage: 131 bytes . In summary, try to use uniform dtypes where possible - they are more memory efficient! . One more gotcha, NaN (frequently used to represent missing values in data) is a float: . type(np.NaN) . float . This can be problematic if you have a series of integers and one missing value, because Pandas will cast the whole series to a float: . pd.Series([1, 2, 3, np.NaN]) . 0 1.0 1 2.0 2 3.0 3 NaN dtype: float64 . Only recently, Pandas has implemented a &quot;nullable integer dtype&quot;, which can handle NaN in an integer series without affecting the dtype. Note the captial &quot;I&quot; in the type below, differentiating it from numpy&#39;s int64 dtype: . pd.Series([1, 2, 3, np.NaN]).astype(&#39;Int64&#39;) . 0 1 1 2 2 3 3 &lt;NA&gt; dtype: Int64 . This is not the default in Pandas yet and functionality of this new feature is still subject to change. . 3. Pandas DataFrames . What are DataFrames? . Pandas DataFrames are you&#39;re new best friend. They are like the Excel spreadsheets you may be used to. DataFrames are really just Series stuck together! Think of a DataFrame as a dictionary of series, with the &quot;keys&quot; being the column labels and the &quot;values&quot; being the series data: . . Creating DataFrames . Dataframes can be created using pd.DataFrame() (note the capital &quot;D&quot; and &quot;F&quot;). Like series, index and column labels of dataframes are labelled starting from 0 by default: . pd.DataFrame([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) . 0 1 2 . 0 1 | 2 | 3 | . 1 4 | 5 | 6 | . 2 7 | 8 | 9 | . We can use the index and columns arguments to give them labels: . pd.DataFrame([[1, 2, 3], [4, 5, 6], [7, 8, 9]], index = [&quot;R1&quot;, &quot;R2&quot;, &quot;R3&quot;], columns = [&quot;C1&quot;, &quot;C2&quot;, &quot;C3&quot;]) . C1 C2 C3 . R1 1 | 2 | 3 | . R2 4 | 5 | 6 | . R3 7 | 8 | 9 | . There are so many ways to create dataframes. I most often create them from dictionaries or ndarrays: . pd.DataFrame({&quot;C1&quot;: [1, 2, 3], &quot;C2&quot;: [&#39;A&#39;, &#39;B&#39;, &#39;C&#39;]}, index=[&quot;R1&quot;, &quot;R2&quot;, &quot;R3&quot;]) . C1 C2 . R1 1 | A | . R2 2 | B | . R3 3 | C | . pd.DataFrame(np.random.randn(5, 5), index=[f&quot;row_{_}&quot; for _ in range(1, 6)], columns=[f&quot;col_{_}&quot; for _ in range(1, 6)]) . col_1 col_2 col_3 col_4 col_5 . row_1 -1.511598 | -1.073875 | 2.990474 | 2.408082 | 0.101569 | . row_2 0.767246 | 0.423030 | -0.135450 | 0.369545 | 0.761417 | . row_3 0.714677 | 1.489430 | 0.843088 | -1.284666 | 1.310033 | . row_4 -0.513656 | 0.539531 | 0.207057 | 0.425888 | 0.481794 | . row_5 -1.361988 | -0.479158 | 0.158281 | -0.196813 | 0.136745 | . pd.DataFrame(np.array([[&#39;Tom&#39;, 7], [&#39;Mike&#39;, 15], [&#39;Tiffany&#39;, 3]])) . 0 1 . 0 Tom | 7 | . 1 Mike | 15 | . 2 Tiffany | 3 | . Here&#39;s a table of the main ways you can create dataframes (see the Pandas documentation for more): . Create DataFrame from Code . Lists of lists | pd.DataFrame([[&#39;Tom&#39;, 7], [&#39;Mike&#39;, 15], [&#39;Tiffany&#39;, 3]]) | . ndarray | pd.DataFrame(np.array([[&#39;Tom&#39;, 7], [&#39;Mike&#39;, 15], [&#39;Tiffany&#39;, 3]])) | . Dictionary | pd.DataFrame({&quot;Name&quot;: [&#39;Tom&#39;, &#39;Mike&#39;, &#39;Tiffany&#39;], &quot;Number&quot;: [7, 15, 3]}) | . List of tuples | pd.DataFrame(zip([&#39;Tom&#39;, &#39;Mike&#39;, &#39;Tiffany&#39;], [7, 15, 3])) | . Series | pd.DataFrame({&quot;Name&quot;: pd.Series([&#39;Tom&#39;, &#39;Mike&#39;, &#39;Tiffany&#39;]), &quot;Number&quot;: pd.Series([7, 15, 3])}) | . Indexing and Slicing DataFrames . There are several main ways to select data from a DataFrame: . [] | .loc[] | .iloc[] | Boolean indexing | .query() | df = pd.DataFrame({&quot;Name&quot;: [&quot;Tom&quot;, &quot;Mike&quot;, &quot;Tiffany&quot;], &quot;Language&quot;: [&quot;Python&quot;, &quot;Python&quot;, &quot;R&quot;], &quot;Courses&quot;: [5, 4, 7]}) df . Name Language Courses . 0 Tom | Python | 5 | . 1 Mike | Python | 4 | . 2 Tiffany | R | 7 | . Indexing with [] . Select columns by single labels, lists of labels, or slices: . df[&#39;Name&#39;] # returns a series . 0 Tom 1 Mike 2 Tiffany Name: Name, dtype: object . df[[&#39;Name&#39;]] # returns a dataframe! . Name . 0 Tom | . 1 Mike | . 2 Tiffany | . df[[&#39;Name&#39;, &#39;Language&#39;]] . Name Language . 0 Tom | Python | . 1 Mike | Python | . 2 Tiffany | R | . You can only index rows by using slices, not single values (but not recommended, see preferred methods below). . df[0] # doesn&#39;t work . KeyError Traceback (most recent call last) /opt/miniconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance) 2888 try: -&gt; 2889 return self._engine.get_loc(casted_key) 2890 except KeyError as err: pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc() pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc() pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item() pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item() KeyError: 0 The above exception was the direct cause of the following exception: KeyError Traceback (most recent call last) &lt;ipython-input-57-feb9bd85061b&gt; in &lt;module&gt; -&gt; 1 df[0] # doesn&#39;t work /opt/miniconda3/lib/python3.7/site-packages/pandas/core/frame.py in __getitem__(self, key) 2900 if self.columns.nlevels &gt; 1: 2901 return self._getitem_multilevel(key) -&gt; 2902 indexer = self.columns.get_loc(key) 2903 if is_integer(indexer): 2904 indexer = [indexer] /opt/miniconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance) 2889 return self._engine.get_loc(casted_key) 2890 except KeyError as err: -&gt; 2891 raise KeyError(key) from err 2892 2893 if tolerance is not None: KeyError: 0 . df[0:1] # does work . Name Language Courses . 0 Tom | Python | 5 | . df[1:] # does work . Name Language Courses . 1 Mike | Python | 4 | . 2 Tiffany | R | 7 | . Indexing with .loc and .iloc . Pandas created the methods .loc[] and .iloc[] as more flexible alternatives for accessing data from a dataframe. Use df.iloc[] for indexing with integers and df.loc[] for indexing with labels. These are typically the recommended methods of indexing in Pandas. . df . Name Language Courses . 0 Tom | Python | 5 | . 1 Mike | Python | 4 | . 2 Tiffany | R | 7 | . First we&#39;ll try out .iloc which accepts integers as references to rows/columns: . df.iloc[0] # returns a series . Name Tom Language Python Courses 5 Name: 0, dtype: object . df.iloc[0:2] # slicing returns a dataframe . Name Language Courses . 0 Tom | Python | 5 | . 1 Mike | Python | 4 | . df.iloc[2, 1] # returns the indexed object . &#39;R&#39; . df.iloc[[0, 1], [1, 2]] # returns a dataframe . Language Courses . 0 Python | 5 | . 1 Python | 4 | . Now let&#39;s look at .loc which accepts labels as references to rows/columns: . df.loc[:, &#39;Name&#39;] . 0 Tom 1 Mike 2 Tiffany Name: Name, dtype: object . df.loc[:, &#39;Name&#39;:&#39;Language&#39;] . Name Language . 0 Tom | Python | . 1 Mike | Python | . 2 Tiffany | R | . df.loc[[0, 2], [&#39;Language&#39;]] . Language . 0 Python | . 2 R | . Sometimes we want to use a mix of integers and labels to reference data in a dataframe. The easiest way to do this is to use .loc[] with a label then use an integer in combinations with .index or .columns: . df.index . RangeIndex(start=0, stop=3, step=1) . df.columns . Index([&#39;Name&#39;, &#39;Language&#39;, &#39;Courses&#39;], dtype=&#39;object&#39;) . df.loc[df.index[0], &#39;Courses&#39;] # I want to reference the first row and the column named &quot;Courses&quot; . 5 . df.loc[2, df.columns[1]] # I want to reference row &quot;2&quot; and the second column . &#39;R&#39; . Boolean indexing . Just like with series, we can select data based on boolean masks: . df . Name Language Courses . 0 Tom | Python | 5 | . 1 Mike | Python | 4 | . 2 Tiffany | R | 7 | . df[df[&#39;Courses&#39;] &gt; 5] . Name Language Courses . 2 Tiffany | R | 7 | . df[df[&#39;Name&#39;] == &quot;Tom&quot;] . Name Language Courses . 0 Tom | Python | 5 | . Indexing with .query() . Boolean masks work fine, but I prefer to use the .query() method for selecting data. df.query() is a powerful tool for filtering data. It has an odd syntax, one of the strangest I&#39;ve seen in Python, it is more like SQL - df.query() accepts a string expression to evaluate and it &quot;knows&quot; the names of the columns in your dataframe. . df.query(&quot;Courses &gt; 4 &amp; Language == &#39;Python&#39;&quot;) . Name Language Courses . 0 Tom | Python | 5 | . Note the use of single quotes AND double quotes above, lucky we have both in Python! Compare this to the equivalent boolean indexing operation and you can see that .query() is much more readable, especially as the query gets bigger! . df[(df[&#39;Courses&#39;] &gt; 4) &amp; (df[&#39;Language&#39;] == &#39;Python&#39;)] . Name Language Courses . 0 Tom | Python | 5 | . Query also allows you to reference variable in the current workspace using the @ symbol: . course_threshold = 4 df.query(&quot;Courses &gt; @course_threshold&quot;) . Name Language Courses . 0 Tom | Python | 5 | . 2 Tiffany | R | 7 | . Indexing cheatsheet . Method Syntax Output . Select column | df[col_label] | Series | . Select row slice | df[row_1_int:row_2_int] | DataFrame | . Select row/column by label | df.loc[row_label(s), col_label(s)] | Object for single selection, Series for one row/column, otherwise DataFrame | . Select row/column by integer | df.iloc[row_int(s), col_int(s)] | Object for single selection, Series for one row/column, otherwise DataFrame | . Select by row integer &amp; column label | df.loc[df.index[row_int], col_label] | Object for single selection, Series for one row/column, otherwise DataFrame | . Select by row label &amp; column integer | df.loc[row_label, df.columns[col_int]] | Object for single selection, Series for one row/column, otherwise DataFrame | . Select by boolean | df[bool_vec] | Object for single selection, Series for one row/column, otherwise DataFrame | . Select by boolean expression | df.query(&quot;expression&quot;) | Object for single selection, Series for one row/column, otherwise DataFrame | . Reading/Writing Data From External Sources . .csv files . A lot of the time you will be loading .csv files for use in pandas. You can use the pd.read_csv() function for this. In the following chapters we&#39;ll use a real dataset of my cycling commutes to the University of British Columbia. There are so many arguments that can be used to help read in your .csv file in an efficient and appropriate manner, feel free to check them out now (by using shift + tab in Jupyter, or typing help(pd.read_csv)). . path = &#39;data/cycling_data.csv&#39; df = pd.read_csv(path, index_col=0, parse_dates=True) df . Name Type Time Distance Comments . Date . 2019-09-10 00:13:04 Afternoon Ride | Ride | 2084 | 12.62 | Rain | . 2019-09-10 13:52:18 Morning Ride | Ride | 2531 | 13.03 | rain | . 2019-09-11 00:23:50 Afternoon Ride | Ride | 1863 | 12.52 | Wet road but nice weather | . 2019-09-11 14:06:19 Morning Ride | Ride | 2192 | 12.84 | Stopped for photo of sunrise | . 2019-09-12 00:28:05 Afternoon Ride | Ride | 1891 | 12.48 | Tired by the end of the week | . 2019-09-16 13:57:48 Morning Ride | Ride | 2272 | 12.45 | Rested after the weekend! | . 2019-09-17 00:15:47 Afternoon Ride | Ride | 1973 | 12.45 | Legs feeling strong! | . 2019-09-17 13:43:34 Morning Ride | Ride | 2285 | 12.60 | Raining | . 2019-09-18 13:49:53 Morning Ride | Ride | 2903 | 14.57 | Raining today | . 2019-09-18 00:15:52 Afternoon Ride | Ride | 2101 | 12.48 | Pumped up tires | . 2019-09-19 00:30:01 Afternoon Ride | Ride | 48062 | 12.48 | Feeling good | . 2019-09-19 13:52:09 Morning Ride | Ride | 2090 | 12.59 | Getting colder which is nice | . 2019-09-20 01:02:05 Afternoon Ride | Ride | 2961 | 12.81 | Feeling good | . 2019-09-23 13:50:41 Morning Ride | Ride | 2462 | 12.68 | Rested after the weekend! | . 2019-09-24 00:35:42 Afternoon Ride | Ride | 2076 | 12.47 | Oiled chain, bike feels smooth | . 2019-09-24 13:41:24 Morning Ride | Ride | 2321 | 12.68 | Bike feeling much smoother | . 2019-09-25 00:07:21 Afternoon Ride | Ride | 1775 | 12.10 | Feeling really tired | . 2019-09-25 13:35:41 Morning Ride | Ride | 2124 | 12.65 | Stopped for photo of sunrise | . 2019-09-26 00:13:33 Afternoon Ride | Ride | 1860 | 12.52 | raining | . 2019-09-26 13:42:43 Morning Ride | Ride | 2350 | 12.91 | Detour around trucks at Jericho | . 2019-09-27 01:00:18 Afternoon Ride | Ride | 1712 | 12.47 | Tired by the end of the week | . 2019-09-30 13:53:52 Morning Ride | Ride | 2118 | 12.71 | Rested after the weekend! | . 2019-10-01 00:15:07 Afternoon Ride | Ride | 1732 | NaN | Legs feeling strong! | . 2019-10-01 13:45:55 Morning Ride | Ride | 2222 | 12.82 | Beautiful morning! Feeling fit | . 2019-10-02 00:13:09 Afternoon Ride | Ride | 1756 | NaN | A little tired today but good weather | . 2019-10-02 13:46:06 Morning Ride | Ride | 2134 | 13.06 | Bit tired today but good weather | . 2019-10-03 00:45:22 Afternoon Ride | Ride | 1724 | 12.52 | Feeling good | . 2019-10-03 13:47:36 Morning Ride | Ride | 2182 | 12.68 | Wet road | . 2019-10-04 01:08:08 Afternoon Ride | Ride | 1870 | 12.63 | Very tired, riding into the wind | . 2019-10-09 13:55:40 Morning Ride | Ride | 2149 | 12.70 | Really cold! But feeling good | . 2019-10-10 00:10:31 Afternoon Ride | Ride | 1841 | 12.59 | Feeling good after a holiday break! | . 2019-10-10 13:47:14 Morning Ride | Ride | 2463 | 12.79 | Stopped for photo of sunrise | . 2019-10-11 00:16:57 Afternoon Ride | Ride | 1843 | 11.79 | Bike feeling tight, needs an oil and pump | . You can print a dataframe to .csv using df.to_csv(). Be sure to check out all of the possible arguments to write your dataframe exactly how you want it. . url . Pandas also facilitates reading directly from a url - pd.read_csv() accepts urls as input: . url = &#39;https://raw.githubusercontent.com/TomasBeuzen/toy-datasets/master/wine_1.csv&#39; pd.read_csv(url) . Bottle Grape Origin Alcohol pH Colour Aroma . 0 1 | Chardonnay | Australia | 14.23 | 3.51 | White | Floral | . 1 2 | Pinot Grigio | Italy | 13.20 | 3.30 | White | Fruity | . 2 3 | Pinot Blanc | France | 13.16 | 3.16 | White | Citrus | . 3 4 | Shiraz | Chile | 14.91 | 3.39 | Red | Berry | . 4 5 | Malbec | Argentina | 13.83 | 3.28 | Red | Fruity | . Other . Pandas can read data from all sorts of other file types including HTML, JSON, Excel, Parquet, Feather, etc. There are generally dedicated functions for reading these file types, see the Pandas documentation here. . Common DataFrame Operations . DataFrames have built-in functions for performing most common operations, e.g., .min(), idxmin(), sort_values(), etc. They&#39;re all documented in the Pandas documentation here but I&#39;ll demonstrate a few below: . df = pd.read_csv(&#39;data/cycling_data.csv&#39;) df . Date Name Type Time Distance Comments . 0 10 Sep 2019, 00:13:04 | Afternoon Ride | Ride | 2084 | 12.62 | Rain | . 1 10 Sep 2019, 13:52:18 | Morning Ride | Ride | 2531 | 13.03 | rain | . 2 11 Sep 2019, 00:23:50 | Afternoon Ride | Ride | 1863 | 12.52 | Wet road but nice weather | . 3 11 Sep 2019, 14:06:19 | Morning Ride | Ride | 2192 | 12.84 | Stopped for photo of sunrise | . 4 12 Sep 2019, 00:28:05 | Afternoon Ride | Ride | 1891 | 12.48 | Tired by the end of the week | . 5 16 Sep 2019, 13:57:48 | Morning Ride | Ride | 2272 | 12.45 | Rested after the weekend! | . 6 17 Sep 2019, 00:15:47 | Afternoon Ride | Ride | 1973 | 12.45 | Legs feeling strong! | . 7 17 Sep 2019, 13:43:34 | Morning Ride | Ride | 2285 | 12.60 | Raining | . 8 18 Sep 2019, 13:49:53 | Morning Ride | Ride | 2903 | 14.57 | Raining today | . 9 18 Sep 2019, 00:15:52 | Afternoon Ride | Ride | 2101 | 12.48 | Pumped up tires | . 10 19 Sep 2019, 00:30:01 | Afternoon Ride | Ride | 48062 | 12.48 | Feeling good | . 11 19 Sep 2019, 13:52:09 | Morning Ride | Ride | 2090 | 12.59 | Getting colder which is nice | . 12 20 Sep 2019, 01:02:05 | Afternoon Ride | Ride | 2961 | 12.81 | Feeling good | . 13 23 Sep 2019, 13:50:41 | Morning Ride | Ride | 2462 | 12.68 | Rested after the weekend! | . 14 24 Sep 2019, 00:35:42 | Afternoon Ride | Ride | 2076 | 12.47 | Oiled chain, bike feels smooth | . 15 24 Sep 2019, 13:41:24 | Morning Ride | Ride | 2321 | 12.68 | Bike feeling much smoother | . 16 25 Sep 2019, 00:07:21 | Afternoon Ride | Ride | 1775 | 12.10 | Feeling really tired | . 17 25 Sep 2019, 13:35:41 | Morning Ride | Ride | 2124 | 12.65 | Stopped for photo of sunrise | . 18 26 Sep 2019, 00:13:33 | Afternoon Ride | Ride | 1860 | 12.52 | raining | . 19 26 Sep 2019, 13:42:43 | Morning Ride | Ride | 2350 | 12.91 | Detour around trucks at Jericho | . 20 27 Sep 2019, 01:00:18 | Afternoon Ride | Ride | 1712 | 12.47 | Tired by the end of the week | . 21 30 Sep 2019, 13:53:52 | Morning Ride | Ride | 2118 | 12.71 | Rested after the weekend! | . 22 1 Oct 2019, 00:15:07 | Afternoon Ride | Ride | 1732 | NaN | Legs feeling strong! | . 23 1 Oct 2019, 13:45:55 | Morning Ride | Ride | 2222 | 12.82 | Beautiful morning! Feeling fit | . 24 2 Oct 2019, 00:13:09 | Afternoon Ride | Ride | 1756 | NaN | A little tired today but good weather | . 25 2 Oct 2019, 13:46:06 | Morning Ride | Ride | 2134 | 13.06 | Bit tired today but good weather | . 26 3 Oct 2019, 00:45:22 | Afternoon Ride | Ride | 1724 | 12.52 | Feeling good | . 27 3 Oct 2019, 13:47:36 | Morning Ride | Ride | 2182 | 12.68 | Wet road | . 28 4 Oct 2019, 01:08:08 | Afternoon Ride | Ride | 1870 | 12.63 | Very tired, riding into the wind | . 29 9 Oct 2019, 13:55:40 | Morning Ride | Ride | 2149 | 12.70 | Really cold! But feeling good | . 30 10 Oct 2019, 00:10:31 | Afternoon Ride | Ride | 1841 | 12.59 | Feeling good after a holiday break! | . 31 10 Oct 2019, 13:47:14 | Morning Ride | Ride | 2463 | 12.79 | Stopped for photo of sunrise | . 32 11 Oct 2019, 00:16:57 | Afternoon Ride | Ride | 1843 | 11.79 | Bike feeling tight, needs an oil and pump | . df.min() . Date 1 Oct 2019, 00:15:07 Name Afternoon Ride Type Ride Time 1712 Distance 11.79 Comments A little tired today but good weather dtype: object . df[&#39;Time&#39;].min() . 1712 . df[&#39;Time&#39;].idxmin() . 20 . df.iloc[20] . Date 27 Sep 2019, 01:00:18 Name Afternoon Ride Type Ride Time 1712 Distance 12.47 Comments Tired by the end of the week Name: 20, dtype: object . df.sum() . Date 10 Sep 2019, 00:13:0410 Sep 2019, 13:52:1811 S... Name Afternoon RideMorning RideAfternoon RideMornin... Type RideRideRideRideRideRideRideRideRideRideRideRi... Time 115922 Distance 392.69 Comments RainrainWet road but nice weatherStopped for p... dtype: object . Some methods like .mean() will only operate on numeric columns: . df.mean() . Time 3512.787879 Distance 12.667419 dtype: float64 . Some methods require arguments to be specified, like .sort_values(): . df.sort_values(by=&#39;Time&#39;) . Date Name Type Time Distance Comments . 20 27 Sep 2019, 01:00:18 | Afternoon Ride | Ride | 1712 | 12.47 | Tired by the end of the week | . 26 3 Oct 2019, 00:45:22 | Afternoon Ride | Ride | 1724 | 12.52 | Feeling good | . 22 1 Oct 2019, 00:15:07 | Afternoon Ride | Ride | 1732 | NaN | Legs feeling strong! | . 24 2 Oct 2019, 00:13:09 | Afternoon Ride | Ride | 1756 | NaN | A little tired today but good weather | . 16 25 Sep 2019, 00:07:21 | Afternoon Ride | Ride | 1775 | 12.10 | Feeling really tired | . 30 10 Oct 2019, 00:10:31 | Afternoon Ride | Ride | 1841 | 12.59 | Feeling good after a holiday break! | . 32 11 Oct 2019, 00:16:57 | Afternoon Ride | Ride | 1843 | 11.79 | Bike feeling tight, needs an oil and pump | . 18 26 Sep 2019, 00:13:33 | Afternoon Ride | Ride | 1860 | 12.52 | raining | . 2 11 Sep 2019, 00:23:50 | Afternoon Ride | Ride | 1863 | 12.52 | Wet road but nice weather | . 28 4 Oct 2019, 01:08:08 | Afternoon Ride | Ride | 1870 | 12.63 | Very tired, riding into the wind | . 4 12 Sep 2019, 00:28:05 | Afternoon Ride | Ride | 1891 | 12.48 | Tired by the end of the week | . 6 17 Sep 2019, 00:15:47 | Afternoon Ride | Ride | 1973 | 12.45 | Legs feeling strong! | . 14 24 Sep 2019, 00:35:42 | Afternoon Ride | Ride | 2076 | 12.47 | Oiled chain, bike feels smooth | . 0 10 Sep 2019, 00:13:04 | Afternoon Ride | Ride | 2084 | 12.62 | Rain | . 11 19 Sep 2019, 13:52:09 | Morning Ride | Ride | 2090 | 12.59 | Getting colder which is nice | . 9 18 Sep 2019, 00:15:52 | Afternoon Ride | Ride | 2101 | 12.48 | Pumped up tires | . 21 30 Sep 2019, 13:53:52 | Morning Ride | Ride | 2118 | 12.71 | Rested after the weekend! | . 17 25 Sep 2019, 13:35:41 | Morning Ride | Ride | 2124 | 12.65 | Stopped for photo of sunrise | . 25 2 Oct 2019, 13:46:06 | Morning Ride | Ride | 2134 | 13.06 | Bit tired today but good weather | . 29 9 Oct 2019, 13:55:40 | Morning Ride | Ride | 2149 | 12.70 | Really cold! But feeling good | . 27 3 Oct 2019, 13:47:36 | Morning Ride | Ride | 2182 | 12.68 | Wet road | . 3 11 Sep 2019, 14:06:19 | Morning Ride | Ride | 2192 | 12.84 | Stopped for photo of sunrise | . 23 1 Oct 2019, 13:45:55 | Morning Ride | Ride | 2222 | 12.82 | Beautiful morning! Feeling fit | . 5 16 Sep 2019, 13:57:48 | Morning Ride | Ride | 2272 | 12.45 | Rested after the weekend! | . 7 17 Sep 2019, 13:43:34 | Morning Ride | Ride | 2285 | 12.60 | Raining | . 15 24 Sep 2019, 13:41:24 | Morning Ride | Ride | 2321 | 12.68 | Bike feeling much smoother | . 19 26 Sep 2019, 13:42:43 | Morning Ride | Ride | 2350 | 12.91 | Detour around trucks at Jericho | . 13 23 Sep 2019, 13:50:41 | Morning Ride | Ride | 2462 | 12.68 | Rested after the weekend! | . 31 10 Oct 2019, 13:47:14 | Morning Ride | Ride | 2463 | 12.79 | Stopped for photo of sunrise | . 1 10 Sep 2019, 13:52:18 | Morning Ride | Ride | 2531 | 13.03 | rain | . 8 18 Sep 2019, 13:49:53 | Morning Ride | Ride | 2903 | 14.57 | Raining today | . 12 20 Sep 2019, 01:02:05 | Afternoon Ride | Ride | 2961 | 12.81 | Feeling good | . 10 19 Sep 2019, 00:30:01 | Afternoon Ride | Ride | 48062 | 12.48 | Feeling good | . df.sort_values(by=&#39;Time&#39;, ascending=False) . Date Name Type Time Distance Comments . 10 19 Sep 2019, 00:30:01 | Afternoon Ride | Ride | 48062 | 12.48 | Feeling good | . 12 20 Sep 2019, 01:02:05 | Afternoon Ride | Ride | 2961 | 12.81 | Feeling good | . 8 18 Sep 2019, 13:49:53 | Morning Ride | Ride | 2903 | 14.57 | Raining today | . 1 10 Sep 2019, 13:52:18 | Morning Ride | Ride | 2531 | 13.03 | rain | . 31 10 Oct 2019, 13:47:14 | Morning Ride | Ride | 2463 | 12.79 | Stopped for photo of sunrise | . 13 23 Sep 2019, 13:50:41 | Morning Ride | Ride | 2462 | 12.68 | Rested after the weekend! | . 19 26 Sep 2019, 13:42:43 | Morning Ride | Ride | 2350 | 12.91 | Detour around trucks at Jericho | . 15 24 Sep 2019, 13:41:24 | Morning Ride | Ride | 2321 | 12.68 | Bike feeling much smoother | . 7 17 Sep 2019, 13:43:34 | Morning Ride | Ride | 2285 | 12.60 | Raining | . 5 16 Sep 2019, 13:57:48 | Morning Ride | Ride | 2272 | 12.45 | Rested after the weekend! | . 23 1 Oct 2019, 13:45:55 | Morning Ride | Ride | 2222 | 12.82 | Beautiful morning! Feeling fit | . 3 11 Sep 2019, 14:06:19 | Morning Ride | Ride | 2192 | 12.84 | Stopped for photo of sunrise | . 27 3 Oct 2019, 13:47:36 | Morning Ride | Ride | 2182 | 12.68 | Wet road | . 29 9 Oct 2019, 13:55:40 | Morning Ride | Ride | 2149 | 12.70 | Really cold! But feeling good | . 25 2 Oct 2019, 13:46:06 | Morning Ride | Ride | 2134 | 13.06 | Bit tired today but good weather | . 17 25 Sep 2019, 13:35:41 | Morning Ride | Ride | 2124 | 12.65 | Stopped for photo of sunrise | . 21 30 Sep 2019, 13:53:52 | Morning Ride | Ride | 2118 | 12.71 | Rested after the weekend! | . 9 18 Sep 2019, 00:15:52 | Afternoon Ride | Ride | 2101 | 12.48 | Pumped up tires | . 11 19 Sep 2019, 13:52:09 | Morning Ride | Ride | 2090 | 12.59 | Getting colder which is nice | . 0 10 Sep 2019, 00:13:04 | Afternoon Ride | Ride | 2084 | 12.62 | Rain | . 14 24 Sep 2019, 00:35:42 | Afternoon Ride | Ride | 2076 | 12.47 | Oiled chain, bike feels smooth | . 6 17 Sep 2019, 00:15:47 | Afternoon Ride | Ride | 1973 | 12.45 | Legs feeling strong! | . 4 12 Sep 2019, 00:28:05 | Afternoon Ride | Ride | 1891 | 12.48 | Tired by the end of the week | . 28 4 Oct 2019, 01:08:08 | Afternoon Ride | Ride | 1870 | 12.63 | Very tired, riding into the wind | . 2 11 Sep 2019, 00:23:50 | Afternoon Ride | Ride | 1863 | 12.52 | Wet road but nice weather | . 18 26 Sep 2019, 00:13:33 | Afternoon Ride | Ride | 1860 | 12.52 | raining | . 32 11 Oct 2019, 00:16:57 | Afternoon Ride | Ride | 1843 | 11.79 | Bike feeling tight, needs an oil and pump | . 30 10 Oct 2019, 00:10:31 | Afternoon Ride | Ride | 1841 | 12.59 | Feeling good after a holiday break! | . 16 25 Sep 2019, 00:07:21 | Afternoon Ride | Ride | 1775 | 12.10 | Feeling really tired | . 24 2 Oct 2019, 00:13:09 | Afternoon Ride | Ride | 1756 | NaN | A little tired today but good weather | . 22 1 Oct 2019, 00:15:07 | Afternoon Ride | Ride | 1732 | NaN | Legs feeling strong! | . 26 3 Oct 2019, 00:45:22 | Afternoon Ride | Ride | 1724 | 12.52 | Feeling good | . 20 27 Sep 2019, 01:00:18 | Afternoon Ride | Ride | 1712 | 12.47 | Tired by the end of the week | . Some methods will operate on the index/columns, like .sort_index(): . df.sort_index(ascending=False) . Date Name Type Time Distance Comments . 32 11 Oct 2019, 00:16:57 | Afternoon Ride | Ride | 1843 | 11.79 | Bike feeling tight, needs an oil and pump | . 31 10 Oct 2019, 13:47:14 | Morning Ride | Ride | 2463 | 12.79 | Stopped for photo of sunrise | . 30 10 Oct 2019, 00:10:31 | Afternoon Ride | Ride | 1841 | 12.59 | Feeling good after a holiday break! | . 29 9 Oct 2019, 13:55:40 | Morning Ride | Ride | 2149 | 12.70 | Really cold! But feeling good | . 28 4 Oct 2019, 01:08:08 | Afternoon Ride | Ride | 1870 | 12.63 | Very tired, riding into the wind | . 27 3 Oct 2019, 13:47:36 | Morning Ride | Ride | 2182 | 12.68 | Wet road | . 26 3 Oct 2019, 00:45:22 | Afternoon Ride | Ride | 1724 | 12.52 | Feeling good | . 25 2 Oct 2019, 13:46:06 | Morning Ride | Ride | 2134 | 13.06 | Bit tired today but good weather | . 24 2 Oct 2019, 00:13:09 | Afternoon Ride | Ride | 1756 | NaN | A little tired today but good weather | . 23 1 Oct 2019, 13:45:55 | Morning Ride | Ride | 2222 | 12.82 | Beautiful morning! Feeling fit | . 22 1 Oct 2019, 00:15:07 | Afternoon Ride | Ride | 1732 | NaN | Legs feeling strong! | . 21 30 Sep 2019, 13:53:52 | Morning Ride | Ride | 2118 | 12.71 | Rested after the weekend! | . 20 27 Sep 2019, 01:00:18 | Afternoon Ride | Ride | 1712 | 12.47 | Tired by the end of the week | . 19 26 Sep 2019, 13:42:43 | Morning Ride | Ride | 2350 | 12.91 | Detour around trucks at Jericho | . 18 26 Sep 2019, 00:13:33 | Afternoon Ride | Ride | 1860 | 12.52 | raining | . 17 25 Sep 2019, 13:35:41 | Morning Ride | Ride | 2124 | 12.65 | Stopped for photo of sunrise | . 16 25 Sep 2019, 00:07:21 | Afternoon Ride | Ride | 1775 | 12.10 | Feeling really tired | . 15 24 Sep 2019, 13:41:24 | Morning Ride | Ride | 2321 | 12.68 | Bike feeling much smoother | . 14 24 Sep 2019, 00:35:42 | Afternoon Ride | Ride | 2076 | 12.47 | Oiled chain, bike feels smooth | . 13 23 Sep 2019, 13:50:41 | Morning Ride | Ride | 2462 | 12.68 | Rested after the weekend! | . 12 20 Sep 2019, 01:02:05 | Afternoon Ride | Ride | 2961 | 12.81 | Feeling good | . 11 19 Sep 2019, 13:52:09 | Morning Ride | Ride | 2090 | 12.59 | Getting colder which is nice | . 10 19 Sep 2019, 00:30:01 | Afternoon Ride | Ride | 48062 | 12.48 | Feeling good | . 9 18 Sep 2019, 00:15:52 | Afternoon Ride | Ride | 2101 | 12.48 | Pumped up tires | . 8 18 Sep 2019, 13:49:53 | Morning Ride | Ride | 2903 | 14.57 | Raining today | . 7 17 Sep 2019, 13:43:34 | Morning Ride | Ride | 2285 | 12.60 | Raining | . 6 17 Sep 2019, 00:15:47 | Afternoon Ride | Ride | 1973 | 12.45 | Legs feeling strong! | . 5 16 Sep 2019, 13:57:48 | Morning Ride | Ride | 2272 | 12.45 | Rested after the weekend! | . 4 12 Sep 2019, 00:28:05 | Afternoon Ride | Ride | 1891 | 12.48 | Tired by the end of the week | . 3 11 Sep 2019, 14:06:19 | Morning Ride | Ride | 2192 | 12.84 | Stopped for photo of sunrise | . 2 11 Sep 2019, 00:23:50 | Afternoon Ride | Ride | 1863 | 12.52 | Wet road but nice weather | . 1 10 Sep 2019, 13:52:18 | Morning Ride | Ride | 2531 | 13.03 | rain | . 0 10 Sep 2019, 00:13:04 | Afternoon Ride | Ride | 2084 | 12.62 | Rain | . 4. Why ndarrays and Series and DataFrames? . At this point, you might be asking why we need all these different data structures. Well, they all serve different purposes and are suited to different tasks. For example: . NumPy is typically faster/uses less memory than Pandas; | not all Python packages are compatible with NumPy &amp; Pandas; | the ability to add labels to data can be useful (e.g., for time series); | NumPy and Pandas have different built-in functions available. | . My advice: use the simplest data structure that fulfills your needs! . Finally, we&#39;ve seen how to go from: ndarray (np.array()) -&gt; series (pd.series()) -&gt; dataframe (pd.DataFrame()). Remember that we can also go the other way: dataframe/series -&gt; ndarray using df.to_numpy(). .",
            "url": "https://millermuttu.github.io/blog/jupyter/python/data%20science/programming/pandas/2022/05/11/chapter7-pandas.html",
            "relUrl": "/jupyter/python/data%20science/programming/pandas/2022/05/11/chapter7-pandas.html",
            "date": " ‚Ä¢ May 11, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Data science revision - chapter 5",
            "content": ". Chapter 5: Introduction to NumPy . Chapter Outline . . 1. Introduction to NumPy | 2. NumPy Arrays | 3. Array Operations and Broadcasting | 4. Indexing and slicing | 5. More Useful NumPy Functions | . Chapter Learning Objectives . Use NumPy to create arrays with built-in functions inlcuding np.array(), np.arange(), np.linspace() and np.full(), np.zeros(), np.ones() | Be able to access values from a NumPy array by numeric indexing and slicing and boolean indexing | Perform mathematical operations on and with arrays. | Explain what broadcasting is and how to use it. | Reshape arrays by adding/removing/reshaping axes with .reshape(), np.newaxis(), .ravel(), .flatten() | Understand how to use built-in NumPy functions like np.sum(), np.mean(), np.log() as stand alone functions or as methods of numpy arrays (when available) | . 1. Introduction to NumPy . . NumPy stands for &quot;Numerical Python&quot; and it is the standard Python library used for working with arrays (i.e., vectors &amp; matrices), linear algerba, and other numerical computations. NumPy is written in C, making NumPy arrays faster and more memory efficient than Python lists or arrays, read more: (link 1, link 2, link 3). . NumPy can be installed using conda (if not already): . conda install numpy . 2. NumPy Arrays . What are Arrays? . Arrays are &quot;n-dimensional&quot; data structures that can contain all the basic Python data types, e.g., floats, integers, strings etc, but work best with numeric data. NumPy arrays (&quot;ndarrays&quot;) are homogenous, which means that items in the array should be of the same type. ndarrays are also compatible with numpy&#39;s vast collection of in-built functions! . . Source: Medium.com . Usually we import numpy with the alias np (to avoid having to type out n-u-m-p-y every time we want to use it): . import numpy as np . A numpy array is sort of like a list: . my_list = [1, 2, 3, 4, 5] my_list . [1, 2, 3, 4, 5] . my_array = np.array([1, 2, 3, 4, 5]) my_array . array([1, 2, 3, 4, 5]) . But it has the type ndarray: . type(my_array) . numpy.ndarray . Unlike a list, arrays can only hold a single type (usually numbers): . my_list = [1, &quot;hi&quot;] my_list . [1, &#39;hi&#39;] . my_array = np.array((1, &quot;hi&quot;)) my_array . array([&#39;1&#39;, &#39;hi&#39;], dtype=&#39;&lt;U21&#39;) . Above: NumPy converted the integer 1 into the string &#39;1&#39;! . Creating arrays . ndarrays are typically created using two main methods: . From existing data (usually lists or tuples) using np.array(), like we saw above; or, | Using built-in functions such as np.arange(), np.linspace(), np.zeros(), etc. | my_list = [1, 2, 3] np.array(my_list) . array([1, 2, 3]) . Just like you can have &quot;multi-dimensional lists&quot; (by nesting lists in lists), you can have multi-dimensional arrays (indicated by double square brackets [[ ]]): . list_2d = [[1, 2], [3, 4], [5, 6]] list_2d . [[1, 2], [3, 4], [5, 6]] . array_2d = np.array(list_2d) array_2d . array([[1, 2], [3, 4], [5, 6]]) . You&#39;ll probably use the built-in numpy array creators quite often. Here are some common ones (hint - don&#39;t forget to check the docstrings for help with these functions, if you&#39;re in Jupyter, remeber the shift + tab shortcut): . np.arange(1, 5) # from 1 inclusive to 5 exclusive . array([1, 2, 3, 4]) . np.arange(0, 11, 2) # step by 2 from 1 to 11 . array([ 0, 2, 4, 6, 8, 10]) . np.linspace(0, 10, 5) # 5 equally spaced points between 0 and 10 . array([ 0. , 2.5, 5. , 7.5, 10. ]) . np.ones((2, 2)) # an array of ones with size 2 x 2 . array([[1., 1.], [1., 1.]]) . np.zeros((2, 3)) # an array of zeros with size 2 x 3 . array([[0., 0., 0.], [0., 0., 0.]]) . np.full((3, 3), 3.14) # an array of the number 3.14 with size 3 x 3 . array([[3.14, 3.14, 3.14], [3.14, 3.14, 3.14], [3.14, 3.14, 3.14]]) . np.full((3, 3, 3), 3.14) # an array of the number 3.14 with size 3 x 3 x 3 . array([[[3.14, 3.14, 3.14], [3.14, 3.14, 3.14], [3.14, 3.14, 3.14]], [[3.14, 3.14, 3.14], [3.14, 3.14, 3.14], [3.14, 3.14, 3.14]], [[3.14, 3.14, 3.14], [3.14, 3.14, 3.14], [3.14, 3.14, 3.14]]]) . np.random.rand(5, 2) # random numbers uniformly distributed from 0 to 1 with size 5 x 2 . array([[0.99572773, 0.81999212], [0.97339833, 0.10561435], [0.85923126, 0.40018929], [0.97506147, 0.78313652], [0.99229906, 0.14525197]]) . There are many useful attributes/methods that can be called off numpy arrays: . print(dir(np.ndarray)) . [&#39;T&#39;, &#39;__abs__&#39;, &#39;__add__&#39;, &#39;__and__&#39;, &#39;__array__&#39;, &#39;__array_finalize__&#39;, &#39;__array_function__&#39;, &#39;__array_interface__&#39;, &#39;__array_prepare__&#39;, &#39;__array_priority__&#39;, &#39;__array_struct__&#39;, &#39;__array_ufunc__&#39;, &#39;__array_wrap__&#39;, &#39;__bool__&#39;, &#39;__class__&#39;, &#39;__complex__&#39;, &#39;__contains__&#39;, &#39;__copy__&#39;, &#39;__deepcopy__&#39;, &#39;__delattr__&#39;, &#39;__delitem__&#39;, &#39;__dir__&#39;, &#39;__divmod__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__float__&#39;, &#39;__floordiv__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__getitem__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__iadd__&#39;, &#39;__iand__&#39;, &#39;__ifloordiv__&#39;, &#39;__ilshift__&#39;, &#39;__imatmul__&#39;, &#39;__imod__&#39;, &#39;__imul__&#39;, &#39;__index__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__int__&#39;, &#39;__invert__&#39;, &#39;__ior__&#39;, &#39;__ipow__&#39;, &#39;__irshift__&#39;, &#39;__isub__&#39;, &#39;__iter__&#39;, &#39;__itruediv__&#39;, &#39;__ixor__&#39;, &#39;__le__&#39;, &#39;__len__&#39;, &#39;__lshift__&#39;, &#39;__lt__&#39;, &#39;__matmul__&#39;, &#39;__mod__&#39;, &#39;__mul__&#39;, &#39;__ne__&#39;, &#39;__neg__&#39;, &#39;__new__&#39;, &#39;__or__&#39;, &#39;__pos__&#39;, &#39;__pow__&#39;, &#39;__radd__&#39;, &#39;__rand__&#39;, &#39;__rdivmod__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__rfloordiv__&#39;, &#39;__rlshift__&#39;, &#39;__rmatmul__&#39;, &#39;__rmod__&#39;, &#39;__rmul__&#39;, &#39;__ror__&#39;, &#39;__rpow__&#39;, &#39;__rrshift__&#39;, &#39;__rshift__&#39;, &#39;__rsub__&#39;, &#39;__rtruediv__&#39;, &#39;__rxor__&#39;, &#39;__setattr__&#39;, &#39;__setitem__&#39;, &#39;__setstate__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;, &#39;__sub__&#39;, &#39;__subclasshook__&#39;, &#39;__truediv__&#39;, &#39;__xor__&#39;, &#39;all&#39;, &#39;any&#39;, &#39;argmax&#39;, &#39;argmin&#39;, &#39;argpartition&#39;, &#39;argsort&#39;, &#39;astype&#39;, &#39;base&#39;, &#39;byteswap&#39;, &#39;choose&#39;, &#39;clip&#39;, &#39;compress&#39;, &#39;conj&#39;, &#39;conjugate&#39;, &#39;copy&#39;, &#39;ctypes&#39;, &#39;cumprod&#39;, &#39;cumsum&#39;, &#39;data&#39;, &#39;diagonal&#39;, &#39;dot&#39;, &#39;dtype&#39;, &#39;dump&#39;, &#39;dumps&#39;, &#39;fill&#39;, &#39;flags&#39;, &#39;flat&#39;, &#39;flatten&#39;, &#39;getfield&#39;, &#39;imag&#39;, &#39;item&#39;, &#39;itemset&#39;, &#39;itemsize&#39;, &#39;max&#39;, &#39;mean&#39;, &#39;min&#39;, &#39;nbytes&#39;, &#39;ndim&#39;, &#39;newbyteorder&#39;, &#39;nonzero&#39;, &#39;partition&#39;, &#39;prod&#39;, &#39;ptp&#39;, &#39;put&#39;, &#39;ravel&#39;, &#39;real&#39;, &#39;repeat&#39;, &#39;reshape&#39;, &#39;resize&#39;, &#39;round&#39;, &#39;searchsorted&#39;, &#39;setfield&#39;, &#39;setflags&#39;, &#39;shape&#39;, &#39;size&#39;, &#39;sort&#39;, &#39;squeeze&#39;, &#39;std&#39;, &#39;strides&#39;, &#39;sum&#39;, &#39;swapaxes&#39;, &#39;take&#39;, &#39;tobytes&#39;, &#39;tofile&#39;, &#39;tolist&#39;, &#39;tostring&#39;, &#39;trace&#39;, &#39;transpose&#39;, &#39;var&#39;, &#39;view&#39;] . x = np.random.rand(5, 2) x . array([[0.0252237 , 0.3368265 ], [0.57678161, 0.68709061], [0.00721254, 0.61280414], [0.65886063, 0.87372073], [0.82815125, 0.47996047]]) . x.transpose() . array([[0.0252237 , 0.57678161, 0.00721254, 0.65886063, 0.82815125], [0.3368265 , 0.68709061, 0.61280414, 0.87372073, 0.47996047]]) . x.mean() . 0.5086632185578763 . x.astype(int) . array([[0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]) . Array Shapes . As you just saw above, arrays can be of any dimension, shape and size you desire. In fact, there are three main array attributes you need to know to work out the characteristics of an array: . .ndim: the number of dimensions of an array | .shape: the number of elements in each dimension (like calling len() on each dimension) | .size: the total number of elements in an array (i.e., the product of .shape) | . array_1d = np.ones(3) print(f&quot;Dimensions: {array_1d.ndim}&quot;) print(f&quot; Shape: {array_1d.shape}&quot;) print(f&quot; Size: {array_1d.size}&quot;) . Dimensions: 1 Shape: (3,) Size: 3 . Let&#39;s turn that print action into a function and try out some other arrays: . def print_array(x): print(f&quot;Dimensions: {x.ndim}&quot;) print(f&quot; Shape: {x.shape}&quot;) print(f&quot; Size: {x.size}&quot;) print(&quot;&quot;) print(x) . array_2d = np.ones((3, 2)) print_array(array_2d) . Dimensions: 2 Shape: (3, 2) Size: 6 [[1. 1.] [1. 1.] [1. 1.]] . array_4d = np.ones((1, 2, 3, 4)) print_array(array_4d) . Dimensions: 4 Shape: (1, 2, 3, 4) Size: 24 [[[[1. 1. 1. 1.] [1. 1. 1. 1.] [1. 1. 1. 1.]] [[1. 1. 1. 1.] [1. 1. 1. 1.] [1. 1. 1. 1.]]]] . After 3 dimensions, printing arrays starts getting pretty messy. As you can see above, the number of square brackets ([ ]) in the printed output indicate how many dimensions there are: for example, above, the output starts with 4 square brackets [[[[ indicative of a 4D array. . 1-d Arrays . One of the most confusing things about numpy is 1-d arrays (vectors) can have 3 possible shapes! . x = np.ones(5) print_array(x) . Dimensions: 1 Shape: (5,) Size: 5 [1. 1. 1. 1. 1.] . y = np.ones((1, 5)) print_array(y) . Dimensions: 2 Shape: (1, 5) Size: 5 [[1. 1. 1. 1. 1.]] . z = np.ones((5, 1)) print_array(z) . Dimensions: 2 Shape: (5, 1) Size: 5 [[1.] [1.] [1.] [1.] [1.]] . We can use np.array_equal() to determine if two arrays have the same shape and elements: . np.array_equal(x, x) . True . np.array_equal(x, y) . False . np.array_equal(x, z) . False . np.array_equal(y, z) . False . The shape of your 1-d arrays can actually have big implications on your mathematical oeprations! . print(f&quot;x: {x}&quot;) print(f&quot;y: {y}&quot;) print(f&quot;z: {z}&quot;) . x: [1. 1. 1. 1. 1.] y: [[1. 1. 1. 1. 1.]] z: [[1.] [1.] [1.] [1.] [1.]] . x + y # makes sense . array([[2., 2., 2., 2., 2.]]) . y + z # wait, what? . array([[2., 2., 2., 2., 2.], [2., 2., 2., 2., 2.], [2., 2., 2., 2., 2.], [2., 2., 2., 2., 2.], [2., 2., 2., 2., 2.]]) . What happened in the cell above is &quot;broadcasting&quot; and we&#39;ll discuss it below. . 3. Array Operations and Broadcasting . Elementwise operations . Elementwise operations refer to operations applied to each element of an array or between the paired elements of two arrays. . x = np.ones(4) x . array([1., 1., 1., 1.]) . y = x + 1 y . array([2., 2., 2., 2.]) . x - y . array([-1., -1., -1., -1.]) . x == y . array([False, False, False, False]) . x * y . array([2., 2., 2., 2.]) . x ** y . array([1., 1., 1., 1.]) . x / y . array([0.5, 0.5, 0.5, 0.5]) . np.array_equal(x, y) . False . Broadcasting . ndarrays with different sizes cannot be directly used in arithmetic operations: . a = np.ones((2, 2)) b = np.ones((3, 3)) a + b . ValueError Traceback (most recent call last) &lt;ipython-input-54-7b429ebe865f&gt; in &lt;module&gt; 1 a = np.ones((2, 2)) 2 b = np.ones((3, 3)) -&gt; 3 a + b ValueError: operands could not be broadcast together with shapes (2,2) (3,3) . Broadcasting describes how NumPy treats arrays with different shapes during arithmetic operations. The idea is to wrangle data so that operations can occur element-wise. . Let&#39;s see an example. Say I sell pies on my weekends. I sell 3 types of pies at different prices, and I sold the following number of each pie last weekend. I want to know how much money I made per pie type per day. . . cost = np.array([20, 15, 25]) print(&quot;Pie cost:&quot;) print(cost) sales = np.array([[2, 3, 1], [6, 3, 3], [5, 3, 5]]) print(&quot; nPie sales (#):&quot;) print(sales) . Pie cost: [20 15 25] Pie sales (#): [[2 3 1] [6 3 3] [5 3 5]] . How can we multiply these two arrays together? We could use a loop: . . total = np.zeros((3, 3)) # initialize an array of 0&#39;s for col in range(sales.shape[1]): total[:, col] = sales[:, col] * cost total . array([[ 40., 60., 20.], [ 90., 45., 45.], [125., 75., 125.]]) . Or we could make them the same size, and multiply corresponding elements &quot;elementwise&quot;: . . cost = np.repeat(cost, 3).reshape((3, 3)) cost . array([[20, 20, 20], [15, 15, 15], [25, 25, 25]]) . cost * sales . array([[ 40, 60, 20], [ 90, 45, 45], [125, 75, 125]]) . Congratulations! You just broadcasted! Broadcasting is just Numpy eessentially doing the np.repeat() for you under the hood: . cost = np.array([20, 15, 25]).reshape(3, 1) print(f&quot; cost shape: {cost.shape}&quot;) sales = np.array([[2, 3, 1], [6, 3, 3], [5, 3, 5]]) print(f&quot;sales shape: {sales.shape}&quot;) . cost shape: (3, 1) sales shape: (3, 3) . sales * cost . array([[ 40, 60, 20], [ 90, 45, 45], [125, 75, 125]]) . In NumPy the smaller array is ‚Äúbroadcast‚Äù across the larger array so that they have compatible shapes: . . Source: Python Data Science Handbook by Jake VanderPlas (2016) . Why should you care about broadcasting? Well, it&#39;s cleaner and faster than looping and it also affects the array shapes resulting from arithmetic operations. Below, we can time how long it takes to loop vs broadcast: . cost = np.array([20, 15, 25]).reshape(3, 1) sales = np.array([[2, 3, 1], [6, 3, 3], [5, 3, 5]]) total = np.zeros((3, 3)) time_loop = %timeit -q -o -r 3 for col in range(sales.shape[1]): total[:, col] = sales[:, col] * np.squeeze(cost) time_vec = %timeit -q -o -r 3 cost * sales print(f&quot;Broadcasting is {time_loop.average / time_vec.average:.2f}x faster than looping here.&quot;) . Broadcasting is 8.67x faster than looping here. . Of course, not all arrays are compatible! NumPy compares arrays element-wise. It starts with the trailing dimensions, and works its way forward. Dimensions are compatible if: . they are equal, or | one of them is 1. | . Use the code below to test out array compatibitlity: . a = np.ones((3, 2)) b = np.ones((3, 2, 1)) print(f&quot;The shape of a is: {a.shape}&quot;) print(f&quot;The shape of b is: {b.shape}&quot;) print(&quot;&quot;) try: print(f&quot;The shape of a + b is: {(a + b).shape}&quot;) except: print(f&quot;ERROR: arrays are NOT broadcast compatible!&quot;) . The shape of a is: (3, 2) The shape of b is: (3, 2, 1) ERROR: arrays are NOT broadcast compatible! . Reshaping Arrays . There are 3 key reshaping methods I want you to know about for reshaping numpy arrays: . .rehshape() | np.newaxis | .ravel()/.flatten() | . x = np.full((4, 3), 3.14) x . array([[3.14, 3.14, 3.14], [3.14, 3.14, 3.14], [3.14, 3.14, 3.14], [3.14, 3.14, 3.14]]) . You&#39;ll reshape arrays farily often and the .reshape() method is pretty intuitive: . x.reshape(6, 2) . array([[3.14, 3.14], [3.14, 3.14], [3.14, 3.14], [3.14, 3.14], [3.14, 3.14], [3.14, 3.14]]) . x.reshape(2, -1) # using -1 will calculate the dimension for you (if possible) . array([[3.14, 3.14, 3.14, 3.14, 3.14, 3.14], [3.14, 3.14, 3.14, 3.14, 3.14, 3.14]]) . a = np.ones(3) print_array(a) b = np.ones((3, 2)) print_array(b) . Dimensions: 1 Shape: (3,) Size: 3 [1. 1. 1.] Dimensions: 2 Shape: (3, 2) Size: 6 [[1. 1.] [1. 1.] [1. 1.]] . If I want to add these two arrays I won&#39;t be able to because their dimensions are not compatible: . a + b . ValueError Traceback (most recent call last) &lt;ipython-input-69-bd58363a63fc&gt; in &lt;module&gt; -&gt; 1 a + b ValueError: operands could not be broadcast together with shapes (3,) (3,2) . Sometimes you&#39;ll want to add dimensions to an array for broadcasting purposes like this. We can do that with np.newaxis (note that None is an alias for np.newaxis). We can add a dimension to a to make the arrays compatible: . print_array(a[:, np.newaxis]) # same as a[:, None] . Dimensions: 2 Shape: (3, 1) Size: 3 [[1.] [1.] [1.]] . a[:, np.newaxis] + b . array([[2., 2.], [2., 2.], [2., 2.]]) . Finally, sometimes you&#39;ll want to &quot;flatten&quot; arrays to a single dimension using .ravel() or .flatten(). .flatten() used to return a copy and .ravel() a view/reference but now they both return a copy so I can&#39;t think of an important reason to use one over the other ü§∑‚Äç‚ôÇÔ∏è . x . array([[3.14, 3.14, 3.14], [3.14, 3.14, 3.14], [3.14, 3.14, 3.14], [3.14, 3.14, 3.14]]) . print_array(x.flatten()) . Dimensions: 1 Shape: (12,) Size: 12 [3.14 3.14 3.14 3.14 3.14 3.14 3.14 3.14 3.14 3.14 3.14 3.14] . print_array(x.ravel()) . Dimensions: 1 Shape: (12,) Size: 12 [3.14 3.14 3.14 3.14 3.14 3.14 3.14 3.14 3.14 3.14 3.14 3.14] . 4. Indexing and slicing . Concepts of indexing should be pretty familiar by now. Indexing arrays is similar to indexing lists but there are just more dimensions. . Numeric Indexing . x = np.arange(10) x . array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) . x[3] . 3 . x[2:] . array([2, 3, 4, 5, 6, 7, 8, 9]) . x[:4] . array([0, 1, 2, 3]) . x[2:5] . array([2, 3, 4]) . x[2:3] . array([2]) . x[-1] . 9 . x[-2] . 8 . x[5:0:-1] . array([5, 4, 3, 2, 1]) . For 2D arrays: . x = np.random.randint(10, size=(4, 6)) x . array([[9, 5, 0, 2, 3, 4], [0, 8, 2, 1, 0, 7], [0, 9, 4, 2, 9, 0], [4, 4, 0, 8, 3, 9]]) . x[3, 4] # do this . 3 . x[3][4] # i do not like this as much . 3 . x[3] . array([4, 4, 0, 8, 3, 9]) . len(x) # generally, just confusing . 4 . x.shape . (4, 6) . x[:, 2] # column number 2 . array([0, 2, 4, 0]) . x[2:, :3] . array([[0, 9, 4], [4, 4, 0]]) . x.T . array([[9, 0, 0, 4], [5, 8, 9, 4], [0, 2, 4, 0], [2, 1, 2, 8], [3, 0, 9, 3], [4, 7, 0, 9]]) . x . array([[9, 5, 0, 2, 3, 4], [0, 8, 2, 1, 0, 7], [0, 9, 4, 2, 9, 0], [4, 4, 0, 8, 3, 9]]) . x[1, 1] = 555555 x . array([[ 9, 5, 0, 2, 3, 4], [ 0, 555555, 2, 1, 0, 7], [ 0, 9, 4, 2, 9, 0], [ 4, 4, 0, 8, 3, 9]]) . z = np.zeros(5) z . array([0., 0., 0., 0., 0.]) . z[0] = 5 z . array([5., 0., 0., 0., 0.]) . Boolean Indexing . x = np.random.rand(10) x . array([0.59134864, 0.18116891, 0.07089194, 0.71975976, 0.95339547, 0.71630937, 0.80956702, 0.94886201, 0.58996084, 0.03238515]) . x + 1 . array([1.59134864, 1.18116891, 1.07089194, 1.71975976, 1.95339547, 1.71630937, 1.80956702, 1.94886201, 1.58996084, 1.03238515]) . x_thresh = x &gt; 0.5 x_thresh . array([ True, False, False, True, True, True, True, True, True, False]) . x[x_thresh] = 0.5 # set all elements &gt; 0.5 to be equal to 0.5 x . array([0.5 , 0.18116891, 0.07089194, 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 , 0.03238515]) . x = np.random.rand(10) x . array([0.62912775, 0.82278712, 0.26534708, 0.8847088 , 0.48882526, 0.04902107, 0.55230008, 0.6579308 , 0.24630664, 0.77826484]) . x[x &gt; 0.5] = 0.5 x . array([0.5 , 0.5 , 0.26534708, 0.5 , 0.48882526, 0.04902107, 0.5 , 0.5 , 0.24630664, 0.5 ]) . 5. More Useful NumPy Functions . Numpy has many built-in functions for mathematical operations, really it has almost every numerical operation you might want to do in its library. I&#39;m not going to explore the whole library here, but as an example of some of the available functions, consider working out the hypotenuse of a triangle that with sides 3m and 4m: . . sides = np.array([3, 4]) . There are several ways we could solve this problem. We could directly use Pythagoras&#39;s Theorem: . $$c = sqrt{a^2+b^2}$$ . np.sqrt(np.sum([np.power(sides[0], 2), np.power(sides[1], 2)])) . 5.0 . We can leverage the fact that we&#39;re dealing with a numpy array and apply a &quot;vectorized&quot; operation (more on that in a bit) to the whole vector at one time: . (sides ** 2).sum() ** 0.5 . 5.0 . Or we can simply use a numpy built-in function (if it exists): . np.linalg.norm(sides) # you&#39;ll learn more about norms in 573 . 5.0 . np.hypot(*sides) . 5.0 . Vectorization . Broadly speaking, &quot;vectorization&quot; in NumPy refers to the use of optmized C code to perform an operation. Long-story-short, because numpy arrays are homogenous (contain the same dtype), we don&#39;t need to check that we can perform an operation on elements of a sequence before we do the operation which results in a huge speed-up. You can kind of think of this concept as NumPy being able to perform an operation on the whole array at the same time rather than one-by-one (this is not actually the case, a super-efficient C loop is still running under the hood, but that&#39;s an irrelevant detail). You can read more about vectorization here but all you need to know is that most operations in NumPy are vectorized, so just try to do things at an &quot;array-level&quot; rather than an &quot;element-level&quot;, e.g.: . array = np.array(range(5)) for i, element in enumerate(array): array[i] = element ** 2 array . array([ 0, 1, 4, 9, 16]) . array = np.array(range(5)) array **= 2 . Let&#39;s do a quick timing experiment: . array = np.array(range(5)) time_loop = %timeit -q -o -r 3 for i, element in enumerate(array): array[i] = element ** 2 # vectorized method array = np.array(range(5)) time_vec = %timeit -q -o -r 3 array ** 2 print(f&quot;Vectorized operation is {time_loop.average / time_vec.average:.2f}x faster than looping here.&quot;) . Vectorized operation is 4.57x faster than looping here. .",
            "url": "https://millermuttu.github.io/blog/jupyter/python/data%20science/programming/numpy/2022/04/24/chapter5-numpy.html",
            "relUrl": "/jupyter/python/data%20science/programming/numpy/2022/04/24/chapter5-numpy.html",
            "date": " ‚Ä¢ Apr 24, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "Data science revision - chapter 4",
            "content": ". Chapter 4: Style Guides, Scripts, Imports . Chapter Outline . . 1. Style Guide | 2. Python Scripts | 3. Importing | 4. Intriguing Behaviour in Python | . Chapter Learning Objectives . Describe why code style is important. | Differentiate between the role of a linter like flake8 and an autoformatter like black. | Implement linting and formatting from the command line or within Jupyter or another IDE. | Write a Python module (.py file) in VSCode or other IDE of your choice. | Import installed or custom packages using the import syntax. | Explain the notion of a reference in Python. | Explain the notion of scoping in Python. | Anticipate whether changing one variable will change another in Python. | Anticipate whether a function changes the caller&#39;s version of an argument variable in Python. | Select the appropriate choice between == and is in Python. | . 1. Style Guide . . Source: xkcd.com. . It is incorrect to think that if your code works then you are done. Code has two &quot;users&quot; - the computer (which turns it into machine instructions) and humans, who will likely read and/or modify the code in the future. This section is about how to make your code suitable to that second audience, humans. . Styling is particularly important for sharing your code to other users (including your future self!). Remember: &quot;Code is read much more often than it is written&quot;. PEP8 is the Python Style Guide. It is worth skimming through PEP 8, but here are some highlights: . Indent using 4 spaces | Have whitespace around operators, e.g. x = 1 not x=1 | But avoid extra whitespace, e.g. f(1) not f (1) | Single and double quotes both fine for strings, but use &quot;&quot;&quot;triple double quotes&quot;&quot;&quot; for docstrings, not &#39;&#39;&#39;triple single quotes&#39;&#39;&#39; | Variable and function names use underscores_between_words | And much more... | . There&#39;s lots to remember but luckily linters &amp; formatters can help you adhere to uniform styling! . Linters . Linting refers to highlighting programmatic and stylistic problems in your Python source code. Think of it like &quot;spell check&quot; in word processing software. Common linters include pycodestyle, pylint, pyflakes, flake8, etc. We&#39;ll use flake8 in this chapter, which, if you don&#39;t have it, you can install with: . conda install -c anaconda flake8 . flake8 can be used from the command line to check files: . flake8 path/file_to_check.py . You can execute shell commands in Jupyter by prepending a command with an exclamation mark !. I&#39;ve included an example script called bad_style.py in the data sub-directory of this directory, let&#39;s use flake8 on that now: . !flake8 data/bad_style.py . data/bad_style.py:1:6: E201 whitespace after &#39;{&#39; data/bad_style.py:1:11: E231 missing whitespace after &#39;:&#39; data/bad_style.py:1:14: E231 missing whitespace after &#39;,&#39; data/bad_style.py:1:18: E231 missing whitespace after &#39;:&#39; data/bad_style.py:2:1: E128 continuation line under-indented for visual indent data/bad_style.py:2:4: E231 missing whitespace after &#39;:&#39; data/bad_style.py:4:25: E128 continuation line under-indented for visual indent data/bad_style.py:5:5: E225 missing whitespace around operator data/bad_style.py:7:80: E501 line too long (119 &gt; 79 characters) data/bad_style.py:8:2: E111 indentation is not a multiple of four data/bad_style.py:10:2: E111 indentation is not a multiple of four data/bad_style.py:11:2: E111 indentation is not a multiple of four data/bad_style.py:12:2: E111 indentation is not a multiple of four data/bad_style.py:13:10: E701 multiple statements on one line (colon) data/bad_style.py:13:31: E261 at least two spaces before inline comment data/bad_style.py:13:31: E262 inline comment should start with &#39;# &#39; data/bad_style.py:14:1: E302 expected 2 blank lines, found 0 data/bad_style.py:14:13: E201 whitespace after &#39;(&#39; data/bad_style.py:14:25: E202 whitespace before &#39;)&#39; data/bad_style.py:15:3: E111 indentation is not a multiple of four data/bad_style.py:15:8: E211 whitespace before &#39;(&#39; data/bad_style.py:15:19: E202 whitespace before &#39;)&#39; data/bad_style.py:16:11: E271 multiple spaces after keyword data/bad_style.py:17:3: E301 expected 1 blank line, found 0 data/bad_style.py:17:3: E111 indentation is not a multiple of four data/bad_style.py:17:16: E231 missing whitespace after &#39;,&#39; data/bad_style.py:18:7: E111 indentation is not a multiple of four data/bad_style.py:20:1: E305 expected 2 blank lines after class or function definition, found 0 data/bad_style.py:28:2: W292 no newline at end of file . Formatters . Formatting refers to restructuring how code appears by applying specific rules for line spacing, indents, line length, etc. Common formatters include autopep8, black, yapf, etc. We&#39;ll use black in this chapter, which, if you don&#39;t have it, you can install with: . conda install -c conda-forge black . black can also be used from the command line to format your files: . black path/file_to_check.py --check . The --check argument just checks if your code conforms to black style but doesn&#39;t reformat it in place, if you want your file reformatted, remove the argument. . !black data/bad_style.py --check . would reformat data/bad_style.py Oh no! üí• üíî üí• 1 file would be reformatted. . Here&#39;s the file content before formatting: . x = { &#39;a&#39;:37,&#39;b&#39;:42, &#39;c&#39;:927} very_long_variable_name = {&#39;field&#39;: 1, &#39;is_debug&#39;: True} this=True if very_long_variable_name is not None and very_long_variable_name[&quot;field&quot;] &gt; 0 or very_long_variable_name[&#39;is_debug&#39;]: z = &#39;hello &#39;+&#39;world&#39; else: world = &#39;world&#39; a = &#39;hello {}&#39;.format(world) f = rf&#39;hello {world}&#39; if (this): y = &#39;hello &#39;&#39;world&#39;#FIXME: https://github.com/python/black/issues/26 class Foo ( object ): def f (self ): return 37*-2 def g(self, x,y=42): return y # fmt: off custom_formatting = [ 0, 1, 2, 3, 4, 5 ] # fmt: on regular_formatting = [ 0, 1, 2, 3, 4, 5 ] . ANd here it is after formatting (note how you can toggle formatting on or off in your code using the # fmt: off/# fmt: on tags): . x = {&quot;a&quot;: 37, &quot;b&quot;: 42, &quot;c&quot;: 927} very_long_variable_name = {&quot;field&quot;: 1, &quot;is_debug&quot;: True} this = True if ( very_long_variable_name is not None and very_long_variable_name[&quot;field&quot;] &gt; 0 or very_long_variable_name[&quot;is_debug&quot;] ): z = &quot;hello &quot; + &quot;world&quot; else: world = &quot;world&quot; a = &quot;hello {}&quot;.format(world) f = rf&quot;hello {world}&quot; if this: y = &quot;hello &quot; &quot;world&quot; # FIXME: https://github.com/python/black/issues/26 class Foo(object): def f(self): return 37 * -2 def g(self, x, y=42): return y # fmt: off custom_formatting = [ 0, 1, 2, 3, 4, 5 ] # fmt: on regular_formatting = [0, 1, 2, 3, 4, 5] . Comments . Comments are important for understanding your code. While docstrings cover what a function does, your comments will help document how your code achieves its goal. There are PEP 8 guidelines on the length, spacing, etc of comments. . Comments: should start with a # followed by a single space and be preceded by at least two spaces. | Block Comments: each line of a block comment should start with a # followed by a single space and should be indented to the same level as the code it precedes. | Generally, comments should not be unnecessarily verbose or just state the obvious, as this can be distracting and can actually make your code more difficult to read! | . Here is an example of a reasonable comment: . def random_walker(T): x = 0 y = 0 for i in range(T): # Generate a random number between 0 and 1. # Then, go right, left, up or down if the number # is in the interval [0,0.25), [0.25,0.5), # [0.5,0.75) or [0.75,1) respectively. r = random() if r &lt; 0.25: x += 1 # Go right elif r &lt; 0.5: x -= 1 # Go left elif r &lt; 0.75: y += 1 # Go up else: y -= 1 # Go down print((x,y)) return x**2 + y**2 . Here are some bad examples of comments, because they are unnecessary or poorly formatted: . def random_walker(T): # intalize coords x = 0 y = 0 for i in range(T):# loop T times r = random() if r &lt; 0.25: x += 1 # go right elif r &lt; 0.5: x -= 1 # go left elif r &lt; 0.75: y += 1 # go up else: y -= 1 # Print the location print((x, y)) # In Python, the ** operator means exponentiation. return x ** 2 + y ** 2 . 2. Python Scripts . Jupyter is a fantastic data science tool which allows you to code and create visualisations alongside text and images to create narratives. However, as your project grows, eventually you&#39;re going to need to create python scripts, .py files .py files are also called &quot;modules&quot; in Python and may contain functions, classes, variables, and/or runnable code. I typically start my projects in Jupyter, and then begin to offload my functions, classes, scripts, etc to .py files as I formalise, structure and streamline my code. . IDEs . You don&#39;t need any special software to write Python modules, you can write your code using any text editor and just save your file with a .py extension. But software exists to make your life much easier! . IDE stands for &quot;integrated development environment&quot; and refers to software that provides comprehensive functionality for code development (e.g., compiling, debugging, formatting, testing, linting, etc). In my experience the most popular out-of-the-box Python IDEs are PyCharm and Spyder. There are also many editors available that can be customized with extensions to act as Python IDEs, e.g., VSCode, Atom, Sublime Text. The benefit of these customisable editors is that they are light-weight and you can choose only the extensions you really need (as opposed to downloading a big, full-blown IDE like PyCharm). . VSCode is my favourite editor at the moment and they have a great Python tutorial online which I&#39;d highly recommend if you&#39;re interested! We&#39;ll do some importing of custom .py files in these chapters but won&#39;t do any work in an IDE. . 3. Importing . Python can access code in another module by importing it. This is done using the import statement, which you&#39;ve probably seen a few times already. We&#39;ll discuss importing more in DSCI 524 and you can read all about it in the Python documentation but for now, it&#39;s easiest to see it in action. . Ways of Importing Things . I&#39;ve written a .py file called wallet.py that contains a class Wallet that can be used to store, spend, and earn cash. I recommend taking a look at that file on GitHub before moving on. . Let&#39;s import the code from wallet.py . We can import our .py file (our module) simply by: . import wallet . We can take a look at all the useable parts of that module by typing dir(wallet): . dir(wallet) . [&#39;InsufficientCashError&#39;, &#39;Wallet&#39;, &#39;__builtins__&#39;, &#39;__cached__&#39;, &#39;__doc__&#39;, &#39;__file__&#39;, &#39;__loader__&#39;, &#39;__name__&#39;, &#39;__package__&#39;, &#39;__spec__&#39;] . We can import a package using an alias with the as keyword: . import wallet as w . w.Wallet(100) . &lt;wallet.Wallet at 0x15b781ad0&gt; . w.InsufficientCashError() . wallet.InsufficientCashError() . And we can import just a specific function/class/variable from our module: . from wallet import Wallet . Wallet(100) # now I can refer to it without the module name prefix . &lt;wallet.Wallet at 0x15b79da50&gt; . You can even mix up all these methods: . from wallet import Wallet as w . w(100) . &lt;wallet.Wallet at 0x15b7acdd0&gt; . It&#39;s also possible to import everything in a module, though this is generally not recommended: . from wallet import * . Wallet(100) . &lt;wallet.Wallet at 0x15b7b2f90&gt; . InsufficientCashError() . wallet.InsufficientCashError() . Importing Functions from Outside your Working Directory . I could do import wallet above because wallet.py is in my current working directory. But there are a few extra steps needed if it is in a different location. I&#39;ve included a script called hello.py in a data/ sub-directory of the directory housing this notebook. All it has in it is: . PLANET = &quot;Earth&quot; def hello_world(): print(f&quot;Hello {PLANET}!&quot;) . Unfortunately I can&#39;t do this: . from hello import hello_world . ModuleNotFoundError Traceback (most recent call last) &lt;ipython-input-25-20bbd0c111a6&gt; in &lt;module&gt; -&gt; 1 from hello import hello_world ModuleNotFoundError: No module named &#39;hello&#39; . What I need to do is add this directory location to the paths that Python searches through when looking to import something. I usually do this using the sys module: . import sys sys.path.append(&#39;data/&#39;) sys.path # display the current paths Python is looking through . [&#39;/Users/tbeuzen/GitHub/online-courses/python-programming-for-data-science/chapters&#39;, &#39;/opt/miniconda3/lib/python37.zip&#39;, &#39;/opt/miniconda3/lib/python3.7&#39;, &#39;/opt/miniconda3/lib/python3.7/lib-dynload&#39;, &#39;&#39;, &#39;/Users/tbeuzen/.local/lib/python3.7/site-packages&#39;, &#39;/opt/miniconda3/lib/python3.7/site-packages&#39;, &#39;/opt/miniconda3/lib/python3.7/site-packages/IPython/extensions&#39;, &#39;/Users/tbeuzen/.ipython&#39;, &#39;data/&#39;] . See that data/ is now a valid path. So now I can import from hello.py: . from hello import hello_world, PLANET . PLANET # note that I can import variable defined in a .py file! . &#39;Earth&#39; . hello_world() . Hello Earth! . Packages . As your code gets more complex, grows in modules, and you wish to share it, you&#39;ll want to turn it into a Python package. Packages are logical collections of modules that can be easily imported. If you&#39;re interested in creating your own packages, take a look at the py-pkgs book. For now, we&#39;ll be using other people&#39;s popular data science packages, specifically, next chapter we&#39;ll look at numpy: &quot;the fundamental package for scientific computing with Python&quot;. . Importing Installed Packages . In the next few chapters we&#39;ll be using the numpy and pandas packages, which are probably the most popular for data science. When you install those packages, they are put in a location on your computer that Python already knows about, so we can simply import them at will. . import numpy as np . np.array([1, 2, 3]) . array([1, 2, 3]) . np.random.randint(0, 10, 3) . array([2, 6, 7]) . There are plenty of packages that come with the Python Standard Library - these do not require installation with conda and you&#39;ll come across them throughout your data science journey, I&#39;ll show one example, random, below. But for more advanced stuff you&#39;ll to install and use packages like numpy, pandas and others. If you need some specific functionality, make sure you check if there&#39;s a package for it (there often is!). For example, one functionality I often want is a progress bar when looping over a for loop. This is available in the tqdm package: . from tqdm import tqdm for i in tqdm(range(int(10e5))): i ** 2 . 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000000/1000000 [00:00&lt;00:00, 2063659.83it/s] . 4. Intriguing Behaviour in Python . References . What do you think the code below will print? . x = 1 y = x x = 2 y . 1 . And how about the next one? . x = [1] y = x x[0] = 2 y . [2] . In Python, the list x is a reference to an object in the computer&#39;s memory. When you set y = x these two variables now refer to the same object in memory - the one that x referred to. Setting x[0] = 2 modifies the object in memory. So x and y are both modified (it makes no different if you set x[0] = 2 or y[0] = 2, both modify the same memory). . Here&#39;s an analogy that might help understand what&#39;s going on: . I share a Dropbox folder (or git repo) with you, and you modify it -- I sent you the location of the stuff (this is like the list case) | I send you an email with a file attached, you download it and modify the file -- I sent you the stuff itself (this is like the integer case) | . Okay, what do you think will happen here: . x = [1] y = x x = [2] # before we had x[0] = 2 y . [1] . Here we are not modifying the contents of x, we are setting x to refer to a new list [2]. . Additional Weirdness . We can use id() to return the unique id of an object in memory. . x = np.array([1, 2, 3, 4, 5]) # this is a numpy array which we&#39;ll learn more about next chapter y = x x = x + 5 print(f&quot;x has the value: {x}, id: {id(x)}&quot;) print(f&quot;y has the value: {y}, id: {id(y)}&quot;) . x has the value: [ 6 7 8 9 10], id: 5830667440 y has the value: [1 2 3 4 5], id: 5830666800 . x = np.array([1, 2, 3, 4, 5]) y = x x += 5 print(f&quot;x has the value: {x}, id: {id(x)}&quot;) print(f&quot;y has the value: {y}, id: {id(y)}&quot;) . x has the value: [ 6 7 8 9 10], id: 5830669856 y has the value: [ 6 7 8 9 10], id: 5830669856 . So, it turns out x += 5 is not identical x = x + 5. The former modifies the contents of x. The latter first evaluates x + 5 to a new array of the same size, and then overwrites the name x with a reference to this new array. . But there&#39;s good news - we don&#39;t need to memorize special rules for calling functions. Copying happens with int, float, bool, (maybe some other ones I&#39;m forgetting?), the rest is &quot;by reference&quot;. Now you see why we care if objects are mutable or immutable... passing around a reference can be dangerous! General rule - if you do x = ... then you&#39;re not modifying the original, but if you do x.SOMETHING = y or x[SOMETHING] = y or x *= y then you probably are. . copy and deepcopy . We can force the certain copying behaviour using the copy library if we want to: . import copy # part of the standard library . x = [1] y = x x[0] = 2 y . [2] . x = [1] y = copy.copy(x) # We &quot;copied&quot; x and saved that new object as y x[0] = 2 y . [1] . Ok, so what do you think will happen here? . x = [[1], [2, 99], [3, &quot;hi&quot;]] # a list of lists y = copy.copy(x) print(&quot;After copy.copy():&quot;) print(x) print(y) x[0][0] = &quot;pikachu&quot; print(&quot;&quot;) print(&quot;After modifying x:&quot;) print(x) print(y) . After copy.copy(): [[1], [2, 99], [3, &#39;hi&#39;]] [[1], [2, 99], [3, &#39;hi&#39;]] After modifying x: [[&#39;pikachu&#39;], [2, 99], [3, &#39;hi&#39;]] [[&#39;pikachu&#39;], [2, 99], [3, &#39;hi&#39;]] . But wait.. we used copy, why are x and y both changed in the latter example? copy makes the containers different, i.e., only the outer list. But the outer lists contain references to objects which were not copied! This is what happens after y = copy.copy(x): . . We can use is to tell apart these scenarios (as opposed to ==). is tells us if two objects are referring to the same object in memory, while == tells us if their contents are the same: . x == y # they are both lists containing the same lists . True . x is y # but they are not the *same* lists of lists . False . So, by that logic we should be able to append to y without affecting x: . y.append(5) print(x) print(y) . [[&#39;pikachu&#39;], [2, 99], [3, &#39;hi&#39;]] [[&#39;pikachu&#39;], [2, 99], [3, &#39;hi&#39;], 5] . x == y . False . That makes sense, as weird as it seems: . . In short, copy only copies one level down. What if we want to copy everything? i.e., even the inner lists in our outer list... Enter our friend deepcopy: . x = [[1], [2, 99], [3, &quot;hi&quot;]] y = copy.deepcopy(x) x[0][0] = &quot;pikachu&quot; print(x) print(y) . [[&#39;pikachu&#39;], [2, 99], [3, &#39;hi&#39;]] [[1], [2, 99], [3, &#39;hi&#39;]] . . {tip} If you&#39;re interested, you can find a whole compilation of more intriguing behaviour in Python [here](https://github.com/satwikkansal/wtfpython/blob/master/README.md)! .",
            "url": "https://millermuttu.github.io/blog/jupyter/python/data%20science/programming/2022/04/15/chapter4-style-scripts-imports.html",
            "relUrl": "/jupyter/python/data%20science/programming/2022/04/15/chapter4-style-scripts-imports.html",
            "date": " ‚Ä¢ Apr 15, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "Data science revision - chapter 2",
            "content": ". Chapter 2: Loops &amp; Functions . Chapter Outline . . 1. for Loops | 2. while loops | 3. Comprehensions | 4. try / except | 5. Functions | 6. Functions as a data type | 7. Anonymous functions | 8. DRY principle, designing good functions | 9. Generators | 10. Docstrings | . Chapter Learning Objectives . Write for and while loops in Python. | Identify iterable datatypes which can be used in for loops. | Create a list, dictionary, or set using comprehension. | Write a try/except statement. | Define a function and an anonymous function in Python. | Describe the difference between positional and keyword arguments. | Describe the difference between local and global arguments. | Apply the DRY principle to write modular code. | Assess whether a function has side effects. | Write a docstring for a function that describes parameters, return values, behaviour and usage. | . 1. for Loops . For loops allow us to execute code a specific number of times. . for n in [2, 7, -1, 5]: print(f&quot;The number is {n} and its square is {n**2}&quot;) print(&quot;I&#39;m outside the loop!&quot;) . The number is 2 and its square is 4 The number is 7 and its square is 49 The number is -1 and its square is 1 The number is 5 and its square is 25 I&#39;m outside the loop! . The main points to notice: . Keyword for begins the loop. Colon : ends the first line of the loop. | Block of code indented is executed for each value in the list (hence the name &quot;for&quot; loops) | The loop ends after the variable n has taken all the values in the list | We can iterate over any kind of &quot;iterable&quot;: list, tuple, range, set, string. | An iterable is really just any object with a sequence of values that can be looped over. In this case, we are iterating over the values in a list. | . word = &quot;Python&quot; for letter in word: print(&quot;Gimme a &quot; + letter + &quot;!&quot;) print(f&quot;What&#39;s that spell?!! {word}!&quot;) . Gimme a P! Gimme a y! Gimme a t! Gimme a h! Gimme a o! Gimme a n! What&#39;s that spell?!! Python! . A very common pattern is to use for with the range(). range() gives you a sequence of integers up to some value (non-inclusive of the end-value) and is typically used for looping. . range(10) . range(0, 10) . list(range(10)) . [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] . for i in range(10): print(i) . 0 1 2 3 4 5 6 7 8 9 . We can also specify a start value and a skip-by value with range: . for i in range(1, 101, 10): print(i) . 1 11 21 31 41 51 61 71 81 91 . We can write a loop inside another loop to iterate over multiple dimensions of data: . for x in [1, 2, 3]: for y in [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;]: print((x, y)) . (1, &#39;a&#39;) (1, &#39;b&#39;) (1, &#39;c&#39;) (2, &#39;a&#39;) (2, &#39;b&#39;) (2, &#39;c&#39;) (3, &#39;a&#39;) (3, &#39;b&#39;) (3, &#39;c&#39;) . list_1 = [0, 1, 2] list_2 = [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;] for i in range(3): print(list_1[i], list_2[i]) . 0 a 1 b 2 c . There are many clever ways of doing these kinds of things in Python. When looping over objects, I tend to use zip() and enumerate() quite a lot in my work. zip() returns a zip object which is an iterable of tuples. . for i in zip(list_1, list_2): print(i) . (0, &#39;a&#39;) (1, &#39;b&#39;) (2, &#39;c&#39;) . We can even &quot;unpack&quot; these tuples directly in the for loop: . for i, j in zip(list_1, list_2): print(i, j) . 0 a 1 b 2 c . enumerate() adds a counter to an iterable which we can use within the loop. . for i in enumerate(list_2): print(i) . (0, &#39;a&#39;) (1, &#39;b&#39;) (2, &#39;c&#39;) . for n, i in enumerate(list_2): print(f&quot;index {n}, value {i}&quot;) . index 0, value a index 1, value b index 2, value c . We can loop through key-value pairs of a dictionary using .items(). The general syntax is for key, value in dictionary.items(). . courses = {521 : &quot;awesome&quot;, 551 : &quot;riveting&quot;, 511 : &quot;naptime!&quot;} for course_num, description in courses.items(): print(f&quot;DSCI {course_num}, is {description}&quot;) . DSCI 521, is awesome DSCI 551, is riveting DSCI 511, is naptime! . We can even use enumerate() to do more complex un-packing: . for n, (course_num, description) in enumerate(courses.items()): print(f&quot;Item {n}: DSCI {course_num}, is {description}&quot;) . Item 0: DSCI 521, is awesome Item 1: DSCI 551, is riveting Item 2: DSCI 511, is naptime! . 2. while loops . We can also use a while loop to excute a block of code several times. But beware! If the conditional expression is always True, then you&#39;ve got an infintite loop! . n = 10 while n &gt; 0: print(n) n -= 1 print(&quot;Blast off!&quot;) . 10 9 8 7 6 5 4 3 2 1 Blast off! . Let&#39;s read the while statement above as if it were in English. It means, ‚ÄúWhile n is greater than 0, display the value of n and then decrement n by 1. When you get to 0, display the word Blast off!‚Äù . For some loops, it&#39;s hard to tell when, or if, they will stop! Take a look at the Collatz conjecture. The conjecture states that no matter what positive integer n we start with, the sequence will always eventually reach 1 - we just don&#39;t know how many iterations it will take. . n = 11 while n != 1: print(int(n)) if n % 2 == 0: # n is even n = n / 2 else: # n is odd n = n * 3 + 1 print(int(n)) . 11 34 17 52 26 13 40 20 10 5 16 8 4 2 1 . Hence, in some cases, you may want to force a while loop to stop based on some criteria, using the break keyword. . n = 123 i = 0 while n != 1: print(int(n)) if n % 2 == 0: # n is even n = n / 2 else: # n is odd n = n * 3 + 1 i += 1 if i == 10: print(f&quot;Ugh, too many iterations!&quot;) break . 123 370 185 556 278 139 418 209 628 314 Ugh, too many iterations! . The continue keyword is similar to break but won&#39;t stop the loop. Instead, it just restarts the loop from the top. . n = 10 while n &gt; 0: if n % 2 != 0: # n is odd n = n - 1 continue break # this line is never executed because continue restarts the loop from the top print(n) n = n - 1 print(&quot;Blast off!&quot;) . 10 8 6 4 2 Blast off! . 3. Comprehensions . Comprehensions allow us to build lists/tuples/sets/dictionaries in one convenient, compact line of code. I use these quite a bit! Below is a standard for loop you might use to iterate over an iterable and create a list: . subliminal = [&#39;Tom&#39;, &#39;ingests&#39;, &#39;many&#39;, &#39;eggs&#39;, &#39;to&#39;, &#39;outrun&#39;, &#39;large&#39;, &#39;eagles&#39;, &#39;after&#39;, &#39;running&#39;, &#39;near&#39;, &#39;!&#39;] first_letters = [] for word in subliminal: first_letters.append(word[0]) print(first_letters) . [&#39;T&#39;, &#39;i&#39;, &#39;m&#39;, &#39;e&#39;, &#39;t&#39;, &#39;o&#39;, &#39;l&#39;, &#39;e&#39;, &#39;a&#39;, &#39;r&#39;, &#39;n&#39;, &#39;!&#39;] . List comprehension allows us to do this in one compact line: . letters = [word[0] for word in subliminal] # list comprehension letters . [&#39;T&#39;, &#39;i&#39;, &#39;m&#39;, &#39;e&#39;, &#39;t&#39;, &#39;o&#39;, &#39;l&#39;, &#39;e&#39;, &#39;a&#39;, &#39;r&#39;, &#39;n&#39;, &#39;!&#39;] . We can make things more complicated by doing multiple iteration or conditional iteration: . [(i, j) for i in range(3) for j in range(4)] . [(0, 0), (0, 1), (0, 2), (0, 3), (1, 0), (1, 1), (1, 2), (1, 3), (2, 0), (2, 1), (2, 2), (2, 3)] . [i for i in range(11) if i % 2 == 0] # condition the iterator, select only even numbers . [0, 2, 4, 6, 8, 10] . [-i if i % 2 else i for i in range(11)] # condition the value, -ve odd and +ve even numbers . [0, -1, 2, -3, 4, -5, 6, -7, 8, -9, 10] . There is also set comprehension: . words = [&#39;hello&#39;, &#39;goodbye&#39;, &#39;the&#39;, &#39;antidisestablishmentarianism&#39;] y = {word[-1] for word in words} # set comprehension y # only has 3 elements because a set contains only unique items and there would have been two e&#39;s . {&#39;e&#39;, &#39;m&#39;, &#39;o&#39;} . Dictionary comprehension: . word_lengths = {word:len(word) for word in words} # dictionary comprehension word_lengths . {&#39;hello&#39;: 5, &#39;goodbye&#39;: 7, &#39;the&#39;: 3, &#39;antidisestablishmentarianism&#39;: 28} . Tuple comprehension doesn&#39;t work as you might expect... We get a &quot;generator&quot; instead (more on that later). . y = (word[-1] for word in words) # this is NOT a tuple comprehension - more on generators later print(y) . &lt;generator object &lt;genexpr&gt; at 0x15e469c50&gt; . 4. try / except . . Above: the Blue Screen of Death at a Nine Inch Nails concert! Source: cnet.com. . If something goes wrong, we don&#39;t want our code to crash - we want it to fail gracefully. In Python, this can be accomplished using try/except. Here is a basic example: . this_variable_does_not_exist print(&quot;Another line&quot;) # code fails before getting to this line . NameError Traceback (most recent call last) &lt;ipython-input-30-dd878f68d557&gt; in &lt;module&gt; -&gt; 1 this_variable_does_not_exist 2 print(&#34;Another line&#34;) # code fails before getting to this line NameError: name &#39;this_variable_does_not_exist&#39; is not defined . try: this_variable_does_not_exist except: pass # do nothing print(&quot;You did something bad! But I won&#39;t raise an error.&quot;) # print something print(&quot;Another line&quot;) . You did something bad! But I won&#39;t raise an error. Another line . Python tries to execute the code in the try block. If an error is encountered, we &quot;catch&quot; this in the except block (also called try/catch in other languages). There are many different error types, or exceptions - we saw NameError above. . 5/0 # ZeroDivisionError . ZeroDivisionError Traceback (most recent call last) &lt;ipython-input-32-9866726f0353&gt; in &lt;module&gt; -&gt; 1 5/0 # ZeroDivisionError ZeroDivisionError: division by zero . my_list = [1,2,3] my_list[5] # IndexError . IndexError Traceback (most recent call last) &lt;ipython-input-33-8f0c4b3b2ce1&gt; in &lt;module&gt; 1 my_list = [1,2,3] -&gt; 2 my_list[5] # IndexError IndexError: list index out of range . my_tuple = (1,2,3) my_tuple[0] = 0 # TypeError . TypeError Traceback (most recent call last) &lt;ipython-input-34-90cd0bd9ddec&gt; in &lt;module&gt; 1 my_tuple = (1,2,3) -&gt; 2 my_tuple[0] = 0 # TypeError TypeError: &#39;tuple&#39; object does not support item assignment . Ok, so there are apparently a bunch of different errors one could run into. With try/except you can also catch the exception itself: . try: this_variable_does_not_exist except Exception as ex: print(&quot;You did something bad!&quot;) print(ex) print(type(ex)) . You did something bad! name &#39;this_variable_does_not_exist&#39; is not defined &lt;class &#39;NameError&#39;&gt; . In the above, we caught the exception and assigned it to the variable ex so that we could print it out. This is useful because you can see what the error message would have been, without crashing your program. You can also catch specific exceptions types. This is typically the recommended way to catch errors, you want to be specific in catching your error so you know exactly where and why your code failed. . try: this_variable_does_not_exist # name error # (1, 2, 3)[0] = 1 # type error # 5/0 # ZeroDivisionError except TypeError: print(&quot;You made a type error!&quot;) except NameError: print(&quot;You made a name error!&quot;) except: print(&quot;You made some other sort of error&quot;) . You made a name error! . The final except would trigger if the error is none of the above types, so this sort of has an if/elif/else feel to it. There is also an optional else and finally keyword (which I almost never used), but you can read more about here. . try: this_variable_does_not_exist except: print(&quot;The variable does not exist!&quot;) finally: print(&quot;I&#39;m printing anyway!&quot;) . The variable does not exist! I&#39;m printing anyway! . We can also write code that raises an exception on purpose, using raise: . def add_one(x): # we&#39;ll get to functions in the next section return x + 1 . add_one(&quot;blah&quot;) . TypeError Traceback (most recent call last) &lt;ipython-input-39-96e0142692a3&gt; in &lt;module&gt; -&gt; 1 add_one(&#34;blah&#34;) &lt;ipython-input-38-eabf290fc405&gt; in add_one(x) 1 def add_one(x): # we&#39;ll get to functions in the next section -&gt; 2 return x + 1 TypeError: can only concatenate str (not &#34;int&#34;) to str . def add_one(x): if not isinstance(x, float) and not isinstance(x, int): raise TypeError(f&quot;Sorry, x must be numeric, you entered a {type(x)}.&quot;) return x + 1 . add_one(&quot;blah&quot;) . TypeError Traceback (most recent call last) &lt;ipython-input-41-96e0142692a3&gt; in &lt;module&gt; -&gt; 1 add_one(&#34;blah&#34;) &lt;ipython-input-40-3a3a8b564774&gt; in add_one(x) 1 def add_one(x): 2 if not isinstance(x, float) and not isinstance(x, int): -&gt; 3 raise TypeError(f&#34;Sorry, x must be numeric, you entered a {type(x)}.&#34;) 4 5 return x + 1 TypeError: Sorry, x must be numeric, you entered a &lt;class &#39;str&#39;&gt;. . This is useful when your function is complicated and would fail in a complicated way, with a weird error message. You can make the cause of the error much clearer to the user of the function. If you do this, you should ideally describe these exceptions in the function documentation, so a user knows what to expect if they call your function. . Finally, we can even define our own exception types. We do this by inheriting from the Exception class - we&#39;ll explore classes and inheritance more in the next chapter! . class CustomAdditionError(Exception): pass . def add_one(x): if not isinstance(x, float) and not isinstance(x, int): raise CustomAdditionError(&quot;Sorry, x must be numeric&quot;) return x + 1 . add_one(&quot;blah&quot;) . CustomAdditionError Traceback (most recent call last) &lt;ipython-input-3-96e0142692a3&gt; in &lt;module&gt; -&gt; 1 add_one(&#34;blah&#34;) &lt;ipython-input-2-25db54189b4f&gt; in add_one(x) 1 def add_one(x): 2 if not isinstance(x, float) and not isinstance(x, int): -&gt; 3 raise CustomAdditionError(&#34;Sorry, x must be numeric&#34;) 4 5 return x + 1 CustomAdditionError: Sorry, x must be numeric . 5. Functions . A function is a reusable piece of code that can accept input parameters, also known as &quot;arguments&quot;. For example, let&#39;s define a function called square which takes one input parameter n and returns the square n**2: . def square(n): n_squared = n**2 return n_squared . square(2) . 4 . square(100) . 10000 . square(12345) . 152399025 . Functions begin with the def keyword, then the function name, arguments in parentheses, and then a colon (:). The code executed by the function is defined by indentation. The output or &quot;return&quot; value of the function is specified using the return keyword. . Side Effects &amp; Local Variables . When you create a variable inside a function, it is local, which means that it only exists inside the function. For example: . def cat_string(str1, str2): string = str1 + str2 return string . cat_string(&#39;My name is &#39;, &#39;Tom&#39;) . &#39;My name is Tom&#39; . string . NameError Traceback (most recent call last) &lt;ipython-input-10-edbf08a562d5&gt; in &lt;module&gt; -&gt; 1 string NameError: name &#39;string&#39; is not defined . If a function changes the variables passed into it, then it is said to have side effects. For example: . def silly_sum(my_list): my_list.append(0) return sum(my_list) . l = [1, 2, 3, 4] out = silly_sum(l) out . 10 . The above looks like what we wanted? But wait... it changed our l object... . l . [1, 2, 3, 4, 0] . If your function has side effects like this, you must mention it in the documentation (which we&#39;ll touch on later in this chapter). . Null Return Type . If you do not specify a return value, the function returns None when it terminates: . def f(x): x + 1 # no return! if x == 999: return print(f(0)) . None . Optional &amp; Required Arguments . Sometimes it is convenient to have default values for some arguments in a function. Because they have default values, these arguments are optional, and are hence called &quot;optional arguments&quot;. For example: . def repeat_string(s, n=2): return s*n . repeat_string(&quot;mds&quot;, 2) . &#39;mdsmds&#39; . repeat_string(&quot;mds&quot;, 5) . &#39;mdsmdsmdsmdsmds&#39; . repeat_string(&quot;mds&quot;) # do not specify `n`; it is optional . &#39;mdsmds&#39; . Ideally, the default value for optional arguments should be carefully chosen. In the function above, the idea of &quot;repeating&quot; something makes me think of having 2 copies, so n=2 feels like a reasonable default. . You can have any number of required arguments and any number of optional arguments. All the optional arguments must come after the required arguments. The required arguments are mapped by the order they appear. The optional arguments can be specified out of order when using the function. . def example(a, b, c=&quot;DEFAULT&quot;, d=&quot;DEFAULT&quot;): print(a, b, c, d) example(1, 2, 3, 4) . 1 2 3 4 . Using the defaults for c and d: . example(1, 2) . 1 2 DEFAULT DEFAULT . Specifying c and d as keyword arguments (i.e. by name): . example(1, 2, c=3, d=4) . 1 2 3 4 . Specifying only one of the optional arguments, by keyword: . example(1, 2, c=3) . 1 2 3 DEFAULT . Specifying all the arguments as keyword arguments, even though only c and d are optional: . example(a=1, b=2, c=3, d=4) . 1 2 3 4 . Specifying c by the fact that it comes 3rd (I do not recommend this because I find it is confusing): . example(1, 2, 3) . 1 2 3 DEFAULT . Specifying the optional arguments by keyword, but in the wrong order (this can also be confusing, but not so terrible - I am fine with it): . example(1, 2, d=4, c=3) . 1 2 3 4 . Specifying the non-optional arguments by keyword (I am fine with this): . example(a=1, b=2) . 1 2 DEFAULT DEFAULT . Specifying the non-optional arguments by keyword, but in the wrong order (not recommended, I find it confusing): . example(b=2, a=1) . 1 2 DEFAULT DEFAULT . Specifying keyword arguments before non-keyword arguments (this throws an error): . example(a=2, 1) . File &#34;&lt;ipython-input-23-a37b920e8205&gt;&#34;, line 1 example(a=2, 1) ^ SyntaxError: positional argument follows keyword argument . Multiple Return Values . In many programming languages, functions can only return one object. That is technically true in Python too, but there is a &quot;workaround&quot;, which is to return a tuple. . def sum_and_product(x, y): return (x + y, x * y) . sum_and_product(5, 6) . (11, 30) . The parentheses can be omitted (and often are), and a tuple is implicitly returned as defined by the use of the comma: . def sum_and_product(x, y): return x + y, x * y . sum_and_product(5, 6) . (11, 30) . It is common to immediately unpack a returned tuple into separate variables, so it really feels like the function is returning multiple values: . s, p = sum_and_product(5, 6) . s . 11 . p . 30 . As an aside, it is conventional in Python to use _ for values you don&#39;t want: . s, _ = sum_and_product(5, 6) . s . 11 . _ . 30 . Functions with Arbitrary Number of Arguments . You can also call/define functions that accept an arbitrary number of positional or keyword arguments using *args and **kwargs. . def add(*args): print(args) return sum(args) . add(1, 2, 3, 4, 5, 6) . (1, 2, 3, 4, 5, 6) . 21 . def add(**kwargs): print(kwargs) return sum(kwargs.values()) . add(a=3, b=4, c=5) . {&#39;a&#39;: 3, &#39;b&#39;: 4, &#39;c&#39;: 5} . 12 . 6. Functions as a Data Type . In Python, functions are actually a data type: . def do_nothing(x): return x . type(do_nothing) . function . print(do_nothing) . &lt;function do_nothing at 0x1102450e0&gt; . This means you can pass functions as arguments into other functions. . def square(y): return y**2 def evaluate_function_on_x_plus_1(fun, x): return fun(x+1) . evaluate_function_on_x_plus_1(square, 5) . 36 . So what happened above? . fun(x+1) becomes square(5+1) | square(6) becomes 36 | . 7. Anonymous Functions . There are two ways to define functions in Python. The way we&#39;ve beenusing up until now: . def add_one(x): return x+1 . add_one(7.2) . 8.2 . Or by using the lambda keyword: . add_one = lambda x: x+1 . type(add_one) . function . add_one(7.2) . 8.2 . The two approaches above are identical. The one with lambda is called an anonymous function. Anonymous functions can only take up one line of code, so they aren&#39;t appropriate in most cases, but can be useful for smaller things. . evaluate_function_on_x_plus_1(lambda x: x ** 2, 5) . 36 . Above: . First, lambda x: x**2 evaluates to a value of type function (otice that this function is never given a name - hence &quot;anonymous functions&quot;). | Then, the function and the integer 5 are passed into evaluate_function_on_x_plus_1 | At which point the anonymous function is evaluated on 5+1, and we get 36. | . 8. DRY Principle, Designing Good Functions . DRY stands for Don&#39;t Repeat Yourself. See the relevant Wikipedia article for more about this principle. . As an example, consider the task of turning each element of a list into a palindrome. . names = [&quot;milad&quot;, &quot;tom&quot;, &quot;tiffany&quot;] . name = &quot;tom&quot; name[::-1] # creates a slice that starts at the end and moves backwards, syntax is [begin:end:step] . &#39;mot&#39; . names_backwards = list() names_backwards.append(names[0] + names[0][::-1]) names_backwards.append(names[1] + names[1][::-1]) names_backwards.append(names[2] + names[2][::-1]) names_backwards . [&#39;miladdalim&#39;, &#39;tommot&#39;, &#39;tiffanyynaffit&#39;] . The code above is gross, terrible, yucky code for several reasons: . It only works for a list with 3 elements; | It only works for a list named names; | If we want to change its functionality, we need to change 3 similar lines of code (Don&#39;t Repeat Yourself!!); | It is hard to understand what it does just by looking at it. | Let&#39;s try this a different way: . names_backwards = list() for name in names: names_backwards.append(name + name[::-1]) names_backwards . [&#39;miladdalim&#39;, &#39;tommot&#39;, &#39;tiffanyynaffit&#39;] . The above is slightly better and we have solved problems (1) and (3). But let&#39;s create a function to make our life easier: . def make_palindromes(names): names_backwards = list() for name in names: names_backwards.append(name + name[::-1]) return names_backwards make_palindromes(names) . [&#39;miladdalim&#39;, &#39;tommot&#39;, &#39;tiffanyynaffit&#39;] . Okay, this is even better. We have now also solved problem (2), because you can call the function with any list, not just names. For example, what if we had multiple lists: . names1 = [&quot;milad&quot;, &quot;tom&quot;, &quot;tiffany&quot;] names2 = [&quot;apple&quot;, &quot;orange&quot;, &quot;banana&quot;] . make_palindromes(names1) . [&#39;miladdalim&#39;, &#39;tommot&#39;, &#39;tiffanyynaffit&#39;] . make_palindromes(names2) . [&#39;appleelppa&#39;, &#39;orangeegnaro&#39;, &#39;bananaananab&#39;] . Designing Good Functions . How far you go and how you choose to apply the DRY principle is up to you and the programming context. These decisions are often ambiguous. Should make_palindromes() be a function if I&#39;m only ever doing it once? Twice? Should the loop be inside the function, or outside? Should there be TWO functions, one that loops over the other? . In my personal opinion, make_palindromes() does a bit too much to be understandable. I prefer this: . def make_palindrome(name): return name + name[::-1] make_palindrome(&quot;milad&quot;) . &#39;miladdalim&#39; . From here, if we want to &quot;apply make_palindrome to every element of a list&quot; we could use list comprehension: . [make_palindrome(name) for name in names] . [&#39;miladdalim&#39;, &#39;tommot&#39;, &#39;tiffanyynaffit&#39;] . There is also the in-built map() function which does exactly this, applies a function to every element of a sequence: . list(map(make_palindrome, names)) . [&#39;miladdalim&#39;, &#39;tommot&#39;, &#39;tiffanyynaffit&#39;] . 9. Generators . Recall list comprehension from earlier in the chapter: . [n for n in range(10)] . [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] . Comprehensions evaluate the entire expression at once, and then returns the full data product. Sometimes, we want to work with just one part of our data at a time, for example, when we can&#39;t fit all of our data in memory. For this, we can use generators. . (n for n in range(10)) . &lt;generator object &lt;genexpr&gt; at 0x110220650&gt; . Notice that we just created a generator object. Generator objects are like a &quot;recipe&quot; for generating values. They don&#39;t actually do any computation until they are asked to. We can get values from a generator in three main ways: . Using next() | Using list() | Looping | . gen = (n for n in range(10)) . next(gen) . 0 . next(gen) . 1 . Once the generator is exhausted, it will no longer return values: . gen = (n for n in range(10)) for i in range(11): print(next(gen)) . 0 1 2 3 4 5 6 7 8 9 . StopIteration Traceback (most recent call last) &lt;ipython-input-67-14d35f56c593&gt; in &lt;module&gt; 1 gen = (n for n in range(10)) 2 for i in range(11): -&gt; 3 print(next(gen)) StopIteration: . We can see all the values of a generator using list() but this defeats the purpose of using a generator in the first place: . gen = (n for n in range(10)) list(gen) . [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] . Finally, we can loop over generator objects too: . gen = (n for n in range(10)) for i in gen: print(i) . 0 1 2 3 4 5 6 7 8 9 . Above, we saw how to create a generator object using comprehension syntax but with parentheses. We can also create a generator using functions and the yield keyword (instead of the return keyword): . def gen(): for n in range(10): yield (n, n ** 2) . g = gen() print(next(g)) print(next(g)) print(next(g)) . (0, 0) (1, 1) (2, 4) . Below is some real-world motivation of a case where a generator might be useful. Say we want to create a list of dictionaries containing information about houses in Canada. . import random # we&#39;ll learn about imports in a later chapter import time import memory_profiler city = [&#39;Vancouver&#39;, &#39;Toronto&#39;, &#39;Ottawa&#39;, &#39;Montreal&#39;] . def house_list(n): houses = [] for i in range(n): house = { &#39;id&#39;: i, &#39;city&#39;: random.choice(city), &#39;bedrooms&#39;: random.randint(1, 5), &#39;bathrooms&#39;: random.randint(1, 3), &#39;price ($1000s)&#39;: random.randint(300, 1000) } houses.append(house) return houses . house_list(2) . [{&#39;id&#39;: 0, &#39;city&#39;: &#39;Ottawa&#39;, &#39;bedrooms&#39;: 5, &#39;bathrooms&#39;: 2, &#39;price ($1000s)&#39;: 420}, {&#39;id&#39;: 1, &#39;city&#39;: &#39;Montreal&#39;, &#39;bedrooms&#39;: 5, &#39;bathrooms&#39;: 1, &#39;price ($1000s)&#39;: 652}] . What happens if we want to create a list of 1,000,000 houses? How much time/memory will it take? . start = time.time() mem = memory_profiler.memory_usage() print(f&quot;Memory usage before: {mem[0]:.0f} mb&quot;) people = house_list(500000) print(f&quot;Memory usage after: {memory_profiler.memory_usage()[0]:.0f} mb&quot;) print(f&quot;Time taken: {time.time() - start:.2f}s&quot;) . Memory usage before: 86 mb Memory usage after: 251 mb Time taken: 2.24s . def house_generator(n): for i in range(n): house = { &#39;id&#39;: i, &#39;city&#39;: random.choice(city), &#39;bedrooms&#39;: random.randint(1, 5), &#39;bathrooms&#39;: random.randint(1, 3), &#39;price ($1000s)&#39;: random.randint(300, 1000) } yield house . start = time.time() print(f&quot;Memory usage before: {mem[0]:.0f} mb&quot;) people = house_generator(500000) print(f&quot;Memory usage after: {memory_profiler.memory_usage()[0]:.0f} mb&quot;) print(f&quot;Time taken: {time.time() - start:.2f}s&quot;) . Memory usage before: 86 mb Memory usage after: 89 mb Time taken: 0.17s . Although, if we used list() to extract all of the genertator values, we&#39;d lose our memory savings: . print(f&quot;Memory usage before: {mem[0]:.0f} mb&quot;) people = list(house_generator(500000)) print(f&quot;Memory usage after: {memory_profiler.memory_usage()[0]:.0f} mb&quot;) . Memory usage before: 36 mb Memory usage after: 202 mb . 10. Docstrings . One problem we never really solved when talking about writing good functions was: &quot;4. It is hard to understand what it does just by looking at it&quot;. This brings up the idea of function documentation, called &quot;docstrings&quot;. The docstring goes right after the def line and is wrapped in triple quotes &quot;&quot;&quot;. . def make_palindrome(string): &quot;&quot;&quot;Turns the string into a palindrome by concatenating itself with a reversed version of itself.&quot;&quot;&quot; return string + string[::-1] . In Python we can use the help() function to view another function&#39;s documentation. In IPython/Jupyter, we can use ? to view the documentation string of any function in our environment. . make_palindrome? . Signature: make_palindrome(string) Docstring: Turns the string into a palindrome by concatenating itself with a reversed version of itself. File: ~/GitHub/online-courses/python-programming-for-data-science/chapters/&lt;ipython-input-78-3399edf39112&gt; Type: function . But, even easier than that, if your cursor is in the function parentheses, you can use the shortcut shift + tab to open the docstring at will. . . Docstring Structure . General docstring convention in Python is described in PEP 257 - Docstring Conventions. There are many different docstring style conventions used in Python. The exact style you use can be important for helping you to render your documentation, or for helping your IDE parse your documentation. Common styles include: . Single-line: If it&#39;s short, then just a single line describing the function will do (as above). | reST style: see here. | NumPy style: see here. (RECOMMENDED! and MDS-preferred) | Google style: see here. | The NumPy style: . def function_name(param1, param2, param3): &quot;&quot;&quot;First line is a short description of the function. A paragraph describing in a bit more detail what the function does and what algorithms it uses and common use cases. Parameters - param1 : datatype A description of param1. param2 : datatype A description of param2. param3 : datatype A longer description because maybe this requires more explanation and we can use several lines. Returns - datatype A description of the output, datatypes and behaviours. Describe special cases and anything the user needs to know to use the function. Examples -- &gt;&gt;&gt; function_name(3,8,-5) 2.0 &quot;&quot;&quot; . def make_palindrome(string): &quot;&quot;&quot;Turns the string into a palindrome by concatenating itself with a reversed version of itself. Parameters - string : str The string to turn into a palindrome. Returns - str string concatenated with a reversed version of string Examples -- &gt;&gt;&gt; make_palindrome(&#39;tom&#39;) &#39;tommot&#39; &quot;&quot;&quot; return string + string[::-1] . make_palindrome? . Signature: make_palindrome(string) Docstring: Turns the string into a palindrome by concatenating itself with a reversed version of itself. Parameters - string : str The string to turn into a palindrome. Returns - str string concatenated with a reversed version of string Examples -- &gt;&gt;&gt; make_palindrome(&#39;tom&#39;) &#39;tommot&#39; File: ~/GitHub/online-courses/python-programming-for-data-science/chapters/&lt;ipython-input-1-a382cd1ad1e6&gt; Type: function . Docstrings with Optional Arguments . When specifying function arguments, we specify the defaults for optional arguments: . def repeat_string(s, n=2): &quot;&quot;&quot; Repeat the string s, n times. Parameters - s : str the string n : int, optional the number of times, by default = 2 Returns - str the repeated string Examples -- &gt;&gt;&gt; repeat_string(&quot;Blah&quot;, 3) &quot;BlahBlahBlah&quot; &quot;&quot;&quot; return s * n . Type Hints . Type hinting is exactly what it sounds like, it hints at the data type of function arguments. You can indicate the type of an argument in a function using the syntax argument : dtype, and the type of the return value using def func() -&gt; dtype. Let&#39;s see an example: . def repeat_string(s: str, n: int = 2) -&gt; str: # &lt;- note the type hinting here &quot;&quot;&quot; Repeat the string s, n times. Parameters - s : str the string n : int, optional (default = 2) the number of times Returns - str the repeated string Examples -- &gt;&gt;&gt; repeat_string(&quot;Blah&quot;, 3) &quot;BlahBlahBlah&quot; &quot;&quot;&quot; return s * n . repeat_string? . Signature: repeat_string(s: str, n: int = 2) -&gt; str Docstring: Repeat the string s, n times. Parameters - s : str the string n : int, optional (default = 2) the number of times Returns - str the repeated string Examples -- &gt;&gt;&gt; repeat_string(&#34;Blah&#34;, 3) &#34;BlahBlahBlah&#34; File: ~/GitHub/online-courses/python-programming-for-data-science/chapters/&lt;ipython-input-83-964bad9c977b&gt; Type: function . Type hinting just helps your users and IDE identify dtypes and identify bugs. It&#39;s just another level of documentation. They do not force users to use that date type, for example, I can still pass an dict to repeat_string if I want to: . repeat_string({&#39;key_1&#39;: 1, &#39;key_2&#39;: 2}) . TypeError Traceback (most recent call last) &lt;ipython-input-85-48f8613bcebb&gt; in &lt;module&gt; -&gt; 1 repeat_string({&#39;key_1&#39;: 1, &#39;key_2&#39;: 2}) &lt;ipython-input-83-964bad9c977b&gt; in repeat_string(s, n) 21 &#34;BlahBlahBlah&#34; 22 &#34;&#34;&#34; &gt; 23 return s * n TypeError: unsupported operand type(s) for *: &#39;dict&#39; and &#39;int&#39; . Most IDE&#39;s are clever enough to even read your type hinting and warn you if you&#39;re using a different dtype in the function, e.g., this VScode screenshot: . .",
            "url": "https://millermuttu.github.io/blog/jupyter/python/data%20science/programming/basics/2022/03/22/chapter2-loops-functions.html",
            "relUrl": "/jupyter/python/data%20science/programming/basics/2022/03/22/chapter2-loops-functions.html",
            "date": " ‚Ä¢ Mar 22, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "Data science revision - chapter 1",
            "content": ". Chapter 1: Python . lets get started . Chapter Outline . . 1. Introduction | 2. Basic Python Data Types | 3. Lists and Tuples | 4. String Methods | 5. Dictionaries | 6. Empties | 7. Conditionals | . Chapter Learning Objectives . Create, describe and differentiate standard Python datatypes such as int, float, string, list, dict, tuple, etc. | Perform arithmetic operations like +, -, *, ** on numeric values. | Perform basic string operations like .lower(), .split() to manipulate strings. | Compute boolean values using comparison operators operations (==, !=, &gt;, etc.) and boolean operators (and, or, not). | Assign, index, slice and subset values to and from tuples, lists, strings and dictionaries. | Write a conditional statement with if, elif and else. | Identify code blocks by levels of indentation. | Explain the difference between mutable objects like a list and immutable objects like a tuple. | . 1. Introduction . The material presented on this website assumes no prior knowledge of Python. Experience with programming concepts or another programming language will help, but is not required to understand the material. . The website comprises the following: . Chapters: these contain the core content. Read through these at your leisure. | Practice Exercises: there are optional practice exercise sets to complement each chapter (solutions included). Try your hand at these for extra practice and to help solidify concepts in the Chapters. | 2. Basic Python Data Types . A value is a piece of data that a computer program works with such as a number or text. There are different types of values: 42 is an integer and &quot;Hello!&quot; is a string. A variable is a name that refers to a value. In mathematics and statistics, we usually use variable names like $x$ and $y$. In Python, we can use any word as a variable name as long as it starts with a letter or an underscore. However, it should not be a reserved word in Python such as for, while, class, lambda, etc. as these words encode special functionality in Python that we don&#39;t want to overwrite! . It can be helpful to think of a variable as a box that holds some information (a single number, a vector, a string, etc). We use the assignment operator = to assign a value to a variable. . . Image modified from: medium.com . {tip} See the [Python 3 documentation](https://docs.python.org/3/library/stdtypes.html) for a summary of the standard built-in Python datatypes. . Common built-in Python data types . English name Type name Type Category Description Example . integer | int | Numeric Type | positive/negative whole numbers | 42 | . floating point number | float | Numeric Type | real number in decimal form | 3.14159 | . boolean | bool | Boolean Values | true or false | True | . string | str | Sequence Type | text | &quot;I Can Has Cheezburger?&quot; | . list | list | Sequence Type | a collection of objects - mutable &amp; ordered | [&#39;Ali&#39;, &#39;Xinyi&#39;, &#39;Miriam&#39;] | . tuple | tuple | Sequence Type | a collection of objects - immutable &amp; ordered | (&#39;Thursday&#39;, 6, 9, 2018) | . dictionary | dict | Mapping Type | mapping of key-value pairs | {&#39;name&#39;:&#39;DSCI&#39;, &#39;code&#39;:511, &#39;credits&#39;:2} | . none | NoneType | Null Object | represents no value | None | . Numeric data types . There are three distinct numeric types: integers, floating point numbers, and complex numbers (not covered here). We can determine the type of an object in Python using type(). We can print the value of the object using print(). . x = 42 . type(x) . int . print(x) . 42 . In Jupyter/IPython (an interactive version of Python), the last line of a cell will automatically be printed to screen so we don&#39;t actually need to explicitly call print(). . x # Anything after the pound/hash symbol is a comment and will not be run . 42 . pi = 3.14159 pi . 3.14159 . type(pi) . float . Arithmetic Operators . Below is a table of the syntax for common arithmetic operations in Python: . Operator Description . + | addition | . - | subtraction | . * | multiplication | . / | division | . ** | exponentiation | . // | integer division / floor division | . % | modulo | . Let&#39;s have a go at applying these operators to numeric types and observe the results. . 1 + 2 + 3 + 4 + 5 # add . 15 . 2 * 3.14159 # multiply . 6.28318 . 2 ** 10 # exponent . 1024 . Division may produce a different dtype than expected, it will change int to float. . int_2 = 2 type(int_2) . int . int_2 / int_2 # divison . 1.0 . type(int_2 / int_2) . float . But the syntax // allows us to do &quot;integer division&quot; (aka &quot;floor division&quot;) and retain the int data type, it always rounds down. . 101 / 2 . 50.5 . 101 // 2 # &quot;floor division&quot; - always rounds down . 50 . We refer to this as &quot;integer division&quot; or &quot;floor division&quot; because it&#39;s like calling int on the result of a division, which rounds down to the nearest integer, or &quot;floors&quot; the result. . int(101 / 2) . 50 . The % &quot;modulo&quot; operator gives us the remainder after division. . 100 % 2 # &quot;100 mod 2&quot;, or the remainder when 100 is divided by 2 . 0 . 101 % 2 # &quot;101 mod 2&quot;, or the remainder when 101 is divided by 2 . 1 . 100.5 % 2 . 0.5 . None . NoneType is its own type in Python. It only has one possible value, None - it represents an object with no value. We&#39;ll see it again in a later chapter. . x = None . print(x) . None . type(x) . NoneType . Strings . Text is stored as a data type called a string. We can think of a string as a sequence of characters. . {tip} Actually they are a sequence of Unicode code points. Here&#39;s a [great blog post](https://www.joelonsoftware.com/2003/10/08/the-absolute-minimum-every-software-developer-absolutely-positively-must-know-about-unicode-and-character-sets-no-excuses/) on Unicode if you&#39;re interested. . We write strings as characters enclosed with either: . single quotes, e.g., &#39;Hello&#39; | double quotes, e.g., &quot;Goodbye&quot; | . There&#39;s no difference between the two methods, but there are cases where having both is useful (more on that below)! We also have triple double quotes, which are typically used for function documentation (more on that in a later chapter), e.g., &quot;&quot;&quot;This function adds two numbers&quot;&quot;&quot;. . my_name = &quot;Tomas Beuzen&quot; . my_name . &#39;Tomas Beuzen&#39; . type(my_name) . str . course = &#39;DSCI 511&#39; . course . &#39;DSCI 511&#39; . type(course) . str . If the string contains a quotation or apostrophe, we can use a combination of single and double quotes to define the string. . sentence = &quot;It&#39;s a rainy day.&quot; . sentence . &#34;It&#39;s a rainy day.&#34; . type(sentence) . str . quote = &#39;Donald Knuth: &quot;Premature optimization is the root of all evil.&quot;&#39; . quote . &#39;Donald Knuth: &#34;Premature optimization is the root of all evil.&#34;&#39; . Boolean . The Boolean (bool) type has two values: True and False. . the_truth = True . the_truth . True . type(the_truth) . bool . lies = False . lies . False . type(lies) . bool . Comparison Operators . We can compare objects using comparison operators, and we&#39;ll get back a Boolean result: . Operator Description . x == y | is x equal to y? | . x != y | is x not equal to y? | . x &gt; y | is x greater than y? | . x &gt;= y | is x greater than or equal to y? | . x &lt; y | is x less than y? | . x &lt;= y | is x less than or equal to y? | . x is y | is x the same object as y? | . 2 &lt; 3 . True . &quot;Deep learning&quot; == &quot;Solve all the world&#39;s problems&quot; . False . 2 != &quot;2&quot; . True . 2 is 2 . True . 2 == 2.0 . True . Boolean Operators . We also have so-called &quot;boolean operators&quot; which also evaluates to either True or False: . Operator Description . x and y | are x and y both True? | . x or y | is at least one of x and y True? | . not x | is x False? | . True and True . True . True and False . False . True or False . True . False or False . False . (&quot;Python 2&quot; != &quot;Python 3&quot;) and (2 &lt;= 3) . True . True . True . not True . False . not not True . True . {note} Python also has [bitwise operators](https://wiki.python.org/moin/BitwiseOperators) like `&amp;` and `|`. Bitwise operators literally compare the bits of two integers. That&#39;s beyond the scope of this course but I&#39;ve included a code snippet below to show you them in action. . print(f&quot;Bit representation of the number 5: {5:0b}&quot;) print(f&quot;Bit representation of the number 4: {4:0b}&quot;) print(f&quot; ‚Üì‚Üì‚Üì&quot;) print(f&quot; {5 &amp; 4:0b}&quot;) print(f&quot; ‚Üì &quot;) print(f&quot; {5 &amp; 4}&quot;) . Bit representation of the number 5: 101 Bit representation of the number 4: 100 ‚Üì‚Üì‚Üì 100 ‚Üì 4 . Casting . Sometimes we need to explicitly cast a value from one type to another. We can do this using functions like str(), int(), and float(). Python tries to do the conversion, or throws an error if it can&#39;t. . x = 5.0 type(x) . float . x = int(5.0) x . 5 . type(x) . int . x = str(5.0) x . &#39;5.0&#39; . type(x) . str . str(5.0) == 5.0 . False . int(5.3) . 5 . float(&quot;hello&quot;) . ValueError Traceback (most recent call last) &lt;ipython-input-69-7124e8e12e61&gt; in &lt;module&gt; -&gt; 1 float(&#34;hello&#34;) ValueError: could not convert string to float: &#39;hello&#39; . 3. Lists and Tuples . Lists and tuples allow us to store multiple things (&quot;elements&quot;) in a single object. The elements are ordered (we&#39;ll explore what that means a little later). We&#39;ll start with lists. Lists are defined with square brackets []. . my_list = [1, 2, &quot;THREE&quot;, 4, 0.5] . my_list . [1, 2, &#39;THREE&#39;, 4, 0.5] . type(my_list) . list . Lists can hold any datatype - even other lists! . another_list = [1, &quot;two&quot;, [3, 4, &quot;five&quot;], True, None, {&quot;key&quot;: &quot;value&quot;}] another_list . [1, &#39;two&#39;, [3, 4, &#39;five&#39;], True, None, {&#39;key&#39;: &#39;value&#39;}] . You can get the length of the list with the function len(): . len(my_list) . 5 . Tuples look similar to lists but have a key difference (they are immutable - but more on that a bit later). They are defined with parentheses (). . today = (1, 2, &quot;THREE&quot;, 4, 0.5) . today . (1, 2, &#39;THREE&#39;, 4, 0.5) . type(today) . tuple . len(today) . 5 . Indexing and Slicing Sequences . We can access values inside a list, tuple, or string using square bracket syntax. Python uses zero-based indexing, which means the first element of the list is in position 0, not position 1. . my_list . [1, 2, &#39;THREE&#39;, 4, 0.5] . my_list[0] . 1 . my_list[2] . &#39;THREE&#39; . len(my_list) . 5 . my_list[5] . IndexError Traceback (most recent call last) &lt;ipython-input-84-075ca585e721&gt; in &lt;module&gt; -&gt; 1 my_list[5] IndexError: list index out of range . We can use negative indices to count backwards from the end of the list. . my_list . [1, 2, &#39;THREE&#39;, 4, 0.5] . my_list[-1] . 0.5 . my_list[-2] . 4 . We can use the colon : to access a sub-sequence. This is called &quot;slicing&quot;. . my_list[1:3] . [2, &#39;THREE&#39;] . Note from the above that the start of the slice is inclusive and the end is exclusive. So my_list[1:3] fetches elements 1 and 2, but not 3. . Strings behave the same as lists and tuples when it comes to indexing and slicing. Remember, we think of them as a sequence of characters. . alphabet = &quot;abcdefghijklmnopqrstuvwxyz&quot; . alphabet[0] . &#39;a&#39; . alphabet[-1] . &#39;z&#39; . alphabet[-3] . &#39;x&#39; . alphabet[:5] . &#39;abcde&#39; . alphabet[12:20] . &#39;mnopqrst&#39; . List Methods . A list is an object and it has methods for interacting with its data. A method is like a function, it performs some operation with the data, but a method differs to a function in that it is defined on the object itself and accessed using a period .. For example, my_list.append(item) appends an item to the end of the list called my_list. You can see the documentation for more list methods. . primes = [2, 3, 5, 7, 11] primes . [2, 3, 5, 7, 11] . len(primes) . 5 . primes.append(13) . primes . [2, 3, 5, 7, 11, 13] . Sets . Another built-in Python data type is the set, which stores an un-ordered list of unique items. Being unordered, sets do not record element position or order of insertion and so do not support indexing. . s = {2, 3, 5, 11} s . {2, 3, 5, 11} . {1, 2, 3} == {3, 2, 1} . True . [1, 2, 3] == [3, 2, 1] . False . s.add(2) # does nothing s . {2, 3, 5, 11} . s[0] . TypeError Traceback (most recent call last) &lt;ipython-input-109-c9c96910e542&gt; in &lt;module&gt; -&gt; 1 s[0] TypeError: &#39;set&#39; object is not subscriptable . Above: throws an error because elements are not ordered and can&#39;t be indexing. . Mutable vs. Immutable Types . Strings and tuples are immutable types which means they can&#39;t be modified. Lists are mutable and we can assign new values for its various entries. This is the main difference between lists and tuples. . names_list = [&quot;Indiana&quot;, &quot;Fang&quot;, &quot;Linsey&quot;] names_list . [&#39;Indiana&#39;, &#39;Fang&#39;, &#39;Linsey&#39;] . names_list[0] = &quot;Cool guy&quot; names_list . [&#39;Cool guy&#39;, &#39;Fang&#39;, &#39;Linsey&#39;] . names_tuple = (&quot;Indiana&quot;, &quot;Fang&quot;, &quot;Linsey&quot;) names_tuple . (&#39;Indiana&#39;, &#39;Fang&#39;, &#39;Linsey&#39;) . names_tuple[0] = &quot;Not cool guy&quot; . TypeError Traceback (most recent call last) &lt;ipython-input-113-bd6a1b77b220&gt; in &lt;module&gt; -&gt; 1 names_tuple[0] = &#34;Not cool guy&#34; TypeError: &#39;tuple&#39; object does not support item assignment . Same goes for strings. Once defined we cannot modifiy the characters of the string. . my_name = &quot;Tom&quot; . my_name[-1] = &quot;q&quot; . TypeError Traceback (most recent call last) &lt;ipython-input-115-9bfcf81dbcf0&gt; in &lt;module&gt; -&gt; 1 my_name[-1] = &#34;q&#34; TypeError: &#39;str&#39; object does not support item assignment . x = ([1, 2, 3], 5) . x[1] = 7 . TypeError Traceback (most recent call last) &lt;ipython-input-117-415ce6bd0126&gt; in &lt;module&gt; -&gt; 1 x[1] = 7 TypeError: &#39;tuple&#39; object does not support item assignment . x . ([1, 2, 3], 5) . x[0][1] = 4 . x . ([1, 4, 3], 5) . 4. String Methods . There are various useful string methods in Python. . all_caps = &quot;HOW ARE YOU TODAY?&quot; all_caps . &#39;HOW ARE YOU TODAY?&#39; . new_str = all_caps.lower() new_str . &#39;how are you today?&#39; . Note that the method lower doesn&#39;t change the original string but rather returns a new one. . all_caps . &#39;HOW ARE YOU TODAY?&#39; . There are many string methods. Check out the documentation. . all_caps.split() . [&#39;HOW&#39;, &#39;ARE&#39;, &#39;YOU&#39;, &#39;TODAY?&#39;] . all_caps.count(&quot;O&quot;) . 3 . One can explicitly cast a string to a list: . caps_list = list(all_caps) caps_list . [&#39;H&#39;, &#39;O&#39;, &#39;W&#39;, &#39; &#39;, &#39;A&#39;, &#39;R&#39;, &#39;E&#39;, &#39; &#39;, &#39;Y&#39;, &#39;O&#39;, &#39;U&#39;, &#39; &#39;, &#39;T&#39;, &#39;O&#39;, &#39;D&#39;, &#39;A&#39;, &#39;Y&#39;, &#39;?&#39;] . &quot;&quot;.join(caps_list) . &#39;HOW ARE YOU TODAY?&#39; . &quot;-&quot;.join(caps_list) . &#39;H-O-W- -A-R-E- -Y-O-U- -T-O-D-A-Y-?&#39; . We can also chain multiple methods together (more on this when we get to NumPy and Pandas in later chapters): . &quot;&quot;.join(caps_list).lower().split(&quot; &quot;) . [&#39;how&#39;, &#39;are&#39;, &#39;you&#39;, &#39;today?&#39;] . String formatting . Python has ways of creating strings by &quot;filling in the blanks&quot; and formatting them nicely. This is helpful for when you want to print statements that include variables or statements. There are a few ways of doing this but I use and recommend f-strings which were introduced in Python 3.6. All you need to do is put the letter &quot;f&quot; out the front of your string and then you can include variables with curly-bracket notation {}. . name = &quot;Newborn Baby&quot; age = 4 / 12 day = 10 month = 6 year = 2020 template_new = f&quot;Hello, my name is {name}. I am {age:.2f} years old. I was born {day}/{month:02}/{year}.&quot; template_new . &#39;Hello, my name is Newborn Baby. I am 0.33 years old. I was born 10/06/2020.&#39; . {note} Notes require **no** arguments, In the code above, the notation after the colon in my curly braces is for formatting. For example, `:.2f` means, print this variable with 2 decimal places. See format code options [here](https://docs.python.org/3.4/library/string.html#format-specification-mini-language). . 5. Dictionaries . A dictionary is a mapping between key-values pairs and is defined with curly-brackets: . house = { &quot;bedrooms&quot;: 3, &quot;bathrooms&quot;: 2, &quot;city&quot;: &quot;Vancouver&quot;, &quot;price&quot;: 2499999, &quot;date_sold&quot;: (1, 3, 2015), } condo = { &quot;bedrooms&quot;: 2, &quot;bathrooms&quot;: 1, &quot;city&quot;: &quot;Burnaby&quot;, &quot;price&quot;: 699999, &quot;date_sold&quot;: (27, 8, 2011), } . We can access a specific field of a dictionary with square brackets: . house[&quot;price&quot;] . 2499999 . condo[&quot;city&quot;] . &#39;Burnaby&#39; . We can also edit dictionaries (they are mutable): . condo[&quot;price&quot;] = 5 # price already in the dict condo . {&#39;bedrooms&#39;: 2, &#39;bathrooms&#39;: 1, &#39;city&#39;: &#39;Burnaby&#39;, &#39;price&#39;: 5, &#39;date_sold&#39;: (27, 8, 2011)} . condo[&quot;flooring&quot;] = &quot;wood&quot; . condo . {&#39;bedrooms&#39;: 2, &#39;bathrooms&#39;: 1, &#39;city&#39;: &#39;Burnaby&#39;, &#39;price&#39;: 5, &#39;date_sold&#39;: (27, 8, 2011), &#39;flooring&#39;: &#39;wood&#39;} . We can also delete fields entirely (though I rarely use this): . del condo[&quot;city&quot;] . KeyError Traceback (most recent call last) &lt;ipython-input-138-c9f116276b74&gt; in &lt;module&gt; -&gt; 1 del condo[&#34;city&#34;] KeyError: &#39;city&#39; . condo . {&#39;bedrooms&#39;: 2, &#39;bathrooms&#39;: 1, &#39;price&#39;: 5, &#39;date_sold&#39;: (27, 8, 2011), &#39;flooring&#39;: &#39;wood&#39;} . And we can easily add fields: . condo[5] = 443345 . condo . {&#39;bedrooms&#39;: 2, &#39;bathrooms&#39;: 1, &#39;price&#39;: 5, &#39;date_sold&#39;: (27, 8, 2011), &#39;flooring&#39;: &#39;wood&#39;, 5: 443345} . Keys may be any immutable data type, even a tuple! . condo[(1, 2, 3)] = 777 condo . {&#39;bedrooms&#39;: 2, &#39;bathrooms&#39;: 1, &#39;price&#39;: 5, &#39;date_sold&#39;: (27, 8, 2011), &#39;flooring&#39;: &#39;wood&#39;, 5: 443345, (1, 2, 3): 777} . You&#39;ll get an error if you try to access a non-existent key: . condo[&quot;not-here&quot;] . KeyError Traceback (most recent call last) &lt;ipython-input-143-ab081f66baa5&gt; in &lt;module&gt; -&gt; 1 condo[&#34;not-here&#34;] KeyError: &#39;not-here&#39; . 6. Empties . Sometimes you&#39;ll want to create empty objects that will be filled later on. . lst = list() # empty list lst . [] . lst = [] # empty list lst . [] . There&#39;s no real difference between the two methods above, [] is apparently marginally faster... . tup = tuple() # empty tuple tup . () . tup = () # empty tuple tup . () . dic = dict() # empty dict dic . {} . dic = {} # empty dict dic . {} . st = set() # empty set st . set() . 7. Conditionals . Conditional statements allow us to write programs where only certain blocks of code are executed depending on the state of the program. Let&#39;s look at some examples and take note of the keywords, syntax and indentation. . name = &quot;Tom&quot; if name.lower() == &quot;tom&quot;: print(&quot;That&#39;s my name too!&quot;) elif name.lower() == &quot;santa&quot;: print(&quot;That&#39;s a funny name.&quot;) else: print(f&quot;Hello {name}! That&#39;s a cool name!&quot;) print(&quot;Nice to meet you!&quot;) . That&#39;s my name too! Nice to meet you! . The main points to notice: . Use keywords if, elif and else | The colon : ends each conditional expression | Indentation (by 4 empty space) defines code blocks | In an if statement, the first block whose conditional statement returns True is executed and the program exits the if block | if statements don&#39;t necessarily need elif or else | elif lets us check several conditions | else lets us evaluate a default block if all other conditions are False | the end of the entire if statement is where the indentation returns to the same level as the first if keyword | . If statements can also be nested inside of one another: . name = &quot;Super Tom&quot; if name.lower() == &quot;tom&quot;: print(&quot;That&#39;s my name too!&quot;) elif name.lower() == &quot;santa&quot;: print(&quot;That&#39;s a funny name.&quot;) else: print(f&quot;Hello {name}! That&#39;s a cool name.&quot;) if name.lower().startswith(&quot;super&quot;): print(&quot;Do you really have superpowers?&quot;) print(&quot;Nice to meet you!&quot;) . Hello Super Tom! That&#39;s a cool name. Do you really have superpowers? Nice to meet you! . Inline if/else . We can write simple if statements &quot;inline&quot;, i.e., in a single line, for simplicity. . words = [&quot;the&quot;, &quot;list&quot;, &quot;of&quot;, &quot;words&quot;] x = &quot;long list&quot; if len(words) &gt; 10 else &quot;short list&quot; x . &#39;short list&#39; . if len(words) &gt; 10: x = &quot;long list&quot; else: x = &quot;short list&quot; . x . &#39;short list&#39; . Truth Value Testing . Any object can be tested for &quot;truth&quot; in Python, for use in if and while (next chapter) statements. . True values: all objects return True unless they are a bool object with value False or have len() == 0 | False values: None, False, 0, empty sequences and collections: &#39;&#39;, (), [], {}, set() | . {tip} Read more in the [docs here](https://docs.python.org/3/library/stdtypes.html#truth-value-testing). . x = 1 if x: print(&quot;I&#39;m truthy!&quot;) else: print(&quot;I&#39;m falsey!&quot;) . I&#39;m truthy! . x = False if x: print(&quot;I&#39;m truthy!&quot;) else: print(&quot;I&#39;m falsey!&quot;) . I&#39;m falsey! . x = [] if x: print(&quot;I&#39;m truthy!&quot;) else: print(&quot;I&#39;m falsey!&quot;) . I&#39;m falsey! . Short-circuiting . Python supports a concept known as &quot;short-circuting&quot;. This is the automatic stopping of the execution of boolean operation if the truth value of expression has already been determined. . fake_variable # not defined . NameError Traceback (most recent call last) &lt;ipython-input-159-38b1451e4717&gt; in &lt;module&gt; -&gt; 1 fake_variable # not defined NameError: name &#39;fake_variable&#39; is not defined . True or fake_variable . True . True and fake_variable . NameError Traceback (most recent call last) &lt;ipython-input-161-a7196cc665d5&gt; in &lt;module&gt; -&gt; 1 True and fake_variable NameError: name &#39;fake_variable&#39; is not defined . False and fake_variable . False . Expression Result Detail . A or B | If A is True then A else B | B only executed if A is False | . A and B | If A is False then A else B | B only executed if A is True | .",
            "url": "https://millermuttu.github.io/blog/jupyter/python/data%20science/programming/basics/2022/03/15/chapter1-basics.html",
            "relUrl": "/jupyter/python/data%20science/programming/basics/2022/03/15/chapter1-basics.html",
            "date": " ‚Ä¢ Mar 15, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.‚Ü© . 2. This is the other footnote. You can even have a link!‚Ü© .",
            "url": "https://millermuttu.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " ‚Ä¢ Feb 20, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a ‚Äúlevel 1 heading‚Äù in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here‚Äôs a footnote 1. Here‚Äôs a horizontal rule: . . Lists . Here‚Äôs a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes ‚Ä¶and‚Ä¶ . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote.¬†&#8617; . |",
            "url": "https://millermuttu.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " ‚Ä¢ Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hello, Mallikarjun Sajjan hereüëã :sunglasses: . I am from Koppal, karnataka, i born and bought up in this small town from North Karnataka. i completed my early schooling till 10th in koppal and moved to hubli for Pre university education. i did my PU in science from Chetan PU collge and secured CET rank of 666 (Yes!! fancy number) and got into RVCE for BE in Electronics and communication. . I started my career as data scientist in RBEI. and worked on various intresting topics for almost 5 years in bosch, untill i cameout to do my own startup checkag . This is what i am currently busy with: . üî≠ I‚Äôm currently working on : My own startup | üå± I‚Äôm currently learning : MLops, Web development. | üëØ I‚Äôm looking to collaborate on : Machine learning projects and Deep learning Projects | ü§î I‚Äôm looking for help with : Machine learning projects and Deep learning Projects | üí¨ Ask me about : startup journey, travel, football | üì´ How to reach me: mallikarjunvsaj@gmail.com | üòÑ Pronouns: Mallik | ‚ö° Fun fact: i like to hangout with people and i dont have peopleüòÑ. same with my travle story, l like to roam more countries and havent got a job abroadüòÑ | . . I am an AI/ML Engineer who likes to contribute in computer vision, data science aspects. . GitHub Stats . . . Here my resume. please get connected. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats.¬†&#8617; . |",
          "url": "https://millermuttu.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://millermuttu.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}